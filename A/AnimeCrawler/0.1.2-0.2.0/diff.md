# Comparing `tmp/AnimeCrawler-0.1.2.tar.gz` & `tmp/AnimeCrawler-0.2.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "AnimeCrawler-0.1.2.tar", last modified: Wed May  3 03:36:47 2023, max compression
+gzip compressed data, was "AnimeCrawler-0.2.0.tar", last modified: Sat May  6 14:39:52 2023, max compression
```

## Comparing `AnimeCrawler-0.1.2.tar` & `AnimeCrawler-0.2.0.tar`

### file list

```diff
@@ -1,29 +1,31 @@
-drwxrwxrwx   0        0        0        0 2023-05-03 03:36:47.215752 AnimeCrawler-0.1.2/
-drwxrwxrwx   0        0        0        0 2023-05-03 03:36:47.136797 AnimeCrawler-0.1.2/AnimeCrawler/
--rw-rw-rw-   0        0        0      353 2023-05-03 02:47:47.000000 AnimeCrawler-0.1.2/AnimeCrawler/__init__.py
--rw-rw-rw-   0        0        0      281 2023-05-03 02:47:10.000000 AnimeCrawler-0.1.2/AnimeCrawler/__main__.py
--rw-rw-rw-   0        0        0     2198 2023-05-03 02:47:10.000000 AnimeCrawler-0.1.2/AnimeCrawler/base_spider.py
-drwxrwxrwx   0        0        0        0 2023-05-03 03:36:47.185769 AnimeCrawler-0.1.2/AnimeCrawler/command/
--rw-rw-rw-   0        0        0       76 2023-05-03 02:47:10.000000 AnimeCrawler-0.1.2/AnimeCrawler/command/__init__.py
--rw-rw-rw-   0        0        0     2912 2023-05-03 03:12:56.000000 AnimeCrawler-0.1.2/AnimeCrawler/command/run.py
--rw-rw-rw-   0        0        0      356 2023-05-03 02:47:10.000000 AnimeCrawler-0.1.2/AnimeCrawler/log.py
-drwxrwxrwx   0        0        0        0 2023-05-03 03:36:47.193771 AnimeCrawler-0.1.2/AnimeCrawler/mhyyy/
--rw-rw-rw-   0        0        0      256 2023-05-03 02:47:31.000000 AnimeCrawler-0.1.2/AnimeCrawler/mhyyy/__init__.py
--rw-rw-rw-   0        0        0     2286 2023-05-03 02:47:10.000000 AnimeCrawler-0.1.2/AnimeCrawler/mhyyy/downloader.py
--rw-rw-rw-   0        0        0     4172 2023-05-03 02:47:10.000000 AnimeCrawler-0.1.2/AnimeCrawler/mhyyy/spider.py
-drwxrwxrwx   0        0        0        0 2023-05-03 03:36:47.211754 AnimeCrawler-0.1.2/AnimeCrawler/utils/
--rw-rw-rw-   0        0        0      121 2023-05-03 02:47:10.000000 AnimeCrawler-0.1.2/AnimeCrawler/utils/__init__.py
--rw-rw-rw-   0        0        0     1403 2023-05-03 02:47:10.000000 AnimeCrawler-0.1.2/AnimeCrawler/utils/decode.py
--rw-rw-rw-   0        0        0     2154 2023-05-03 02:47:10.000000 AnimeCrawler-0.1.2/AnimeCrawler/utils/file.py
-drwxrwxrwx   0        0        0        0 2023-05-03 03:36:47.179773 AnimeCrawler-0.1.2/AnimeCrawler.egg-info/
--rw-rw-rw-   0        0        0     3406 2023-05-03 03:36:46.000000 AnimeCrawler-0.1.2/AnimeCrawler.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0      583 2023-05-03 03:36:46.000000 AnimeCrawler-0.1.2/AnimeCrawler.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2023-05-03 03:36:46.000000 AnimeCrawler-0.1.2/AnimeCrawler.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0       63 2023-05-03 03:36:46.000000 AnimeCrawler-0.1.2/AnimeCrawler.egg-info/entry_points.txt
--rw-rw-rw-   0        0        0       27 2023-05-03 03:36:46.000000 AnimeCrawler-0.1.2/AnimeCrawler.egg-info/requires.txt
--rw-rw-rw-   0        0        0       13 2023-05-03 03:36:46.000000 AnimeCrawler-0.1.2/AnimeCrawler.egg-info/top_level.txt
--rw-rw-rw-   0        0        0     1083 2023-04-15 13:28:24.000000 AnimeCrawler-0.1.2/LICENSE
--rw-rw-rw-   0        0        0     3406 2023-05-03 03:36:47.213753 AnimeCrawler-0.1.2/PKG-INFO
--rw-rw-rw-   0        0        0     2957 2023-05-03 02:19:56.000000 AnimeCrawler-0.1.2/README.md
--rw-rw-rw-   0        0        0       42 2023-05-03 03:36:47.215752 AnimeCrawler-0.1.2/setup.cfg
--rw-rw-rw-   0        0        0      884 2023-05-03 03:31:04.000000 AnimeCrawler-0.1.2/setup.py
+drwxrwxrwx   0        0        0        0 2023-05-06 14:39:52.544256 AnimeCrawler-0.2.0/
+drwxrwxrwx   0        0        0        0 2023-05-06 14:39:52.153479 AnimeCrawler-0.2.0/AnimeCrawler/
+-rw-rw-rw-   0        0        0      388 2023-05-06 14:06:08.000000 AnimeCrawler-0.2.0/AnimeCrawler/__init__.py
+-rw-rw-rw-   0        0        0     2682 2023-05-06 13:57:08.000000 AnimeCrawler-0.2.0/AnimeCrawler/base_spider.py
+drwxrwxrwx   0        0        0        0 2023-05-06 14:39:52.392346 AnimeCrawler-0.2.0/AnimeCrawler/command/
+-rw-rw-rw-   0        0        0      117 2023-05-03 05:18:04.000000 AnimeCrawler-0.2.0/AnimeCrawler/command/__init__.py
+-rw-rw-rw-   0        0        0     3250 2023-05-06 14:05:51.000000 AnimeCrawler-0.2.0/AnimeCrawler/command/run.py
+-rw-rw-rw-   0        0        0      356 2023-05-03 05:07:40.000000 AnimeCrawler-0.2.0/AnimeCrawler/log.py
+drwxrwxrwx   0        0        0        0 2023-05-06 14:39:52.493285 AnimeCrawler-0.2.0/AnimeCrawler/mhyyy/
+-rw-rw-rw-   0        0        0      286 2023-05-03 09:50:27.000000 AnimeCrawler-0.2.0/AnimeCrawler/mhyyy/__init__.py
+-rw-rw-rw-   0        0        0     2396 2023-05-06 13:55:26.000000 AnimeCrawler-0.2.0/AnimeCrawler/mhyyy/downloader.py
+-rw-rw-rw-   0        0        0     2084 2023-05-06 13:51:37.000000 AnimeCrawler-0.2.0/AnimeCrawler/mhyyy/searcher.py
+-rw-rw-rw-   0        0        0     4138 2023-05-06 13:58:10.000000 AnimeCrawler-0.2.0/AnimeCrawler/mhyyy/spider.py
+drwxrwxrwx   0        0        0        0 2023-05-06 14:39:52.499281 AnimeCrawler-0.2.0/AnimeCrawler/utils/
+-rw-rw-rw-   0        0        0      128 2023-05-03 15:03:17.000000 AnimeCrawler-0.2.0/AnimeCrawler/utils/__init__.py
+-rw-rw-rw-   0        0        0     1403 2023-05-03 02:47:10.000000 AnimeCrawler-0.2.0/AnimeCrawler/utils/decode.py
+-rw-rw-rw-   0        0        0     3017 2023-05-03 15:13:47.000000 AnimeCrawler-0.2.0/AnimeCrawler/utils/file.py
+drwxrwxrwx   0        0        0        0 2023-05-06 14:39:52.348369 AnimeCrawler-0.2.0/AnimeCrawler.egg-info/
+-rw-rw-rw-   0        0        0     3268 2023-05-06 14:39:51.000000 AnimeCrawler-0.2.0/AnimeCrawler.egg-info/PKG-INFO
+-rw-rw-rw-   0        0        0      602 2023-05-06 14:39:51.000000 AnimeCrawler-0.2.0/AnimeCrawler.egg-info/SOURCES.txt
+-rw-rw-rw-   0        0        0        1 2023-05-06 14:39:51.000000 AnimeCrawler-0.2.0/AnimeCrawler.egg-info/dependency_links.txt
+-rw-rw-rw-   0        0        0       63 2023-05-06 14:39:51.000000 AnimeCrawler-0.2.0/AnimeCrawler.egg-info/entry_points.txt
+-rw-rw-rw-   0        0        0       25 2023-05-06 14:39:51.000000 AnimeCrawler-0.2.0/AnimeCrawler.egg-info/requires.txt
+-rw-rw-rw-   0        0        0       13 2023-05-06 14:39:51.000000 AnimeCrawler-0.2.0/AnimeCrawler.egg-info/top_level.txt
+-rw-rw-rw-   0        0        0     1083 2023-04-15 13:28:24.000000 AnimeCrawler-0.2.0/LICENSE
+-rw-rw-rw-   0        0        0     3268 2023-05-06 14:39:52.505278 AnimeCrawler-0.2.0/PKG-INFO
+-rw-rw-rw-   0        0        0     2819 2023-05-06 14:03:37.000000 AnimeCrawler-0.2.0/README.md
+-rw-rw-rw-   0        0        0       42 2023-05-06 14:39:52.545255 AnimeCrawler-0.2.0/setup.cfg
+-rw-rw-rw-   0        0        0      882 2023-05-06 14:21:26.000000 AnimeCrawler-0.2.0/setup.py
+drwxrwxrwx   0        0        0        0 2023-05-06 14:39:52.503279 AnimeCrawler-0.2.0/test/
+-rw-rw-rw-   0        0        0      264 2023-05-03 13:36:29.000000 AnimeCrawler-0.2.0/test/test.py
```

### Comparing `AnimeCrawler-0.1.2/AnimeCrawler/base_spider.py` & `AnimeCrawler-0.2.0/AnimeCrawler/base_spider.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,32 +1,48 @@
 import urllib.parse
 
-from ruia import Spider
+from ruia import Request, Spider
 
 from AnimeCrawler.log import get_logger
 from AnimeCrawler.utils import is_url
 
 
 class BaseSpider(Spider):
+    '''æ‰€æœ‰çˆ¬è™«çš„åŸºç±»ï¼Œå¿…é¡»æä¾›domainä¸downloaderå±æ€§
+
+    Args:
+        Spider (ruia.Spider): ç»§æ‰¿äºruiaæ¡†æ¶çš„çˆ¬è™«
+
+    Raises:
+        ValueError: æœªå®šä¹‰domainæ—¶æŠ¥é”™
+        ValueError: æœªå®šä¹‰downloaderæ—¶æŠ¥é”™
+
+    Returns:
+        cls: ä¸ºäº†é“¾å¼è°ƒç”¨ruia.Spider.start()
+    '''
+
     logger = get_logger('Spider')
 
+    def __init__(self, *args, **spider_kwargs) -> None:
+        super().__init__(*args, **spider_kwargs)
+        if not hasattr(self, 'domain'):
+            raise ValueError(f'{self.__class__.__name__} æœªå®šä¹‰domainå±æ€§')
+        elif not hasattr(self, 'downloader'):
+            raise ValueError(f'{self.__class__.__name__} æœªå®šä¹‰downloaderä¸‹è½½å™¨')
+
     @classmethod
     def init(cls):
-        if not hasattr(cls, 'domain'):
-            raise ValueError(f'{cls.__name__} æœªå®šä¹‰domainå±æ€§')
-        if not hasattr(cls, 'downloader'):
-            raise ValueError(f'{cls.__name__} æœªå®šä¹‰downloaderä¸‹è½½å™¨')
         cls.start_urls = [
             i if is_url(i) else urllib.parse.urljoin(cls.domain, i)
             for i in cls.start_urls
         ]  # å½“urlä¸ºç›¸å¯¹è·¯å¾„æ—¶ä¸åŸŸåæ‹¼æ¥
         return cls
 
-    async def urljoin(self, base, url, avoid_collision: bool = False) -> str:
-        '''å¯¹urllib.parse.urljoin()çš„å¼‚æ­¥åŒ…è£…
+    def urljoin(self, base, url, avoid_collision: bool = False) -> str:
+        '''å¯¹urllib.parse.urljoin()çš„åŒ…è£…
 
         Args:
             base (str): åŸºç¡€url
             url (str): è¦æ‹¼æ¥çš„url
             avoid_collision (bool, optional): é¿å…å†²çªï¼Œ
 
             è¯¦æƒ…çœ‹ï¼šhttps://docs.python.org/zh-cn/3/library/urllib.parse.html#urllib.parse.urljoin
@@ -37,31 +53,31 @@
             str: æ‹¼æ¥åçš„url
         '''
         if avoid_collision:
             url_parts = urllib.parse.urlsplit(url)
             url = urllib.parse.urlunsplit(url_parts._replace(scheme='', netloc=''))
         return urllib.parse.urljoin(base, url)
 
-    async def follow(self, next_url: str = None, **kwargs):
+    async def follow(self, next_url: str = None, **kwargs) -> Request:
         '''çˆ¬å–ä¸‹ä¸€ä¸ªé¡µé¢
 
         Args:
             next_url (str, optional): ä¸‹ä¸€ä¸ªurlçš„ç›¸å¯¹è·¯å¾„. Defaults to None.
 
         Returns:
             ruia.Request: å¯è¢«yield
         '''
-        return self.request(await self.urljoin(self.domain, next_url), **kwargs)
+        return self.request(self.urljoin(self.domain, next_url), **kwargs)
 
-    def get_domain(self, url: str):
+    def get_domain(self, url: str) -> str:
         url_parts = urllib.parse.urlsplit(url)
         print(url)
         return '://'.join(url_parts[:2])  # e.g. https://docs.python.org/
 
-    def get_path(self, url: str):
+    def get_path(self, url: str) -> str:
         url_parts = urllib.parse.urlsplit(url)
         return url_parts.path
 
 
 if __name__ == '__main__':
     BaseSpider().get_domain(
         'https://docs.python.org/zh-cn/3/library/urllib.parse.html#urllib.parse.urljoin'
```

### Comparing `AnimeCrawler-0.1.2/AnimeCrawler/mhyyy/downloader.py` & `AnimeCrawler-0.2.0/AnimeCrawler/mhyyy/downloader.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,47 +1,48 @@
 import asyncio
+from typing import Any, Union
 
 import aiohttp
 import tqdm.asyncio
 
 from AnimeCrawler.log import get_logger
 from AnimeCrawler.utils import write
 
 
 class Downloader:
     session = None
     logger = get_logger('Downloader')
 
     @property
-    def current_session(self):
+    def current_session(self) -> aiohttp.ClientSession:
         if not self.session:
             self.session = aiohttp.ClientSession(
-                connector = aiohttp.TCPConnector(verify_ssl=False)
+                connector=aiohttp.TCPConnector(verify_ssl=False)
             )
             self.logger.debug('åˆ›å»ºäº†session')
         return self.session
 
     @property
-    def set_url(self, urls):
+    def set_url(self, urls) -> str:
         return urls
 
     @set_url.setter
-    def set_url(self, urls):
+    def set_url(self, urls) -> None:
         self.urls = urls
 
-    async def close_session(self):
+    async def close_session(self) -> None:
         await self.current_session.close()
 
     async def get_ts_file(
         self,
         session: aiohttp.ClientSession,
         title: str,
         url: str,
         error_times: int = 1,
-    ):
+    ) -> Union[tuple[bytes, str], Any]:
         resp = await session.get(
             url=url,
             headers={'User-Agent': 'Mozilla/5.0', ' Transfer-Encoding': 'chunked'},
         )
         try:
             text = await resp.content.read()
             await asyncio.sleep(0)
```

### Comparing `AnimeCrawler-0.1.2/AnimeCrawler/mhyyy/spider.py` & `AnimeCrawler-0.2.0/AnimeCrawler/mhyyy/spider.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import re
 from pathlib import Path
+from typing import AsyncGenerator
 
-from ruia import Item, Response, TextField
+from ruia import Item, Request, Response, TextField
 
 from AnimeCrawler.base_spider import BaseSpider
-from AnimeCrawler.log import get_logger
 from AnimeCrawler.utils import (
     base64_decode,
     folder_path,
     get_video_path,
     merge_ts2mp4,
     unescape,
     write,
@@ -26,17 +26,18 @@
 
 
 class AnimeSpider(BaseSpider):
     _base_ts_url = None
     _mixed_m3u8 = None
     downloader = Downloader()
     headers = {'User-Agent': 'Mozilla/5.0'}
+    concurrency: int = 5
 
     @classmethod
-    def init(cls, anime_title: str, urls: str, del_ts: bool = False):
+    def init(cls, anime_title: str, urls: str, del_ts: bool = False) -> BaseSpider:
         '''åˆå§‹åŒ–çˆ¬è™«
 
         Args:
             anime_title (str): åŠ¨æ¼«æ ‡é¢˜ï¼Œç”¨äºå–æ–‡ä»¶å¤¹çš„åç§°ï¼Œåˆ©äºç®¡ç†åŠ¨æ¼«
             urls (str): ç¬¬ä¸€é¡µçš„ç½‘å€
 
         Returns:
@@ -49,38 +50,40 @@
         return super().init()
 
     async def _mixed_m3u8_url_parse(self, index_m3u8_url: str, item: AnimeItem) -> None:
         resp = await self.request(index_m3u8_url).fetch()
         text = await resp.text()
         if self._mixed_m3u8 is None:
             self._mixed_m3u8 = text.split('\n')[-1]
-        item.mixed_m3u8_url = await self.urljoin(item._base_m3u8_url, self._mixed_m3u8)
+        item.mixed_m3u8_url = self.urljoin(item._base_m3u8_url, self._mixed_m3u8)
 
     def _parse_mixed_m3u8(self, item: AnimeItem):
         '''è§£æmixed.m3u8æ–‡ä»¶ï¼Œè·å¾—tsæ–‡ä»¶ä¸‹è½½åœ°å€
 
         Returns:
             strï¼šts_url
         '''
         base_ts_file = item.mixed_m3u8_url[:-10]
         with open(self.PATH / f'{item.episodes}\\mixed.m3u8', 'r') as fp:
             for i in fp:
                 if '#' not in i:
                     yield base_ts_file + i
 
-    async def have_next_page(self, link_next: str):
+    async def have_next_page(self, link_next: str) -> Request:
         # å½“æœ‰ä¸‹ä¸€é¡µæ—¶
         link_next = link_next.replace('\\', '')
         return await self.follow(
-            await self.urljoin(self.domain, link_next),
+            link_next,
             callback=self.parse,
             headers=self.headers,
         )
 
-    async def parse(self, response: Response):
+    async def parse(
+        self, response: Response
+    ) -> AsyncGenerator[Request | AnimeItem, None]:
         async for item in AnimeItem.get_items(html=await response.text()):
             profile = item.profile
             player_aaaa = eval(re.search('{.*}', profile).group())
             item.episodes = re.findall('\d+', response.url)[2]
             encoded_url = player_aaaa['url']
             index_m3u8_url = unescape(
                 base64_decode(encoded_url)
@@ -89,27 +92,23 @@
             await self._mixed_m3u8_url_parse(index_m3u8_url, item)
 
             link_next = player_aaaa.get('link_next', None)
             if link_next:
                 yield await self.have_next_page(link_next)
             yield item
 
-    async def process_item(self, item: AnimeItem):
+    async def process_item(self, item: AnimeItem) -> None:
         resp = await self.request(item.mixed_m3u8_url, headers=self.headers).fetch()
         text = await resp.text()
         episodes = item.episodes
         folder_path = self.PATH / f'{episodes}'
         self.logger.info('\033[0;32;40må†™å…¥mixed.m3u8\033[0m')
         await write(folder_path, text, 'mixed', 'm3u8', 'w+')
         urls = self._parse_mixed_m3u8(item)
         self.downloader.set_url = urls
         await self.downloader.download_ts_files(folder_path, episodes)
         self.logger.info(f"æ­£åœ¨æŠŠç¬¬ {episodes} é›†çš„tsæ–‡ä»¶è½¬ç æˆ mp4")
         await merge_ts2mp4(folder_path, episodes, self.del_ts)
 
-    async def stop(self, _signal):
+    async def stop(self, _signal) -> None:
         await self.downloader.close_session()
         return await super().stop(_signal)
-
-
-if __name__ == '__main__':
-    AnimeSpider.init('é­”å¥³ä¹‹æ—…', ['https://www.mhyyy.com/play/166269-1-1.html']).start()
```

### Comparing `AnimeCrawler-0.1.2/AnimeCrawler/utils/decode.py` & `AnimeCrawler-0.2.0/AnimeCrawler/utils/decode.py`

 * *Files identical despite different names*

### Comparing `AnimeCrawler-0.1.2/AnimeCrawler.egg-info/PKG-INFO` & `AnimeCrawler-0.2.0/AnimeCrawler.egg-info/PKG-INFO`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: AnimeCrawler
-Version: 0.1.2
+Version: 0.2.0
 Summary: ä¸€ä¸ªå¯å…è´¹ä¸‹è½½åŠ¨æ¼«çš„çˆ¬è™«
 Home-page: https://github.com/Senvlin/AnimeCrawler
 Author: Senvlin
 Author-email: 2994515402@qq.com
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
@@ -27,16 +27,16 @@
 
 
 è¿™æ˜¯ä¸€æ¬¾åœ¨Windowså¹³å°ä¸‹åŸºäºè½»é‡çº§æ¡†æ¶<code>[**Ruia**](https://github.com/howie6879/ruia)</code>ã€ä¸“é—¨çˆ¬å–å…è´¹åŠ¨æ¼«çš„çˆ¬è™«ï¼Œåº•å±‚ä½¿ç”¨<code>aiohttp</code>åº“ï¼Œä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼Œå¤§å¹…å¢åŠ çˆ¬å–åŠä¸‹è½½é€Ÿåº¦ã€‚å¯**ç¦»çº¿æ’­æ”¾**å„å¤§åŠ¨æ¼«ï¼Œæ”¯æŒ**å‘½ä»¤è¡Œè¾“å…¥**ï¼Œç«‹å¿—æˆä¸ºæœ€å®ç”¨ï¼Œæœ€è½»é‡çš„åŠ¨æ¼«ç®¡ç†åŠä¸‹è½½åŠ©æ‰‹
 
 ## â“å¦‚ä½•ä½¿ç”¨
 1. é¦–å…ˆæ¥åˆ°[è¿™é‡Œ](https://www.python.org/downloads)ä¸‹è½½Pythonè§£é‡Šå™¨, è¦æ±‚Python3.8åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œå®‰è£…å³å¯
 2. ç„¶åï¼Œæ‰“å¼€å‘½ä»¤æç¤ºç¬¦ï¼Œè¾“å…¥ <code>pip install AnimeCrawler</code>
-3. å…¶æ¬¡ï¼Œæ¥åˆ°[è¿™ä¸ªç½‘ç«™](https://www.mhyyy.com/),æœç´¢æ‚¨å–œæ¬¢çš„åŠ¨æ¼«ï¼Œç‚¹å‡»æ’­æ”¾é¡µï¼Œå°†ç¬¬ä¸€é¡µçš„urlå¤åˆ¶ä¸€ä¸‹
-4. æœ€åï¼Œæ‰“å¼€å‘½ä»¤æç¤ºç¬¦ï¼Œè¾“å…¥ <code>AnimeCrawler download -t "åŠ¨æ¼«åç§°" -u "url"</code> å›è½¦å°±å¯ä»¥ä¸‹è½½å•¦
+3. å…¶æ¬¡ï¼Œè¾“å…¥ <code>AnimeCrawler search -t "åŠ¨æ¼«æ ‡é¢˜"</code>ï¼Œæ¥æœç´¢åŠ¨æ¼«
+4. æœ€åï¼Œå¤åˆ¶è¾“å‡ºçš„ä¸‹è½½å‘½ä»¤ï¼Œç²˜è´´å›è½¦å°±å¯ä»¥ä¸‹è½½å•¦
     - è¾“å…¥ <code>AnimeCrawler -h</code> ä¼šæœ‰è¯¦ç»†çš„è¯´æ˜
 > ä¸‹è½½åçš„æ–‡ä»¶åœ¨æ‚¨çš„è§†é¢‘æ–‡ä»¶å¤¹é‡Œ
 
 å¦‚æœæ‚¨æƒ³ä½“éªŒæœ€æ–°çš„åŠŸèƒ½ï¼Œè¯·è½¬åˆ°devåˆ†æ”¯~
 
 ## ğŸš€æˆ‘æƒ³å¸®å¿™
 ååˆ†æ„Ÿè°¢æ‚¨æœ‰è¿™ä¸ªæƒ³æ³•ã€‚è¿™ä¸ªé¡¹ç›®ä»åœ¨é’æ¶©å¹´åï¼Œæ€»ä¼šæœ‰ä¸€äº›è·Œè·Œæ’æ’çš„æ—¶å€™ï¼Œä¹Ÿè®¸æ‚¨çš„ä¸¾æ‰‹ä¹‹åŠ³ï¼Œèƒ½é€ å°±æ›´å¥½çš„å®ƒ
@@ -53,15 +53,15 @@
 - [ ] å¯æ›´æ¢ä¸‹è½½æº
 - [ ] æ”¯æŒä¸Šä¼ ç½‘ç›˜
 - [ ] <span style="text-decoration: line-through">ç”šè‡³æ˜¯GUI</span>
 
 ## ğŸ§±ä»æºç æ­å»º
 1. ç‚¹å‡»[è¿™é‡Œ](https://github.com/Senvlin/AnimeCrawler/releases)æ‰¾åˆ°æœ€æ–°ç‰ˆæœ¬ï¼Œä¸‹è½½æºç 
 2. è½¬åˆ°é¡¹ç›®ç›®å½•ï¼Œä½¿ç”¨ <code>pip install -r requirement.txt</code> å®‰è£…ä¾èµ–åº“
-3. éšåä½¿ç”¨ <code>python -m AnimeCrawler "åŠ¨æ¼«åç§°" "url"</code> æˆ–æ‰“å¼€ <code>AnimeCrawler\mhyyy\spider.py</code>è¿è¡Œå³å¯
+3. éšåä½¿ç”¨ <code>python -m AnimeCrawler download -t "åŠ¨æ¼«æ ‡é¢˜" -u "URL"</code> è¿è¡Œå³å¯
 
 ## â— å£°æ˜
 æ­¤é¡¹ç›®åªå› ä¸ªäººå…´è¶£è€Œå¼€å‘ï¼Œä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œæ— ä»»ä½•å•†ä¸šç”¨é€”
 
 ä¸‹è½½çš„èµ„æºå‡æ¥è‡ªå¯æœç´¢åˆ°çš„ã€å„ç½‘ç«™æä¾›çš„å…¬å¼€å¼•ç”¨èµ„æºï¼Œæ‰€æœ‰è§†é¢‘ç‰ˆæƒå‡å½’åŸä½œè€…åŠç½‘ç«™æ‰€æœ‰
 
 æ‚¨åº”è¯¥è‡ªè¡Œæ‰¿æ‹…ä½¿ç”¨æ­¤é¡¹ç›®æœ‰å¯èƒ½çš„é£é™©ï¼Œæˆ‘ä¸ä¿è¯æ‚¨ä¸‹è½½çš„èµ„æºçš„å®‰å…¨æ€§ï¼Œåˆæ³•æ€§ï¼Œå…¬æ­£æ€§ã€‚ç½‘ç»œä¿¡æ¯è‰¯è ä¸é½ï¼Œè¯·è‡ªè¡Œç”„åˆ«ï¼Œè°¢è°¢
```

### Comparing `AnimeCrawler-0.1.2/AnimeCrawler.egg-info/SOURCES.txt` & `AnimeCrawler-0.2.0/AnimeCrawler.egg-info/SOURCES.txt`

 * *Files 12% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 LICENSE
 README.md
 setup.py
 AnimeCrawler/__init__.py
-AnimeCrawler/__main__.py
 AnimeCrawler/base_spider.py
 AnimeCrawler/log.py
 AnimeCrawler.egg-info/PKG-INFO
 AnimeCrawler.egg-info/SOURCES.txt
 AnimeCrawler.egg-info/dependency_links.txt
 AnimeCrawler.egg-info/entry_points.txt
 AnimeCrawler.egg-info/requires.txt
 AnimeCrawler.egg-info/top_level.txt
 AnimeCrawler/command/__init__.py
 AnimeCrawler/command/run.py
 AnimeCrawler/mhyyy/__init__.py
 AnimeCrawler/mhyyy/downloader.py
+AnimeCrawler/mhyyy/searcher.py
 AnimeCrawler/mhyyy/spider.py
 AnimeCrawler/utils/__init__.py
 AnimeCrawler/utils/decode.py
-AnimeCrawler/utils/file.py
+AnimeCrawler/utils/file.py
+test/test.py
```

### Comparing `AnimeCrawler-0.1.2/LICENSE` & `AnimeCrawler-0.2.0/LICENSE`

 * *Files identical despite different names*

### Comparing `AnimeCrawler-0.1.2/PKG-INFO` & `AnimeCrawler-0.2.0/PKG-INFO`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: AnimeCrawler
-Version: 0.1.2
+Version: 0.2.0
 Summary: ä¸€ä¸ªå¯å…è´¹ä¸‹è½½åŠ¨æ¼«çš„çˆ¬è™«
 Home-page: https://github.com/Senvlin/AnimeCrawler
 Author: Senvlin
 Author-email: 2994515402@qq.com
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
@@ -27,16 +27,16 @@
 
 
 è¿™æ˜¯ä¸€æ¬¾åœ¨Windowså¹³å°ä¸‹åŸºäºè½»é‡çº§æ¡†æ¶<code>[**Ruia**](https://github.com/howie6879/ruia)</code>ã€ä¸“é—¨çˆ¬å–å…è´¹åŠ¨æ¼«çš„çˆ¬è™«ï¼Œåº•å±‚ä½¿ç”¨<code>aiohttp</code>åº“ï¼Œä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼Œå¤§å¹…å¢åŠ çˆ¬å–åŠä¸‹è½½é€Ÿåº¦ã€‚å¯**ç¦»çº¿æ’­æ”¾**å„å¤§åŠ¨æ¼«ï¼Œæ”¯æŒ**å‘½ä»¤è¡Œè¾“å…¥**ï¼Œç«‹å¿—æˆä¸ºæœ€å®ç”¨ï¼Œæœ€è½»é‡çš„åŠ¨æ¼«ç®¡ç†åŠä¸‹è½½åŠ©æ‰‹
 
 ## â“å¦‚ä½•ä½¿ç”¨
 1. é¦–å…ˆæ¥åˆ°[è¿™é‡Œ](https://www.python.org/downloads)ä¸‹è½½Pythonè§£é‡Šå™¨, è¦æ±‚Python3.8åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œå®‰è£…å³å¯
 2. ç„¶åï¼Œæ‰“å¼€å‘½ä»¤æç¤ºç¬¦ï¼Œè¾“å…¥ <code>pip install AnimeCrawler</code>
-3. å…¶æ¬¡ï¼Œæ¥åˆ°[è¿™ä¸ªç½‘ç«™](https://www.mhyyy.com/),æœç´¢æ‚¨å–œæ¬¢çš„åŠ¨æ¼«ï¼Œç‚¹å‡»æ’­æ”¾é¡µï¼Œå°†ç¬¬ä¸€é¡µçš„urlå¤åˆ¶ä¸€ä¸‹
-4. æœ€åï¼Œæ‰“å¼€å‘½ä»¤æç¤ºç¬¦ï¼Œè¾“å…¥ <code>AnimeCrawler download -t "åŠ¨æ¼«åç§°" -u "url"</code> å›è½¦å°±å¯ä»¥ä¸‹è½½å•¦
+3. å…¶æ¬¡ï¼Œè¾“å…¥ <code>AnimeCrawler search -t "åŠ¨æ¼«æ ‡é¢˜"</code>ï¼Œæ¥æœç´¢åŠ¨æ¼«
+4. æœ€åï¼Œå¤åˆ¶è¾“å‡ºçš„ä¸‹è½½å‘½ä»¤ï¼Œç²˜è´´å›è½¦å°±å¯ä»¥ä¸‹è½½å•¦
     - è¾“å…¥ <code>AnimeCrawler -h</code> ä¼šæœ‰è¯¦ç»†çš„è¯´æ˜
 > ä¸‹è½½åçš„æ–‡ä»¶åœ¨æ‚¨çš„è§†é¢‘æ–‡ä»¶å¤¹é‡Œ
 
 å¦‚æœæ‚¨æƒ³ä½“éªŒæœ€æ–°çš„åŠŸèƒ½ï¼Œè¯·è½¬åˆ°devåˆ†æ”¯~
 
 ## ğŸš€æˆ‘æƒ³å¸®å¿™
 ååˆ†æ„Ÿè°¢æ‚¨æœ‰è¿™ä¸ªæƒ³æ³•ã€‚è¿™ä¸ªé¡¹ç›®ä»åœ¨é’æ¶©å¹´åï¼Œæ€»ä¼šæœ‰ä¸€äº›è·Œè·Œæ’æ’çš„æ—¶å€™ï¼Œä¹Ÿè®¸æ‚¨çš„ä¸¾æ‰‹ä¹‹åŠ³ï¼Œèƒ½é€ å°±æ›´å¥½çš„å®ƒ
@@ -53,15 +53,15 @@
 - [ ] å¯æ›´æ¢ä¸‹è½½æº
 - [ ] æ”¯æŒä¸Šä¼ ç½‘ç›˜
 - [ ] <span style="text-decoration: line-through">ç”šè‡³æ˜¯GUI</span>
 
 ## ğŸ§±ä»æºç æ­å»º
 1. ç‚¹å‡»[è¿™é‡Œ](https://github.com/Senvlin/AnimeCrawler/releases)æ‰¾åˆ°æœ€æ–°ç‰ˆæœ¬ï¼Œä¸‹è½½æºç 
 2. è½¬åˆ°é¡¹ç›®ç›®å½•ï¼Œä½¿ç”¨ <code>pip install -r requirement.txt</code> å®‰è£…ä¾èµ–åº“
-3. éšåä½¿ç”¨ <code>python -m AnimeCrawler "åŠ¨æ¼«åç§°" "url"</code> æˆ–æ‰“å¼€ <code>AnimeCrawler\mhyyy\spider.py</code>è¿è¡Œå³å¯
+3. éšåä½¿ç”¨ <code>python -m AnimeCrawler download -t "åŠ¨æ¼«æ ‡é¢˜" -u "URL"</code> è¿è¡Œå³å¯
 
 ## â— å£°æ˜
 æ­¤é¡¹ç›®åªå› ä¸ªäººå…´è¶£è€Œå¼€å‘ï¼Œä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œæ— ä»»ä½•å•†ä¸šç”¨é€”
 
 ä¸‹è½½çš„èµ„æºå‡æ¥è‡ªå¯æœç´¢åˆ°çš„ã€å„ç½‘ç«™æä¾›çš„å…¬å¼€å¼•ç”¨èµ„æºï¼Œæ‰€æœ‰è§†é¢‘ç‰ˆæƒå‡å½’åŸä½œè€…åŠç½‘ç«™æ‰€æœ‰
 
 æ‚¨åº”è¯¥è‡ªè¡Œæ‰¿æ‹…ä½¿ç”¨æ­¤é¡¹ç›®æœ‰å¯èƒ½çš„é£é™©ï¼Œæˆ‘ä¸ä¿è¯æ‚¨ä¸‹è½½çš„èµ„æºçš„å®‰å…¨æ€§ï¼Œåˆæ³•æ€§ï¼Œå…¬æ­£æ€§ã€‚ç½‘ç»œä¿¡æ¯è‰¯è ä¸é½ï¼Œè¯·è‡ªè¡Œç”„åˆ«ï¼Œè°¢è°¢
```

### Comparing `AnimeCrawler-0.1.2/README.md` & `AnimeCrawler-0.2.0/README.md`

 * *Files 10% similar despite different names*

```diff
@@ -13,16 +13,16 @@
 
 
 è¿™æ˜¯ä¸€æ¬¾åœ¨Windowså¹³å°ä¸‹åŸºäºè½»é‡çº§æ¡†æ¶<code>[**Ruia**](https://github.com/howie6879/ruia)</code>ã€ä¸“é—¨çˆ¬å–å…è´¹åŠ¨æ¼«çš„çˆ¬è™«ï¼Œåº•å±‚ä½¿ç”¨<code>aiohttp</code>åº“ï¼Œä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼Œå¤§å¹…å¢åŠ çˆ¬å–åŠä¸‹è½½é€Ÿåº¦ã€‚å¯**ç¦»çº¿æ’­æ”¾**å„å¤§åŠ¨æ¼«ï¼Œæ”¯æŒ**å‘½ä»¤è¡Œè¾“å…¥**ï¼Œç«‹å¿—æˆä¸ºæœ€å®ç”¨ï¼Œæœ€è½»é‡çš„åŠ¨æ¼«ç®¡ç†åŠä¸‹è½½åŠ©æ‰‹
 
 ## â“å¦‚ä½•ä½¿ç”¨
 1. é¦–å…ˆæ¥åˆ°[è¿™é‡Œ](https://www.python.org/downloads)ä¸‹è½½Pythonè§£é‡Šå™¨, è¦æ±‚Python3.8åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œå®‰è£…å³å¯
 2. ç„¶åï¼Œæ‰“å¼€å‘½ä»¤æç¤ºç¬¦ï¼Œè¾“å…¥ <code>pip install AnimeCrawler</code>
-3. å…¶æ¬¡ï¼Œæ¥åˆ°[è¿™ä¸ªç½‘ç«™](https://www.mhyyy.com/),æœç´¢æ‚¨å–œæ¬¢çš„åŠ¨æ¼«ï¼Œç‚¹å‡»æ’­æ”¾é¡µï¼Œå°†ç¬¬ä¸€é¡µçš„urlå¤åˆ¶ä¸€ä¸‹
-4. æœ€åï¼Œæ‰“å¼€å‘½ä»¤æç¤ºç¬¦ï¼Œè¾“å…¥ <code>AnimeCrawler download -t "åŠ¨æ¼«åç§°" -u "url"</code> å›è½¦å°±å¯ä»¥ä¸‹è½½å•¦
+3. å…¶æ¬¡ï¼Œè¾“å…¥ <code>AnimeCrawler search -t "åŠ¨æ¼«æ ‡é¢˜"</code>ï¼Œæ¥æœç´¢åŠ¨æ¼«
+4. æœ€åï¼Œå¤åˆ¶è¾“å‡ºçš„ä¸‹è½½å‘½ä»¤ï¼Œç²˜è´´å›è½¦å°±å¯ä»¥ä¸‹è½½å•¦
     - è¾“å…¥ <code>AnimeCrawler -h</code> ä¼šæœ‰è¯¦ç»†çš„è¯´æ˜
 > ä¸‹è½½åçš„æ–‡ä»¶åœ¨æ‚¨çš„è§†é¢‘æ–‡ä»¶å¤¹é‡Œ
 
 å¦‚æœæ‚¨æƒ³ä½“éªŒæœ€æ–°çš„åŠŸèƒ½ï¼Œè¯·è½¬åˆ°devåˆ†æ”¯~
 
 ## ğŸš€æˆ‘æƒ³å¸®å¿™
 ååˆ†æ„Ÿè°¢æ‚¨æœ‰è¿™ä¸ªæƒ³æ³•ã€‚è¿™ä¸ªé¡¹ç›®ä»åœ¨é’æ¶©å¹´åï¼Œæ€»ä¼šæœ‰ä¸€äº›è·Œè·Œæ’æ’çš„æ—¶å€™ï¼Œä¹Ÿè®¸æ‚¨çš„ä¸¾æ‰‹ä¹‹åŠ³ï¼Œèƒ½é€ å°±æ›´å¥½çš„å®ƒ
@@ -39,15 +39,15 @@
 - [ ] å¯æ›´æ¢ä¸‹è½½æº
 - [ ] æ”¯æŒä¸Šä¼ ç½‘ç›˜
 - [ ] <span style="text-decoration: line-through">ç”šè‡³æ˜¯GUI</span>
 
 ## ğŸ§±ä»æºç æ­å»º
 1. ç‚¹å‡»[è¿™é‡Œ](https://github.com/Senvlin/AnimeCrawler/releases)æ‰¾åˆ°æœ€æ–°ç‰ˆæœ¬ï¼Œä¸‹è½½æºç 
 2. è½¬åˆ°é¡¹ç›®ç›®å½•ï¼Œä½¿ç”¨ <code>pip install -r requirement.txt</code> å®‰è£…ä¾èµ–åº“
-3. éšåä½¿ç”¨ <code>python -m AnimeCrawler "åŠ¨æ¼«åç§°" "url"</code> æˆ–æ‰“å¼€ <code>AnimeCrawler\mhyyy\spider.py</code>è¿è¡Œå³å¯
+3. éšåä½¿ç”¨ <code>python -m AnimeCrawler download -t "åŠ¨æ¼«æ ‡é¢˜" -u "URL"</code> è¿è¡Œå³å¯
 
 ## â— å£°æ˜
 æ­¤é¡¹ç›®åªå› ä¸ªäººå…´è¶£è€Œå¼€å‘ï¼Œä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œæ— ä»»ä½•å•†ä¸šç”¨é€”
 
 ä¸‹è½½çš„èµ„æºå‡æ¥è‡ªå¯æœç´¢åˆ°çš„ã€å„ç½‘ç«™æä¾›çš„å…¬å¼€å¼•ç”¨èµ„æºï¼Œæ‰€æœ‰è§†é¢‘ç‰ˆæƒå‡å½’åŸä½œè€…åŠç½‘ç«™æ‰€æœ‰
 
 æ‚¨åº”è¯¥è‡ªè¡Œæ‰¿æ‹…ä½¿ç”¨æ­¤é¡¹ç›®æœ‰å¯èƒ½çš„é£é™©ï¼Œæˆ‘ä¸ä¿è¯æ‚¨ä¸‹è½½çš„èµ„æºçš„å®‰å…¨æ€§ï¼Œåˆæ³•æ€§ï¼Œå…¬æ­£æ€§ã€‚ç½‘ç»œä¿¡æ¯è‰¯è ä¸é½ï¼Œè¯·è‡ªè¡Œç”„åˆ«ï¼Œè°¢è°¢
```

### Comparing `AnimeCrawler-0.1.2/setup.py` & `AnimeCrawler-0.2.0/setup.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,28 +1,28 @@
 import setuptools
 
 with open("README.md", "r", encoding='utf-8') as fh:
     long_description = fh.read()
 
 setuptools.setup(
     name="AnimeCrawler",
-    version="v0.1.2",
+    version="v0.2.0",
     author="Senvlin",
     author_email="2994515402@qq.com",
     description="ä¸€ä¸ªå¯å…è´¹ä¸‹è½½åŠ¨æ¼«çš„çˆ¬è™«",
     long_description=long_description,
     long_description_content_type="text/markdown",
     url="https://github.com/Senvlin/AnimeCrawler",
     packages=setuptools.find_packages(),
     classifiers=[
         "Programming Language :: Python :: 3",
         "License :: OSI Approved :: MIT License",
         "Operating System :: OS Independent",
     ],
-    install_requires=['ruia', 'tqdm', 'aiofiles', 'aiohttp'],
+    install_requires=['ruia', 'tqdm', 'aiofiles', 'wheel'],
     entry_points={
         'console_scripts': [
             'animecrawler = AnimeCrawler.command.run:main',
         ],
     },
     python_requires='>=3.8',
 )
```

