# Comparing `tmp/deephaven_core-0.23.0-py3-none-any.whl.zip` & `tmp/deephaven_core-0.24.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,75 +1,75 @@
-Zip file size: 124714 bytes, number of entries: 73
--rw-r--r--  2.0 unx      827 b- defN 23-Mar-31 22:05 deephaven/__init__.py
--rw-r--r--  2.0 unx      970 b- defN 23-Mar-31 22:05 deephaven/_gc.py
--rw-r--r--  2.0 unx      676 b- defN 23-Mar-31 22:05 deephaven/_jpy.py
--rw-r--r--  2.0 unx     4551 b- defN 23-Mar-31 22:05 deephaven/_wrapper.py
--rw-r--r--  2.0 unx    10817 b- defN 23-Mar-31 22:05 deephaven/agg.py
--rw-r--r--  2.0 unx     1987 b- defN 23-Mar-31 22:05 deephaven/appmode.py
--rw-r--r--  2.0 unx     4727 b- defN 23-Mar-31 22:05 deephaven/arrow.py
--rw-r--r--  2.0 unx    14670 b- defN 23-Mar-31 22:05 deephaven/calendar.py
--rw-r--r--  2.0 unx     7157 b- defN 23-Mar-31 22:05 deephaven/column.py
--rw-r--r--  2.0 unx     3123 b- defN 23-Mar-31 22:05 deephaven/constants.py
--rw-r--r--  2.0 unx     4515 b- defN 23-Mar-31 22:05 deephaven/csv.py
--rw-r--r--  2.0 unx     2890 b- defN 23-Mar-31 22:05 deephaven/dherror.py
--rw-r--r--  2.0 unx     8142 b- defN 23-Mar-31 22:05 deephaven/dtypes.py
--rw-r--r--  2.0 unx     3517 b- defN 23-Mar-31 22:05 deephaven/execution_context.py
--rw-r--r--  2.0 unx     3186 b- defN 23-Mar-31 22:05 deephaven/filters.py
--rw-r--r--  2.0 unx      650 b- defN 23-Mar-31 22:05 deephaven/html.py
--rw-r--r--  2.0 unx     4696 b- defN 23-Mar-31 22:05 deephaven/jcompat.py
--rw-r--r--  2.0 unx     2503 b- defN 23-Mar-31 22:05 deephaven/liveness_scope.py
--rw-r--r--  2.0 unx     6109 b- defN 23-Mar-31 22:05 deephaven/numpy.py
--rw-r--r--  2.0 unx     5110 b- defN 23-Mar-31 22:05 deephaven/pandas.py
--rw-r--r--  2.0 unx     9173 b- defN 23-Mar-31 22:05 deephaven/parquet.py
--rw-r--r--  2.0 unx     8081 b- defN 23-Mar-31 22:05 deephaven/perfmon.py
--rw-r--r--  2.0 unx     2690 b- defN 23-Mar-31 22:05 deephaven/query_library.py
--rw-r--r--  2.0 unx     2471 b- defN 23-Mar-31 22:05 deephaven/replay.py
--rw-r--r--  2.0 unx   147667 b- defN 23-Mar-31 22:05 deephaven/table.py
--rw-r--r--  2.0 unx    10134 b- defN 23-Mar-31 22:05 deephaven/table_factory.py
--rw-r--r--  2.0 unx    15976 b- defN 23-Mar-31 22:05 deephaven/table_listener.py
--rw-r--r--  2.0 unx    21914 b- defN 23-Mar-31 22:05 deephaven/time.py
--rw-r--r--  2.0 unx     4712 b- defN 23-Mar-31 22:05 deephaven/ugp.py
--rw-r--r--  2.0 unx    40759 b- defN 23-Mar-31 22:05 deephaven/updateby.py
--rw-r--r--  2.0 unx     1155 b- defN 23-Mar-31 22:05 deephaven/uri.py
--rw-r--r--  2.0 unx      842 b- defN 23-Mar-31 22:05 deephaven/config/__init__.py
--rw-r--r--  2.0 unx     3200 b- defN 23-Mar-31 22:05 deephaven/dbc/__init__.py
--rw-r--r--  2.0 unx     1794 b- defN 23-Mar-31 22:05 deephaven/dbc/adbc.py
--rw-r--r--  2.0 unx     1668 b- defN 23-Mar-31 22:05 deephaven/dbc/odbc.py
--rw-r--r--  2.0 unx     1316 b- defN 23-Mar-31 22:05 deephaven/experimental/__init__.py
--rw-r--r--  2.0 unx     4815 b- defN 23-Mar-31 22:05 deephaven/experimental/ema.py
--rw-r--r--  2.0 unx     4097 b- defN 23-Mar-31 22:05 deephaven/experimental/outer_joins.py
--rw-r--r--  2.0 unx     7350 b- defN 23-Mar-31 22:05 deephaven/learn/__init__.py
--rw-r--r--  2.0 unx     3120 b- defN 23-Mar-31 22:05 deephaven/learn/gather.py
--rw-r--r--  2.0 unx      347 b- defN 23-Mar-31 22:05 deephaven/pandasplugin/__init__.py
--rw-r--r--  2.0 unx      550 b- defN 23-Mar-31 22:05 deephaven/pandasplugin/pandas_as_table.py
--rw-r--r--  2.0 unx      574 b- defN 23-Mar-31 22:05 deephaven/plot/__init__.py
--rw-r--r--  2.0 unx     2285 b- defN 23-Mar-31 22:05 deephaven/plot/axisformat.py
--rw-r--r--  2.0 unx     1479 b- defN 23-Mar-31 22:05 deephaven/plot/axistransform.py
--rw-r--r--  2.0 unx    16541 b- defN 23-Mar-31 22:05 deephaven/plot/color.py
--rw-r--r--  2.0 unx   110233 b- defN 23-Mar-31 22:05 deephaven/plot/figure.py
--rw-r--r--  2.0 unx     1659 b- defN 23-Mar-31 22:05 deephaven/plot/font.py
--rw-r--r--  2.0 unx     3514 b- defN 23-Mar-31 22:05 deephaven/plot/linestyle.py
--rw-r--r--  2.0 unx     1285 b- defN 23-Mar-31 22:05 deephaven/plot/plotstyle.py
--rw-r--r--  2.0 unx     2493 b- defN 23-Mar-31 22:05 deephaven/plot/selectable_dataset.py
--rw-r--r--  2.0 unx     1213 b- defN 23-Mar-31 22:05 deephaven/plot/shape.py
--rw-r--r--  2.0 unx      248 b- defN 23-Mar-31 22:05 deephaven/server/__init__.py
--rw-r--r--  2.0 unx      584 b- defN 23-Mar-31 22:05 deephaven/server/plugin/__init__.py
--rw-r--r--  2.0 unx     1098 b- defN 23-Mar-31 22:05 deephaven/server/plugin/register.py
--rw-r--r--  2.0 unx     1800 b- defN 23-Mar-31 22:05 deephaven/server/plugin/object/__init__.py
--rw-r--r--  2.0 unx     1850 b- defN 23-Mar-31 22:05 deephaven/server/script_session/__init__.py
--rw-r--r--  2.0 unx      724 b- defN 23-Mar-31 22:05 deephaven/stream/__init__.py
--rw-r--r--  2.0 unx     1019 b- defN 23-Mar-31 22:05 deephaven/stream/kafka/__init__.py
--rw-r--r--  2.0 unx     7082 b- defN 23-Mar-31 22:05 deephaven/stream/kafka/cdc.py
--rw-r--r--  2.0 unx    17567 b- defN 23-Mar-31 22:05 deephaven/stream/kafka/consumer.py
--rw-r--r--  2.0 unx    10818 b- defN 23-Mar-31 22:05 deephaven/stream/kafka/producer.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-31 22:05 deephaven_internal/__init__.py
--rw-r--r--  2.0 unx     1576 b- defN 23-Mar-31 22:05 deephaven_internal/java_threads.py
--rw-r--r--  2.0 unx     3426 b- defN 23-Mar-31 22:05 deephaven_internal/stream.py
--rw-r--r--  2.0 unx      952 b- defN 23-Mar-31 22:05 deephaven_internal/auto_completer/__init__.py
--rw-r--r--  2.0 unx     8508 b- defN 23-Mar-31 22:05 deephaven_internal/auto_completer/_completer.py
--rw-r--r--  2.0 unx     1724 b- defN 23-Mar-31 22:05 deephaven_internal/jvm/__init__.py
--rw-r--r--  2.0 unx     2650 b- defN 23-Mar-31 22:05 deephaven_core-0.23.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Mar-31 22:05 deephaven_core-0.23.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       87 b- defN 23-Mar-31 22:05 deephaven_core-0.23.0.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       29 b- defN 23-Mar-31 22:05 deephaven_core-0.23.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     6079 b- defN 23-Mar-31 22:05 deephaven_core-0.23.0.dist-info/RECORD
-73 files, 593843 bytes uncompressed, 115184 bytes compressed:  80.6%
+Zip file size: 130102 bytes, number of entries: 73
+-rw-r--r--  2.0 unx     1120 b- defN 23-May-05 23:21 deephaven/__init__.py
+-rw-r--r--  2.0 unx      970 b- defN 23-May-05 23:21 deephaven/_gc.py
+-rw-r--r--  2.0 unx      676 b- defN 23-May-05 23:21 deephaven/_jpy.py
+-rw-r--r--  2.0 unx     4551 b- defN 23-May-05 23:21 deephaven/_wrapper.py
+-rw-r--r--  2.0 unx    10817 b- defN 23-May-05 23:21 deephaven/agg.py
+-rw-r--r--  2.0 unx     1987 b- defN 23-May-05 23:21 deephaven/appmode.py
+-rw-r--r--  2.0 unx     4776 b- defN 23-May-05 23:21 deephaven/arrow.py
+-rw-r--r--  2.0 unx    14670 b- defN 23-May-05 23:21 deephaven/calendar.py
+-rw-r--r--  2.0 unx     7276 b- defN 23-May-05 23:21 deephaven/column.py
+-rw-r--r--  2.0 unx     3123 b- defN 23-May-05 23:21 deephaven/constants.py
+-rw-r--r--  2.0 unx     4515 b- defN 23-May-05 23:21 deephaven/csv.py
+-rw-r--r--  2.0 unx     2890 b- defN 23-May-05 23:21 deephaven/dherror.py
+-rw-r--r--  2.0 unx     9049 b- defN 23-May-05 23:21 deephaven/dtypes.py
+-rw-r--r--  2.0 unx     3517 b- defN 23-May-05 23:21 deephaven/execution_context.py
+-rw-r--r--  2.0 unx     4964 b- defN 23-May-05 23:21 deephaven/filters.py
+-rw-r--r--  2.0 unx      650 b- defN 23-May-05 23:21 deephaven/html.py
+-rw-r--r--  2.0 unx     4696 b- defN 23-May-05 23:21 deephaven/jcompat.py
+-rw-r--r--  2.0 unx     2503 b- defN 23-May-05 23:21 deephaven/liveness_scope.py
+-rw-r--r--  2.0 unx     6109 b- defN 23-May-05 23:21 deephaven/numpy.py
+-rw-r--r--  2.0 unx     8745 b- defN 23-May-05 23:21 deephaven/pandas.py
+-rw-r--r--  2.0 unx     9173 b- defN 23-May-05 23:21 deephaven/parquet.py
+-rw-r--r--  2.0 unx     8081 b- defN 23-May-05 23:21 deephaven/perfmon.py
+-rw-r--r--  2.0 unx     2690 b- defN 23-May-05 23:21 deephaven/query_library.py
+-rw-r--r--  2.0 unx     2471 b- defN 23-May-05 23:21 deephaven/replay.py
+-rw-r--r--  2.0 unx   154549 b- defN 23-May-05 23:21 deephaven/table.py
+-rw-r--r--  2.0 unx    11182 b- defN 23-May-05 23:21 deephaven/table_factory.py
+-rw-r--r--  2.0 unx    15976 b- defN 23-May-05 23:21 deephaven/table_listener.py
+-rw-r--r--  2.0 unx    21914 b- defN 23-May-05 23:21 deephaven/time.py
+-rw-r--r--  2.0 unx     4712 b- defN 23-May-05 23:21 deephaven/ugp.py
+-rw-r--r--  2.0 unx    67886 b- defN 23-May-05 23:21 deephaven/updateby.py
+-rw-r--r--  2.0 unx     1155 b- defN 23-May-05 23:21 deephaven/uri.py
+-rw-r--r--  2.0 unx      842 b- defN 23-May-05 23:21 deephaven/config/__init__.py
+-rw-r--r--  2.0 unx     3200 b- defN 23-May-05 23:21 deephaven/dbc/__init__.py
+-rw-r--r--  2.0 unx     1794 b- defN 23-May-05 23:21 deephaven/dbc/adbc.py
+-rw-r--r--  2.0 unx     1668 b- defN 23-May-05 23:21 deephaven/dbc/odbc.py
+-rw-r--r--  2.0 unx     1316 b- defN 23-May-05 23:21 deephaven/experimental/__init__.py
+-rw-r--r--  2.0 unx     4815 b- defN 23-May-05 23:21 deephaven/experimental/ema.py
+-rw-r--r--  2.0 unx     4097 b- defN 23-May-05 23:21 deephaven/experimental/outer_joins.py
+-rw-r--r--  2.0 unx     7350 b- defN 23-May-05 23:21 deephaven/learn/__init__.py
+-rw-r--r--  2.0 unx     3120 b- defN 23-May-05 23:21 deephaven/learn/gather.py
+-rw-r--r--  2.0 unx      347 b- defN 23-May-05 23:21 deephaven/pandasplugin/__init__.py
+-rw-r--r--  2.0 unx      550 b- defN 23-May-05 23:21 deephaven/pandasplugin/pandas_as_table.py
+-rw-r--r--  2.0 unx      574 b- defN 23-May-05 23:21 deephaven/plot/__init__.py
+-rw-r--r--  2.0 unx     2285 b- defN 23-May-05 23:21 deephaven/plot/axisformat.py
+-rw-r--r--  2.0 unx     1479 b- defN 23-May-05 23:21 deephaven/plot/axistransform.py
+-rw-r--r--  2.0 unx    16541 b- defN 23-May-05 23:21 deephaven/plot/color.py
+-rw-r--r--  2.0 unx   110233 b- defN 23-May-05 23:21 deephaven/plot/figure.py
+-rw-r--r--  2.0 unx     1659 b- defN 23-May-05 23:21 deephaven/plot/font.py
+-rw-r--r--  2.0 unx     3514 b- defN 23-May-05 23:21 deephaven/plot/linestyle.py
+-rw-r--r--  2.0 unx     1285 b- defN 23-May-05 23:21 deephaven/plot/plotstyle.py
+-rw-r--r--  2.0 unx     2493 b- defN 23-May-05 23:21 deephaven/plot/selectable_dataset.py
+-rw-r--r--  2.0 unx     1213 b- defN 23-May-05 23:21 deephaven/plot/shape.py
+-rw-r--r--  2.0 unx      248 b- defN 23-May-05 23:21 deephaven/server/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-May-05 23:21 deephaven/server/plugin/__init__.py
+-rw-r--r--  2.0 unx     1098 b- defN 23-May-05 23:21 deephaven/server/plugin/register.py
+-rw-r--r--  2.0 unx     1800 b- defN 23-May-05 23:21 deephaven/server/plugin/object/__init__.py
+-rw-r--r--  2.0 unx     1850 b- defN 23-May-05 23:21 deephaven/server/script_session/__init__.py
+-rw-r--r--  2.0 unx      724 b- defN 23-May-05 23:21 deephaven/stream/__init__.py
+-rw-r--r--  2.0 unx     1019 b- defN 23-May-05 23:21 deephaven/stream/kafka/__init__.py
+-rw-r--r--  2.0 unx     7082 b- defN 23-May-05 23:21 deephaven/stream/kafka/cdc.py
+-rw-r--r--  2.0 unx    17567 b- defN 23-May-05 23:21 deephaven/stream/kafka/consumer.py
+-rw-r--r--  2.0 unx    10818 b- defN 23-May-05 23:21 deephaven/stream/kafka/producer.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-05 23:21 deephaven_internal/__init__.py
+-rw-r--r--  2.0 unx     1576 b- defN 23-May-05 23:21 deephaven_internal/java_threads.py
+-rw-r--r--  2.0 unx     3426 b- defN 23-May-05 23:21 deephaven_internal/stream.py
+-rw-r--r--  2.0 unx      952 b- defN 23-May-05 23:21 deephaven_internal/auto_completer/__init__.py
+-rw-r--r--  2.0 unx     8476 b- defN 23-May-05 23:21 deephaven_internal/auto_completer/_completer.py
+-rw-r--r--  2.0 unx     2585 b- defN 23-May-05 23:21 deephaven_internal/jvm/__init__.py
+-rw-r--r--  2.0 unx     2650 b- defN 23-May-05 23:21 deephaven_core-0.24.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-05 23:21 deephaven_core-0.24.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       87 b- defN 23-May-05 23:21 deephaven_core-0.24.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       29 b- defN 23-May-05 23:21 deephaven_core-0.24.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6080 b- defN 23-May-05 23:21 deephaven_core-0.24.0.dist-info/RECORD
+73 files, 636511 bytes uncompressed, 120572 bytes compressed:  81.1%
```

## zipnote {}

```diff
@@ -198,23 +198,23 @@
 
 Filename: deephaven_internal/auto_completer/_completer.py
 Comment: 
 
 Filename: deephaven_internal/jvm/__init__.py
 Comment: 
 
-Filename: deephaven_core-0.23.0.dist-info/METADATA
+Filename: deephaven_core-0.24.0.dist-info/METADATA
 Comment: 
 
-Filename: deephaven_core-0.23.0.dist-info/WHEEL
+Filename: deephaven_core-0.24.0.dist-info/WHEEL
 Comment: 
 
-Filename: deephaven_core-0.23.0.dist-info/entry_points.txt
+Filename: deephaven_core-0.24.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: deephaven_core-0.23.0.dist-info/top_level.txt
+Filename: deephaven_core-0.24.0.dist-info/top_level.txt
 Comment: 
 
-Filename: deephaven_core-0.23.0.dist-info/RECORD
+Filename: deephaven_core-0.24.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deephaven/__init__.py

```diff
@@ -3,25 +3,31 @@
 #
 
 """Deephaven Python Integration Package provides the ability to access the Deephaven's query engine natively and thus
 unlocks the unique power of Deephaven to the Python community.
 
 """
 
-__version__ = "0.23.0"
+__version__ = "0.24.0"
 
 from deephaven_internal import jvm
+
 try:
     jvm.check_ready()
 finally:
     del jvm
 
 from .dherror import DHError
-from .table import SortDirection, AsOfMatchRule
+from .table import SortDirection
 from .csv import read as read_csv
 from .csv import write as write_csv
 from .stream.kafka import consumer as kafka_consumer
 from .stream.kafka import producer as kafka_producer
-from .table_factory import empty_table, time_table, merge, merge_sorted, new_table, DynamicTableWriter, input_table
+from .table_factory import empty_table, time_table, merge, merge_sorted, new_table, DynamicTableWriter, input_table, \
+    ring_table
 from .replay import TableReplayer
 from ._gc import garbage_collect
 from .dbc import read_sql
+
+__all__ = ["read_csv", "write_csv", "kafka_consumer", "kafka_producer", "empty_table", "time_table", "merge",
+           "merge_sorted", "new_table", "input_table", "ring_table", "DynamicTableWriter", "TableReplayer",
+           "garbage_collect", "read_sql", "DHError", "SortDirection"]
```

## deephaven/arrow.py

```diff
@@ -1,18 +1,18 @@
 #
 #     Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
 #
-"""This module supports conversions between Arrow tables and Deephaven tables."""
+"""This module supports conversions between pyarrow tables and Deephaven tables."""
 
 from typing import List, Dict
 
 import jpy
 import pyarrow as pa
 
-from deephaven import DHError
+from deephaven import DHError, dtypes
 from deephaven.table import Table
 
 _JArrowToTableConverter = jpy.get_type("io.deephaven.extensions.barrage.util.ArrowToTableConverter")
 _JTableToArrowConverter = jpy.get_type("io.deephaven.extensions.barrage.util.TableToArrowConverter")
 
 _ARROW_DH_DATA_TYPE_MAPPING = {
     pa.null(): '',
@@ -57,34 +57,34 @@
     # dictionary(index_type, value_type, …)
 }
 
 SUPPORTED_ARROW_TYPES = [k for k, v in _ARROW_DH_DATA_TYPE_MAPPING.items() if v]
 
 
 def _map_arrow_type(arrow_type) -> Dict[str, str]:
-    """Maps an Arrow type to the corresponding Deephaven column data type."""
+    """Maps a pyarrow type to the corresponding Deephaven column data type."""
     dh_type = _ARROW_DH_DATA_TYPE_MAPPING.get(arrow_type)
     if not dh_type:
         # if this is a case of timestamp with tz specified
         if isinstance(arrow_type, pa.TimestampType):
             dh_type = "io.deephaven.time.DateTime"
 
     if not dh_type:
         raise DHError(message=f'unsupported arrow data type : {arrow_type}, refer to '
-                              f'deephaven.arrow.SUPPORTED_ARROW_TYPES for the list of supported Arrow types.')
+                              f'deephaven.arrow.SUPPORTED_ARROW_TYPES for the list of supported pyarrow types.')
 
     return {"deephaven:type": dh_type}
 
 
 def to_table(pa_table: pa.Table, cols: List[str] = None) -> Table:
-    """Creates a Deephaven table from an Arrow table.
+    """Creates a Deephaven table from a pyarrow table.
 
     Args:
-        pa_table(pa.Table): the Arrow table
-        cols (List[str]): the Arrow table column names, default is None which means including all columns
+        pa_table(pa.Table): the pyarrow table
+        cols (List[str]): the pyarrow table column names, default is None which means including all columns
 
     Returns:
         a new table
 
     Raises:
         DHError
     """
@@ -96,36 +96,36 @@
     dh_fields = []
     for f in pa_table.schema:
         dh_fields.append(pa.field(name=f.name, type=f.type, metadata=_map_arrow_type(f.type)))
     dh_schema = pa.schema(dh_fields)
 
     try:
         pa_buffer = dh_schema.serialize()
-        j_barrage_table_builder.setSchema(pa_buffer.to_pybytes())
+        j_barrage_table_builder.setSchema(dtypes.array(dtypes.byte, pa_buffer))
 
         record_batches = pa_table.to_batches()
         for rb in record_batches:
             pa_buffer = rb.serialize()
-            j_barrage_table_builder.addRecordBatch(pa_buffer.to_pybytes())
+            j_barrage_table_builder.addRecordBatch(dtypes.array(dtypes.byte, pa_buffer))
         j_barrage_table_builder.onCompleted()
 
         return Table(j_table=j_barrage_table_builder.getResultTable())
     except Exception as e:
-        raise DHError(e, message="failed to create a Deephaven table from a Arrow table.") from e
+        raise DHError(e, message="failed to create a Deephaven table from a pyarrow table.") from e
 
 
 def to_arrow(table: Table, cols: List[str] = None) -> pa.Table:
-    """Produces an Arrow table from a Deephaven table
+    """Produces a pyarrow table from a Deephaven table
 
     Args:
         table (Table): the source table
         cols (List[str]): the source column names, default is None which means including all columns
 
     Returns:
-        pandas.DataFrame
+        a pyarrow table
 
     Raise:
         DHError
     """
     try:
         if cols:
             table = table.view(formulas=cols)
@@ -141,8 +141,8 @@
             pa_rb_buffer = j_arrow_builder.next()
             message = pa.ipc.read_message(pa_rb_buffer)
             record_batch = pa.ipc.read_record_batch(message, schema=schema)
             record_batches.append(record_batch)
 
         return pa.Table.from_batches(record_batches, schema=schema)
     except Exception as e:
-        raise DHError(e, message="failed to create an Arrow table from a Deephaven table.") from e
+        raise DHError(e, message="failed to create a pyarrow table from a Deephaven table.") from e
```

## deephaven/column.py

```diff
@@ -62,15 +62,16 @@
 
     def __post_init__(self):
         try:
             if self.input_data is None:
                 self.j_column = _JColumn.empty(self.j_column_header)
             else:
                 if self.data_type.is_primitive:
-                    self.j_column = _JColumn.ofUnsafe(self.name, dtypes.array(self.data_type, self.input_data))
+                    self.j_column = _JColumn.ofUnsafe(self.name, dtypes.array(self.data_type, self.input_data,
+                                                                              remap=dtypes.null_remap(self.data_type)))
                 else:
                     self.j_column = _JColumn.of(self.j_column_header, dtypes.array(self.data_type, self.input_data))
         except Exception as e:
             raise DHError(e, "failed to create an InputColumn.") from e
 
 
 def bool_col(name: str, data: Sequence) -> InputColumn:
```

## deephaven/dtypes.py

```diff
@@ -11,24 +11,28 @@
 from typing import Any, Sequence, Callable, Dict, Type, Union
 
 import jpy
 import numpy as np
 import pandas as pd
 
 from deephaven import DHError
+from deephaven.constants import NULL_BYTE, NULL_SHORT, NULL_INT, NULL_LONG, NULL_FLOAT, NULL_DOUBLE, NULL_CHAR
 
 _JQstType = jpy.get_type("io.deephaven.qst.type.Type")
 _JTableTools = jpy.get_type("io.deephaven.engine.util.TableTools")
 _JPrimitiveArrayConversionUtility = jpy.get_type("io.deephaven.integrations.common.PrimitiveArrayConversionUtility")
 
 _j_name_type_map: Dict[str, DType] = {}
 
 
 def _qst_custom_type(cls_name: str):
-    return _JQstType.find(_JTableTools.typeFromName(cls_name))
+    try:
+        return _JQstType.find(_JTableTools.typeFromName(cls_name))
+    except:
+        return None
 
 
 class DType:
     """ A class representing a data type in Deephaven."""
 
     def __init__(self, j_name: str, j_type: Type = None, qst_type: jpy.JType = None, is_primitive: bool = False,
                  np_type: Any = np.object_):
@@ -133,14 +137,44 @@
 float_array = double_array
 """Double-precision floating-point array type"""
 string_array = DType(j_name='[Ljava.lang.String;')
 """Java String array type"""
 datetime_array = DType(j_name='[Lio.deephaven.time.DateTime;')
 """Deephaven DateTime array type"""
 
+_PRIMITIVE_DTYPE_NULL_MAP = {
+    bool_: NULL_BYTE,
+    byte: NULL_BYTE,
+    char: NULL_CHAR,
+    int16: NULL_SHORT,
+    int32: NULL_INT,
+    int64: NULL_LONG,
+    float32: NULL_FLOAT,
+    float64: NULL_DOUBLE,
+}
+
+
+def null_remap(dtype: DType) -> Callable[[Any], Any]:
+    """ Creates a null value remap function for the provided DType.
+
+    Args:
+        dtype (DType): the DType instance
+
+    Returns:
+        a Callable
+
+    Raises:
+        TypeError
+    """
+    null_value = _PRIMITIVE_DTYPE_NULL_MAP.get(dtype)
+    if null_value is None:
+        raise TypeError("null_remap() must be called with a primitive DType")
+
+    return lambda v: null_value if v is None else v
+
 
 def array(dtype: DType, seq: Sequence, remap: Callable[[Any], Any] = None) -> jpy.JType:
     """ Creates a Java array of the specified data type populated with values from a sequence.
 
     Note:
         this method does unsafe casting, meaning precision and values might be lost with down cast
 
@@ -153,21 +187,22 @@
     Returns:
         a Java array
 
     Raises:
         DHError
     """
     try:
+        if isinstance(seq, str) and dtype == char:
+            # ord is the Python builtin function that takes a unicode character and returns an integer code point value
+            remap = ord
+
         if remap:
             if not callable(remap):
                 raise ValueError("Not a callable")
             seq = [remap(v) for v in seq]
-        else:
-            if isinstance(seq, str) and dtype == char:
-                return array(char, seq, remap=ord)
 
         if isinstance(seq, np.ndarray):
             if dtype == bool_:
                 bytes_ = seq.astype(dtype=np.int8)
                 j_bytes = array(byte, bytes_)
                 seq = _JPrimitiveArrayConversionUtility.translateArrayByteToBoolean(j_bytes)
             elif dtype == DateTime:
```

## deephaven/filters.py

```diff
@@ -1,40 +1,54 @@
 #
 # Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
 #
 
 """ This module implement various filters that can be used in deephaven table's filter operations."""
 from __future__ import annotations
 
+from enum import Enum
 from typing import List, Union
 
 import jpy
+import functools
 
 from deephaven import DHError
 from deephaven._wrapper import JObjectWrapper
 from deephaven.jcompat import to_sequence
 
 _JFilter = jpy.get_type("io.deephaven.api.filter.Filter")
-_JRegexFilter = jpy.get_type("io.deephaven.engine.table.impl.select.RegexFilter")
 _JFilterOr = jpy.get_type("io.deephaven.api.filter.FilterOr")
 _JFilterAnd = jpy.get_type("io.deephaven.api.filter.FilterAnd")
 _JFilterNot = jpy.get_type("io.deephaven.api.filter.FilterNot")
+_JColumnName = jpy.get_type("io.deephaven.api.ColumnName")
+_JFilterPattern = jpy.get_type("io.deephaven.api.filter.FilterPattern")
+_JPatternMode = jpy.get_type("io.deephaven.api.filter.FilterPattern$Mode")
+_JPattern = jpy.get_type("java.util.regex.Pattern")
 
 
 class Filter(JObjectWrapper):
     """A Filter object represents a filter that can be used in Table's filtering(where) operations."""
+
     j_object_type = _JFilter
 
     @property
     def j_object(self) -> jpy.JType:
         return self.j_filter
 
     def __init__(self, j_filter):
         self.j_filter = j_filter
 
+    def not_(self):
+        """Creates a new filter that evaluates to the opposite of what this filter evaluates to.
+
+        Returns:
+            a new not Filter
+        """
+        return Filter(j_filter=_JFilterNot.of(self.j_filter))
+
     @classmethod
     def from_(cls, conditions: Union[str, List[str]]) -> Union[Filter, List[Filter]]:
         """Creates filter(s) from the given condition(s).
 
         Args:
             conditions (Union[str, List[str]]): filter condition(s)
 
@@ -42,67 +56,122 @@
             filter(s)
 
         Raises:
             DHError
         """
         conditions = to_sequence(conditions)
         try:
-            filters = [cls(j_filter=j_filter) for j_filter in _JFilter.from_(conditions).toArray()]
+            filters = [
+                cls(j_filter=j_filter)
+                for j_filter in _JFilter.from_(conditions).toArray()
+            ]
             return filters if len(filters) != 1 else filters[0]
         except Exception as e:
             raise DHError(e, "failed to create filters.") from e
 
 
 def or_(filters: List[Filter]) -> Filter:
     """Creates a new filter that evaluates to true when any of the given filters evaluates to true.
 
     Args:
         filters (List[filter]): the component filters
 
     Returns:
-        a new Filter
+        a new or Filter
     """
     return Filter(j_filter=_JFilterOr.of(*[f.j_filter for f in filters]))
 
 
 def and_(filters: List[Filter]) -> Filter:
     """Creates a new filter that evaluates to true when all of the given filters evaluates to true.
 
     Args:
         filters (List[filter]): the component filters
 
     Returns:
-        a new Filter
+        a new and Filter
     """
     return Filter(j_filter=_JFilterAnd.of(*[f.j_filter for f in filters]))
 
 
 def not_(filter_: Filter) -> Filter:
-    """Creates a new filter that evaluates to true when the given filter evaluates to false.
+    """Creates a new filter that evaluates to the opposite of what filter_ evaluates to.
 
     Args:
         filter_ (Filter): the filter to negate with
 
     Returns:
-        a new Filter
+        a new not Filter
     """
-    return Filter(j_filter=_JFilterNot.of(filter_.j_filter))
+    return filter_.not_()
 
 
-class RegexFilter(Filter):
-    """ The RegexFilter is a filter that matches using a regular expression. """
+def is_null(col: str) -> Filter:
+    """Creates a new filter that evaluates to true when the col is null, and evaluates to false when col is not null.
 
-    j_object_type = _JRegexFilter
+    Args:
+        col (str): the column name
 
-    def __init__(self, col: str, pattern: str):
-        """
-        Args:
-             col (str): the name of the column to apply the filter
-             pattern (str): the regular expression pattern
+    Returns:
+        a new is-null Filter
+    """
+    return Filter(j_filter=_JFilter.isNull(_JColumnName.of(col)))
 
-        Raises:
-            DHError
-        """
-        try:
-            self.j_filter = _JRegexFilter(col, pattern)
-        except Exception as e:
-            raise DHError(e, "failed to create a Regex filter.") from e
+
+def is_not_null(col: str) -> Filter:
+    """Creates a new filter that evaluates to true when the col is not null, and evaluates to false when col is null.
+
+    Args:
+        col (str): the column name
+
+    Returns:
+        a new is-not-null Filter
+    """
+    return Filter(j_filter=_JFilter.isNotNull(_JColumnName.of(col)))
+
+
+class PatternMode(Enum):
+    """The regex mode to use"""
+
+    MATCHES = _JPatternMode.MATCHES
+    """Matches the entire input against the pattern"""
+
+    FIND = _JPatternMode.FIND
+    """Matches any subsequence of the input against the pattern"""
+
+
+def pattern(
+    mode: PatternMode,
+    col: str,
+    regex: str,
+    invert_pattern: bool = False
+) -> Filter:
+    """Creates a regular-expression pattern filter.
+
+    See https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/regex/Pattern.html for documentation on
+    the regex pattern.
+
+    This filter will never match {@code null} values.
+
+    Args:
+        mode (PatternMode): the mode
+        col (str): the column name
+        regex (str): the regex pattern
+        invert_pattern (bool): if the pattern matching logic should be inverted
+
+    Returns:
+        a new pattern filter
+
+    Raises:
+        DHError
+    """
+    try:
+        return Filter(
+            j_filter=_JFilterPattern.of(
+                _JColumnName.of(col),
+                _JPattern.compile(regex),
+                mode.value,
+                invert_pattern,
+            )
+        )
+    except Exception as e:
+        raise DHError(e, "failed to create a pattern filter.") from e
```

## deephaven/numpy.py

```diff
@@ -115,15 +115,15 @@
         for col_def in col_defs:
             data_col = table.j_table.getColumn(col_def.name)
             j_arrays.append(data_col.getDirect())
         return _columns_to_2d_numpy_array(col_defs[0], j_arrays)
     except DHError:
         raise
     except Exception as e:
-        raise DHError(e, "failed to create a Numpy array from the table column.") from e
+        raise DHError(e, "failed to create a numpy array from the table column.") from e
 
 
 def to_table(np_array: np.ndarray, cols: List[str]) -> Table:
     """  Creates a new table from a numpy array.
 
     Args:
         np_array (np.ndarray): the numpy array
```

## deephaven/pandas.py

```diff
@@ -1,70 +1,128 @@
 #
 # Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
 #
 
-""" This module supports the conversion between Deephaven tables and Pandas DataFrames. """
+""" This module supports the conversion between Deephaven tables and pandas DataFrames. """
 from typing import List
 
 import jpy
 import numpy as np
-import pandas
 import pandas as pd
+import pyarrow as pa
 
-from deephaven import DHError, new_table, dtypes
+from deephaven import DHError, new_table, dtypes, arrow
+from deephaven.arrow import SUPPORTED_ARROW_TYPES
 from deephaven.column import Column
-from deephaven.constants import NULL_BYTE, NULL_SHORT, NULL_INT, NULL_LONG, NULL_FLOAT, NULL_DOUBLE, NULL_BOOLEAN
+from deephaven.constants import NULL_BYTE, NULL_SHORT, NULL_INT, NULL_LONG, NULL_FLOAT, NULL_DOUBLE
 from deephaven.numpy import column_to_numpy_array, _make_input_column
 from deephaven.table import Table
 
 _JPrimitiveArrayConversionUtility = jpy.get_type("io.deephaven.integrations.common.PrimitiveArrayConversionUtility")
+_is_dtype_backend_supported = pd.__version__ >= "2.0.0"
 
 
-def _column_to_series(table: Table, col_def: Column) -> pandas.Series:
+def _column_to_series(table: Table, col_def: Column) -> pd.Series:
     """Produce a copy of the specified column as a pandas.Series object.
 
     Args:
         table (Table): the table
         col_def (Column):  the column definition
 
     Returns:
-        pandas.Series
+        a pandas Series
 
     Raises:
         DHError
     """
     try:
         data_col = table.j_table.getColumn(col_def.name)
         np_array = column_to_numpy_array(col_def, data_col.getDirect())
 
-        return pandas.Series(data=np_array, copy=False)
+        return pd.Series(data=np_array, copy=False)
     except DHError:
         raise
     except Exception as e:
-        raise DHError(e, message="failed to create apandas Series for {col}") from e
+        raise DHError(e, message="failed to create a pandas Series for {col}") from e
 
 
-def to_pandas(table: Table, cols: List[str] = None) -> pandas.DataFrame:
-    """Produces a pandas.DataFrame from a table.
+_DTYPE_MAPPING_PYARROW = {
+    pa.int8(): pd.ArrowDtype(pa.int8()),
+    pa.int16(): pd.ArrowDtype(pa.int16()),
+    pa.int32(): pd.ArrowDtype(pa.int32()),
+    pa.int64(): pd.ArrowDtype(pa.int64()),
+    pa.uint8(): pd.ArrowDtype(pa.uint8()),
+    pa.uint16(): pd.ArrowDtype(pa.uint16()),
+    pa.uint32(): pd.ArrowDtype(pa.uint32()),
+    pa.uint64(): pd.ArrowDtype(pa.uint64()),
+    pa.bool_(): pd.ArrowDtype(pa.bool_()),
+    pa.float32(): pd.ArrowDtype(pa.float32()),
+    pa.float64(): pd.ArrowDtype(pa.float64()),
+    pa.string(): pd.ArrowDtype(pa.string()),
+    pa.timestamp('ns'): pd.ArrowDtype(pa.timestamp('ns')),
+    pa.timestamp('ns', tz='UTC'): pd.ArrowDtype(pa.timestamp('ns', tz='UTC')),
+}
+
+_DTYPE_MAPPING_NUMPY_NULLABLE = {
+    pa.int8(): pd.Int8Dtype(),
+    pa.int16(): pd.Int16Dtype(),
+    pa.int32(): pd.Int32Dtype(),
+    pa.int64(): pd.Int64Dtype(),
+    pa.bool_(): pd.BooleanDtype(),
+    pa.float32(): pd.Float32Dtype(),
+    pa.float64(): pd.Float64Dtype(),
+    pa.string(): pd.StringDtype(),
+    # pa.Table.to_pandas() doesn't like explicit mapping to pd.DatetimeTZDtype, however it, on its own,
+    # can correctly map pyarrow timestamp to DatetimeTZDtype and convert null values to NaT
+    # pa.timestamp('ns'): pd.DatetimeTZDtype(unit='ns', tz='UTC'),
+    # pa.timestamp('ns', tz='UTC'): pd.DatetimeTZDtype(unit='ns', tz='UTC'),
+}
+
+_PYARROW_TO_PANDAS_TYPE_MAPPERS = {
+    "pyarrow": _DTYPE_MAPPING_PYARROW.get,
+    "numpy_nullable": _DTYPE_MAPPING_NUMPY_NULLABLE.get,
+}
+
+
+def to_pandas(table: Table, cols: List[str] = None, dtype_backend: str = None) -> pd.DataFrame:
+    """Produces a pandas DataFrame from a table.
 
     Note that the **entire table** is going to be cloned into memory, so the total number of entries in the table
     should be considered before blindly doing this. For large tables, consider using the Deephaven query language to
     select a subset of the table **before** using this method.
 
     Args:
         table (Table): the source table
         cols (List[str]): the source column names, default is None which means include all columns
+        dtype_backend (str): Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,
+            nullable dtypes are used for all dtypes that have a nullable implementation when “numpy_nullable” is set,
+            pyarrow is used for all dtypes if “pyarrow” is set. default is None, meaning Numpy backed DataFrames with
+            no nullable dtypes.
 
     Returns:
-        pandas.DataFrame
+        a pandas DataFrame
 
     Raise:
         DHError
     """
     try:
+        if dtype_backend is not None and not _is_dtype_backend_supported:
+            raise DHError(message=f"the dtype_backend ({dtype_backend}) option is only available for pandas 2.0.0 and "
+                                  f"above. {pd.__version__} is being used.")
+
+        type_mapper = _PYARROW_TO_PANDAS_TYPE_MAPPERS.get(dtype_backend)
+        # if nullable dtypes (pandas or pyarrow) is requested
+        if type_mapper:
+            pa_table = arrow.to_arrow(table=table, cols=cols)
+            df = pa_table.to_pandas(types_mapper=type_mapper)
+            del pa_table
+            return df
+
+        # if regular numpy dtype is requested, direct access of the table column sources is required. In order to get a
+        # consistent view of a ticking table, we need to take a snapshot of it first.
         if table.is_refreshing:
             table = table.snapshot()
 
         col_def_dict = {col.name: col for col in table.columns}
         if not cols:
             cols = list(col_def_dict.keys())
         else:
@@ -75,81 +133,100 @@
         data = {}
         for col in cols:
             series = _column_to_series(table, col_def_dict[col])
             data[col] = series
 
         dtype_set = set([v.dtype for k, v in data.items()])
         if len(dtype_set) == 1:
-            return pandas.DataFrame(data=np.stack([v.array for k, v in data.items()], axis=1),
-                                    columns=cols,
-                                    copy=False)
+            return pd.DataFrame(data=np.stack([v.array for k, v in data.items()], axis=1),
+                                columns=cols,
+                                copy=False)
         else:
-            return pandas.DataFrame(data=data, columns=cols, copy=False)
+            return pd.DataFrame(data=data, columns=cols, copy=False)
     except DHError:
         raise
     except Exception as e:
-        raise DHError(e, "failed to create a Pandas DataFrame from table.") from e
+        raise DHError(e, "failed to create a pandas DataFrame from table.") from e
 
 
-EX_DTYPE_NULL_MAP = {
+_EX_DTYPE_NULL_MAP = {
     # This reflects the fact that in the server we use NULL_BOOLEAN_AS_BYTE - the byte encoding of null boolean to
     # translate boxed Boolean to/from primitive bytes
     pd.BooleanDtype: NULL_BYTE,
     pd.Int8Dtype: NULL_BYTE,
     pd.Int16Dtype: NULL_SHORT,
     pd.Int32Dtype: NULL_INT,
     pd.Int64Dtype: NULL_LONG,
     pd.Float32Dtype: NULL_FLOAT,
     pd.Float64Dtype: NULL_DOUBLE,
     pd.StringDtype: None,
+    pd.ArrowDtype(pa.int8()): NULL_BYTE,
+    pd.ArrowDtype(pa.int16()): NULL_SHORT,
+    pd.ArrowDtype(pa.int32()): NULL_INT,
+    pd.ArrowDtype(pa.int64()): NULL_LONG,
+    pd.ArrowDtype(pa.bool_()): NULL_BYTE,
+    pd.ArrowDtype(pa.float32()): NULL_FLOAT,
+    pd.ArrowDtype(pa.float64()): NULL_DOUBLE,
+    pd.ArrowDtype(pa.string()): None,
 }
 
 
 def _map_na(np_array: np.ndarray):
-    """Replaces the pd.NA values in the array if it is of Pandas ExtensionDtype(nullable)."""
+    """Replaces the pd.NA values in the array if it is of pandas ExtensionDtype(nullable)."""
     pd_dtype = np_array.dtype
-    if not isinstance(pd_dtype, pandas.api.extensions.ExtensionDtype):
+    if not isinstance(pd_dtype, pd.api.extensions.ExtensionDtype):
         return np_array
 
-    dh_null = EX_DTYPE_NULL_MAP.get(type(pd_dtype), None)
-    if isinstance(pd_dtype, pd.StringDtype) or isinstance(pd_dtype, pd.BooleanDtype):
+    dh_null = _EX_DTYPE_NULL_MAP.get(type(pd_dtype)) or _EX_DTYPE_NULL_MAP.get(pd_dtype)
+    if isinstance(pd_dtype, pd.StringDtype) or isinstance(pd_dtype, pd.BooleanDtype) or pd_dtype == pd.ArrowDtype(
+            pa.bool_()):
         np_array = np.array(list(map(lambda v: dh_null if v is pd.NA else v, np_array)))
     elif dh_null is not None:
         np_array = np_array.fillna(dh_null)
 
     return np_array
 
 
-def to_table(df: pandas.DataFrame, cols: List[str] = None) -> Table:
-    """Creates a new table from a pandas.DataFrame.
+def to_table(df: pd.DataFrame, cols: List[str] = None) -> Table:
+    """Creates a new table from a pandas DataFrame.
 
     Args:
-        df (DataFrame): the Pandas DataFrame instance
-        cols (List[str]): the dataframe column names, default is None which means including all columns in the dataframe
+        df (DataFrame): the pandas DataFrame instance
+        cols (List[str]): the dataframe column names, default is None which means including all columns in the DataFrame
 
     Returns:
         a Deephaven table
 
     Raise:
         DHError
     """
 
-    try:
-        if not cols:
-            cols = list(df)
-        else:
-            diff_set = set(cols) - set(list(df))
-            if diff_set:
-                raise DHError(message=f"columns - {list(diff_set)} not found")
+    if not cols:
+        cols = list(df)
+    else:
+        diff_set = set(cols) - set(list(df))
+        if diff_set:
+            raise DHError(message=f"columns - {list(diff_set)} not found")
+
+    # if any arrow backed column is present, create a pyarrow table first, then upload to DH, if error occurs, fall
+    # back to the numpy-array based approach
+    if _is_dtype_backend_supported and any(isinstance(df[col].dtype, pd.ArrowDtype) for col in cols):
+        try:
+            pa_table = pa.Table.from_pandas(df=df, columns=cols)
+            dh_table = arrow.to_table(pa_table)
+            return dh_table
+        except:
+            pass
 
+    try:
         input_cols = []
         for col in cols:
             np_array = df.get(col).values
             dtype = dtypes.from_np_dtype(np_array.dtype)
             np_array = _map_na(np_array)
             input_cols.append(_make_input_column(col, np_array, dtype))
 
         return new_table(cols=input_cols)
     except DHError:
         raise
     except Exception as e:
-        raise DHError(e, "failed to create a Deephaven Table from a Pandas DataFrame.") from e
+        raise DHError(e, "failed to create a Deephaven Table from a pandas DataFrame.") from e
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## deephaven/table.py

```diff
@@ -34,18 +34,18 @@
 _J_Table = jpy.get_type("io.deephaven.engine.table.Table")
 _JLiveAttributeMap = jpy.get_type("io.deephaven.engine.table.impl.LiveAttributeMap")
 _JTableTools = jpy.get_type("io.deephaven.engine.util.TableTools")
 _JColumnName = jpy.get_type("io.deephaven.api.ColumnName")
 _JSortColumn = jpy.get_type("io.deephaven.api.SortColumn")
 _JFilter = jpy.get_type("io.deephaven.api.filter.Filter")
 _JFilterOr = jpy.get_type("io.deephaven.api.filter.FilterOr")
-_JAsOfMatchRule = jpy.get_type("io.deephaven.engine.table.Table$AsOfMatchRule")
 _JPair = jpy.get_type("io.deephaven.api.agg.Pair")
 _JMatchPair = jpy.get_type("io.deephaven.engine.table.MatchPair")
 _JLayoutHintBuilder = jpy.get_type("io.deephaven.engine.util.LayoutHintBuilder")
+_JSearchDisplayMode = jpy.get_type("io.deephaven.engine.util.LayoutHintBuilder$SearchDisplayModes")
 _JSnapshotWhenOptions = jpy.get_type("io.deephaven.api.snapshot.SnapshotWhenOptions")
 
 # PartitionedTable
 _JPartitionedTable = jpy.get_type("io.deephaven.engine.table.PartitionedTable")
 _JPartitionedTableFactory = jpy.get_type("io.deephaven.engine.table.PartitionedTableFactory")
 _JTableDefinition = jpy.get_type("io.deephaven.engine.table.TableDefinition")
 _JPartitionedTableProxy = jpy.get_type("io.deephaven.engine.table.PartitionedTable$Proxy")
@@ -85,14 +85,24 @@
     of the RollupTable. """
     CONSTITUENT = _JNodeType.Constituent
     """Nodes at the leaf level when meth:`~deephaven.table.Table.rollup` method is called with 
     include_constituent=True. The constituent level is the lowest in a rollup table. These nodes have column names 
     and types from the source table of the RollupTable. """
 
 
+class SearchDisplayMode(Enum):
+    """An enum of search display modes for layout hints"""
+    DEFAULT = _JSearchDisplayMode.Default
+    """Use the system default. This may depend on your user and/or system settings."""
+    SHOW = _JSearchDisplayMode.Show
+    """Permit the search bar to be displayed, regardless of user or system settings."""
+    HIDE = _JSearchDisplayMode.Hide
+    """Hide the search bar, regardless of user or system settings."""
+
+
 class _FormatOperationsRecorder(Protocol):
     """A mixin for creating format operations to be applied to individual nodes of either RollupTable or TreeTable."""
 
     def format_column(self, formulas: Union[str, List[str]]):
         """Returns a new recorder with the :meth:`~deephaven.table.Table.format_columns` operation applied to nodes."""
         formulas = to_sequence(formulas)
         j_format_ops_recorder = jpy.cast(self.j_node_ops_recorder, _JFormatOperationsRecorder)
@@ -470,23 +480,14 @@
     """An enum defining the sorting orders."""
     DESCENDING = auto()
     """"""
     ASCENDING = auto()
     """"""
 
 
-class AsOfMatchRule(Enum):
-    """An enum defining matching rules on the final column to match by in as-of join and reverse as-of join
-    operation. """
-    LESS_THAN_EQUAL = _JAsOfMatchRule.LESS_THAN_EQUAL
-    LESS_THAN = _JAsOfMatchRule.LESS_THAN
-    GREATER_THAN_EQUAL = _JAsOfMatchRule.GREATER_THAN_EQUAL
-    GREATER_THAN = _JAsOfMatchRule.GREATER_THAN
-
-
 def _sort_column(col, dir_):
     return (_JSortColumn.desc(_JColumnName.of(col)) if dir_ == SortDirection.DESCENDING else _JSortColumn.asc(
         _JColumnName.of(col)))
 
 
 def _td_to_columns(table_definition):
     cols = []
@@ -873,15 +874,15 @@
                     return Table(j_table=self.j_table.select())
                 formulas = to_sequence(formulas)
                 return Table(j_table=self.j_table.select(*formulas))
         except Exception as e:
             raise DHError(e, "table select operation failed.") from e
 
     def select_distinct(self, formulas: Union[str, Sequence[str]] = None) -> Table:
-        """The select_distinct method creates a new table containing all of the unique values for a set of key
+        """The select_distinct method creates a new table containing all the unique values for a set of key
         columns. When the selectDistinct method is used on multiple columns, it looks for distinct sets of values in
         the selected columns.
 
         Args:
             formulas (Union[str, Sequence[str]], optional): the column name(s), default is None
 
         Returns:
@@ -1257,85 +1258,178 @@
                         )
                     )
                 else:
                     return Table(j_table=self.j_table.join(table.j_table, ",".join(on)))
         except Exception as e:
             raise DHError(e, "table join operation failed.") from e
 
-    def aj(self, table: Table, on: Union[str, Sequence[str]], joins: Union[str, Sequence[str]] = None,
-           match_rule: AsOfMatchRule = AsOfMatchRule.LESS_THAN_EQUAL) -> Table:
+    def aj(self, table: Table, on: Union[str, Sequence[str]], joins: Union[str, Sequence[str]] = None) -> Table:
         """The aj (as-of join) method creates a new table containing all the rows and columns of the left table,
         plus additional columns containing data from the right table. For columns appended to the left table (joins),
         row values equal the row values from the right table where the keys from the left table most closely match
         the keys from the right table without going over. If there is no matching key in the right table, appended row
         values are NULL.
 
         Args:
             table (Table): the right-table of the join
-            on (Union[str, Sequence[str]]): the column(s) to match, can be a common name or an equal expression,
-                i.e. "col_a = col_b" for different column names
+            on (Union[str, Sequence[str]]): the column(s) to match, can be a common name or a match condition of two
+                columns, e.g. 'col_a = col_b'. The first 'N-1' matches are exact matches.  The final match is an inexact
+                match.  The inexact match can use either '<' or '<='.  If a common name is used for the inexact match,
+                '<=' is used for the comparison.
             joins (Union[str, Sequence[str]], optional): the column(s) to be added from the right table to the result
                 table, can be renaming expressions, i.e. "new_col = col"; default is None
-            match_rule (AsOfMatchRule): the inexact matching rule on the last column to match specified in 'on',
-                default is AsOfMatchRule.LESS_THAN_EQUAL. The other valid value is AsOfMatchRule.LESS_THAN.
         Returns:
             a new table
 
         Raises:
             DHError
         """
         try:
-            on = to_sequence(on)
-            joins = to_sequence(joins)
-            if on:
-                on = [_JMatchPair.of(_JPair.parse(p)) for p in on]
-            if joins:
-                joins = [_JMatchPair.of(_JPair.parse(p)) for p in joins]
+            on = ",".join(to_sequence(on))
+            joins = ",".join(to_sequence(joins))
+            table_op = jpy.cast(self.j_object, _JTableOperations)
             with auto_locking_ctx(self, table):
-                return Table(j_table=self.j_table.aj(table.j_table, on, joins, match_rule.value))
+                return Table(j_table=table_op.aj(table.j_table, on, joins))
         except Exception as e:
             raise DHError(e, "table as-of join operation failed.") from e
 
-    def raj(self, table: Table, on: Union[str, Sequence[str]], joins: Union[str, Sequence[str]] = None,
-            match_rule: AsOfMatchRule = AsOfMatchRule.GREATER_THAN_EQUAL) -> Table:
-        """The reverse-as-of join method creates a new table containing all of the rows and columns of the left table,
+    def raj(self, table: Table, on: Union[str, Sequence[str]], joins: Union[str, Sequence[str]] = None) -> Table:
+        """The reverse-as-of join method creates a new table containing all the rows and columns of the left table,
         plus additional columns containing data from the right table. For columns appended to the left table (joins),
         row values equal the row values from the right table where the keys from the left table most closely match
         the keys from the right table without going under. If there is no matching key in the right table, appended row
         values are NULL.
 
         Args:
             table (Table): the right-table of the join
-            on (Union[str, Sequence[str]]): the column(s) to match, can be a common name or an equal expression,
-                i.e. "col_a = col_b" for different column names
+            on (Union[str, Sequence[str]]): the column(s) to match, can be a common name or a match condition of two
+                columns, e.g. 'col_a = col_b'. The first 'N-1' matches are exact matches.  The final match is an inexact
+                match.  The inexact match can use either '>' or '>='.  If a common name is used for the inexact match,
+                '>=' is used for the comparison.
             joins (Union[str, Sequence[str]], optional): the column(s) to be added from the right table to the result
                 table, can be renaming expressions, i.e. "new_col = col"; default is None
-            match_rule (AsOfMatchRule): the inexact matching rule on the last column to match specified in 'on',
-                default is AsOfMatchRule.GREATER_THAN_EQUAL. The other valid value is AsOfMatchRule.GREATER_THAN.
 
         Returns:
             a new table
 
         Raises:
             DHError
         """
         try:
-            on = to_sequence(on)
-            joins = to_sequence(joins)
-            on = to_sequence(on)
-            joins = to_sequence(joins)
-            if on:
-                on = [_JMatchPair.of(_JPair.parse(p)) for p in on]
-            if joins:
-                joins = [_JMatchPair.of(_JPair.parse(p)) for p in joins]
+            on = ",".join(to_sequence(on))
+            joins = ",".join(to_sequence(joins))
+            table_op = jpy.cast(self.j_object, _JTableOperations)
             with auto_locking_ctx(self, table):
-                return Table(j_table=self.j_table.raj(table.j_table, on, joins, match_rule.value))
+                return Table(j_table=table_op.raj(table.j_table, on, joins))
         except Exception as e:
             raise DHError(e, "table reverse-as-of join operation failed.") from e
 
+    def range_join(self, table: Table, on: Union[str, List[str]], aggs: Union[Aggregation, List[Aggregation]]) -> Table:
+        """The range_join method creates a new table containing all the rows and columns of the left table,
+        plus additional columns containing aggregated data from the right table. For columns appended to the
+        left table (joins), cell values equal aggregations over vectors of values from the right table.
+        These vectors are formed from all values in the right table where the right table keys fall within the
+        ranges of keys defined by the left table (responsive ranges).
+
+        range_join is a join plus aggregation that (1) joins arrays of data from the right table onto the left table,
+        and then (2) aggregates over the joined data. Oftentimes this is used to join data for a particular time range
+        from the right table onto the left table.
+
+        Rows from the right table with null or NaN key values are discarded; that is, they are never included in the
+        vectors used for aggregation.  For all rows that are not discarded, the right table must be sorted according
+        to the right range column for all rows within a group.
+
+        Join key ranges, specified by the 'on' argument, are defined by zero-or-more exact join matches and a single
+        range join match. The range join match must be the last match in the list.
+
+        The exact match expressions are parsed as in other join operations. That is, they are either a column name
+        common to both tables or a column name from the left table followed by an equals sign followed by a column
+        name from the right table.
+        Examples:
+            Match on the same column name in both tables:
+                "common_column"
+            Match on different column names in each table:
+                "left_column = right_column"
+                or
+                "left_column == right_column"
+
+        The range match expression is expressed as a ternary logical expression, expressing the relationship between
+        the left start column, the right range column, and the left end column. Each column name pair is separated by
+        a logical operator, either < or <=. Additionally, the entire expression may be preceded by a left arrow <-
+        and/or followed by a right arrow ->.  The arrows indicate that range match can 'allow preceding' or 'allow
+        following' to match values outside the explicit range. 'Allow preceding' means that if no matching right
+        range column value is equal to the left start column value, the immediately preceding matching right row
+        should be included in the aggregation if such a row exists. 'Allow following' means that if no matching right
+        range column value is equal to the left end column value, the immediately following matching right row should
+        be included in the aggregation if such a row exists.
+        Examples:
+            For less than paired with greater than:
+               "left_start_column < right_range_column < left_end_column"
+            For less than or equal paired with greater than or equal:
+               "left_start_column <= right_range_column <= left_end_column"
+            For less than or equal (allow preceding) paired with greater than or equal (allow following):
+               "<- left_start_column <= right_range_column <= left_end_column ->"
+
+        Special Cases
+            In order to produce aggregated output, range match expressions must define a range of values to aggregate
+            over. There are a few noteworthy special cases of ranges.
+
+            Empty Range
+            An empty range occurs for any left row with no matching right rows. That is, no non-null, non-NaN right
+            rows were found using the exact join matches, or none were in range according to the range join match.
+
+            Single-value Ranges
+            A single-value range is a range where the left row's values for the left start column and left end
+            column are equal and both relative matches are inclusive (<= and >=, respectively). For a single-value
+            range, only rows within the bucket where the right range column matches the single value are included in
+            the output aggregations.
+
+            Invalid Ranges
+            An invalid range occurs in two scenarios:
+                (1) When the range is inverted, i.e., when the value of the left start column is greater than the value
+                    of the left end column.
+                (2) When either relative-match is exclusive (< or >) and the value in the left start column is equal to
+                    the value in the left end column.
+            For invalid ranges, the result row will be null for all aggregation output columns.
+
+            Undefined Ranges
+            An undefined range occurs when either the left start column or the left end column is NaN. For rows with an
+            undefined range, the corresponding output values will be null (as with invalid ranges).
+
+            Unbounded Ranges
+            A partially or fully unbounded range occurs when either the left start column or the left end column is
+            null. If the left start column value is null and the left end column value is non-null, the range is
+            unbounded at the beginning, and only the left end column subexpression will be used for the match. If the
+            left start column value is non-null and the left end column value is null, the range is unbounded at the
+            end, and only the left start column subexpression will be used for the match. If the left start column
+            and left end column values are null, the range is unbounded, and all rows will be included.
+
+        Note: At this time, implementations only support static tables. This operation remains under active development.
+
+        Args:
+            table (Table): the right table of the join
+            on (Union[str, List[str]]): the match expression(s) that must include zero-or-more exact match expression,
+                and exactly one range match expression as described above
+            aggs (Union[Aggregation, List[Aggregation]]): the aggregation(s) to perform over the responsive ranges from
+                the right table for each row from this Table
+
+        Returns:
+            a new table
+
+        Raises:
+            DHError
+        """
+        try:
+            on = to_sequence(on)
+            aggs = to_sequence(aggs)
+            j_agg_list = j_array_list([agg.j_aggregation for agg in aggs])
+            return Table(j_table=self.j_table.rangeJoin(table.j_table, j_array_list(on), j_agg_list))
+        except Exception as e:
+            raise DHError(e, message="table range_join operation failed.") from e
+
     # endregion
 
     #
     # Table operation category: Aggregation
     # region Aggregation
 
     def head_by(self, num_rows: int, by: Union[str, Sequence[str]] = None) -> Table:
@@ -1860,29 +1954,33 @@
         try:
             return Table(j_table=self.j_table.formatRowWhere(cond, formula))
         except Exception as e:
             raise DHError(e, "failed to color format rows conditionally.") from e
 
     def layout_hints(self, front: Union[str, List[str]] = None, back: Union[str, List[str]] = None,
                      freeze: Union[str, List[str]] = None, hide: Union[str, List[str]] = None,
-                     column_groups: List[dict] = None) -> Table:
+                     column_groups: List[dict] = None, search_display_mode: SearchDisplayMode = None) -> Table:
         """ Sets layout hints on the Table
 
         Args:
             front (Union[str, List[str]]): the columns to show at the front.
             back (Union[str, List[str]]): the columns to show at the back.
             freeze (Union[str, List[str]]): the columns to freeze to the front.
                 These will not be affected by horizontal scrolling.
             hide (Union[str, List[str]]): the columns to hide.
-            column_groups (List[Dict]): A list of dicts specifying which columns should be grouped in the UI
+            column_groups (List[Dict]): A list of dicts specifying which columns should be grouped in the UI.
                 The dicts can specify the following:
 
-                name (str): The group name
-                children (List[str]): The
-                color (Optional[str]): The hex color string or Deephaven color name
+                * name (str): The group name
+                * children (List[str]): The column names in the group
+                * color (Optional[str]): The hex color string or Deephaven color name
+            search_display_mode (SearchDisplayMode): set the search bar to explicitly be accessible or inaccessible,
+                or use the system default. :attr:`SearchDisplayMode.SHOW` will show the search bar,
+                :attr:`SearchDisplayMode.HIDE` will hide the search bar, and :attr:`SearchDisplayMode.DEFAULT` will
+                use the default value configured by the user and system settings.
 
         Returns:
             a new table with the layout hints set
 
         Raises:
             DHError
         """
@@ -1901,14 +1999,18 @@
             if hide is not None:
                 _j_layout_hint_builder.hide(to_sequence(hide))
 
             if column_groups is not None:
                 for group in column_groups:
                     _j_layout_hint_builder.columnGroup(group.get("name"), j_array_list(group.get("children")),
                                                        group.get("color", ""))
+
+            if search_display_mode is not None:
+                _j_layout_hint_builder.setSearchBarAccess(search_display_mode.value)
+
         except Exception as e:
             raise DHError(e, "failed to create layout hints") from e
 
         try:
             return Table(j_table=self.j_table.setLayoutHints(_j_layout_hint_builder.build()))
         except Exception as e:
             raise DHError(e, "failed to set layout hints on table") from e
@@ -1932,15 +2034,15 @@
         except Exception as e:
             raise DHError(e, "failed to create a partitioned table.") from e
 
     def update_by(self, ops: Union[UpdateByOperation, List[UpdateByOperation]],
                   by: Union[str, List[str]] = None) -> Table:
         """Creates a table with additional columns calculated from window-based aggregations of columns in this table.
         The aggregations are defined by the provided operations, which support incremental aggregations over the
-        corresponding rows in the this table. The aggregations will apply position or time-based windowing and
+        corresponding rows in the table. The aggregations will apply position or time-based windowing and
         compute the results over the entire table or each row group as identified by the provided key columns.
 
         Args:
             ops (Union[UpdateByOperation, List[UpdateByOperation]]): the update-by operation definition(s)
             by (Union[str, List[str]]): the key column name(s) to group the rows of the table
 
         Returns:
@@ -2055,14 +2157,40 @@
             else:
                 j_table = self.j_table
 
             return TreeTable(j_tree_table=j_table.tree(id_col, parent_col), id_col=id_col, parent_col=parent_col)
         except Exception as e:
             raise DHError(e, "table tree operation failed.") from e
 
+    def await_update(self, timeout: int = None) -> bool:
+        """Waits until either this refreshing Table is updated or the timeout elapses if provided.
+
+        Args:
+            timeout (int): the maximum time to wait in milliseconds, default is None, meaning no timeout
+
+        Returns:
+            True when the table is updated or False when the timeout has been reached.
+
+        Raises:
+            DHError
+        """
+        if not self.is_refreshing:
+            raise DHError(message="await_update can only be called on refreshing tables.")
+
+        updated = True
+        try:
+            if timeout is not None:
+                updated = self.j_table.awaitUpdate(timeout)
+            else:
+                self.j_table.awaitUpdate()
+        except Exception as e:
+            raise DHError(e, "await_update was interrupted.") from e
+        else:
+            return updated
+
 
 class PartitionedTable(JObjectWrapper):
     """A partitioned table is a table containing tables, known as constituent tables.
     Each constituent table has the same schema.
 
     The partitioned table contains:
     1. one column containing constituent tables
@@ -2886,104 +3014,78 @@
                 else:
                     return PartitionedTableProxy(
                         j_pt_proxy=self.j_pt_proxy.join(table_op, ",".join(on)))
         except Exception as e:
             raise DHError(e, "join operation on the PartitionedTableProxy failed.") from e
 
     def aj(self, table: Union[Table, PartitionedTableProxy], on: Union[str, Sequence[str]],
-           joins: Union[str, Sequence[str]] = None,
-           match_rule: AsOfMatchRule = AsOfMatchRule.LESS_THAN_EQUAL) -> PartitionedTableProxy:
+           joins: Union[str, Sequence[str]] = None) -> PartitionedTableProxy:
         """Applies the :meth:`~Table.aj` table operation to all constituent tables of the underlying partitioned
         table with the provided right table or PartitionedTableProxy, and produces a new PartitionedTableProxy with
         the result tables as the constituents of its underlying partitioned table.
 
         In the case of the right table being another PartitionedTableProxy, the :meth:`~Table.aj` table operation
         is applied to the matching pairs of the constituent tables from both underlying partitioned tables.
 
         Args:
             table (Union[Table, PartitionedTableProxy]): the right table or PartitionedTableProxy of the join
-            on (Union[str, Sequence[str]]): the column(s) to match, can be a common name or an equal expression,
-                i.e. "col_a = col_b" for different column names
+            on (Union[str, Sequence[str]]): the column(s) to match, can be a common name or a match condition of two
+                columns, e.g. 'col_a = col_b'. The first 'N-1' matches are exact matches.  The final match is an inexact
+                match.  The inexact match can use either '<' or '<='.  If a common name is used for the inexact match,
+                '<=' is used for the comparison.
             joins (Union[str, Sequence[str]], optional): the column(s) to be added from the right table to the result
                 table, can be renaming expressions, i.e. "new_col = col"; default is None
-            match_rule (AsOfMatchRule): the inexact matching rule on the last column to match specified in 'on',
-                default is AsOfMatchRule.LESS_THAN_EQUAL. The other valid value is AsOfMatchRule.LESS_THAN.
         Returns:
             a new PartitionedTableProxy
 
         Raises:
             DHError
         """
         try:
-            on = to_sequence(on)
-            joins = to_sequence(joins)
-            if on:
-                on = [_JJoinMatch.parse(p) for p in on]
-            if joins:
-                joins = [_JJoinAddition.parse(p) for p in joins]
-
-            on = j_array_list(on)
-            joins = j_array_list(joins)
-            table_op = jpy.cast(table.j_object, _JTableOperations)
-            if match_rule is AsOfMatchRule.LESS_THAN_EQUAL:
-                match_rule = _JAsOfJoinRule.LESS_THAN_EQUAL
-            elif match_rule is AsOfMatchRule.LESS_THAN:
-                match_rule = _JAsOfJoinRule.LESS_THAN
-            else:
-                raise ValueError("invalid match_rule value")
+            on = ",".join(to_sequence(on))
+            joins = ",".join(to_sequence(joins))
+            table_op = jpy.cast(self.j_object, _JTableOperations)
+            r_table_op = jpy.cast(table.j_object, _JTableOperations)
 
             with auto_locking_ctx(self, table):
-                return PartitionedTableProxy(j_pt_proxy=self.j_pt_proxy.aj(table_op, on, joins, match_rule))
+                return PartitionedTableProxy(j_pt_proxy=table_op.aj(r_table_op, on, joins))
         except Exception as e:
             raise DHError(e, "as-of join operation on the PartitionedTableProxy failed.") from e
 
     def raj(self, table: Union[Table, PartitionedTableProxy], on: Union[str, Sequence[str]],
-            joins: Union[str, Sequence[str]] = None,
-            match_rule: AsOfMatchRule = AsOfMatchRule.GREATER_THAN_EQUAL) -> PartitionedTableProxy:
+            joins: Union[str, Sequence[str]] = None) -> PartitionedTableProxy:
         """Applies the :meth:`~Table.raj` table operation to all constituent tables of the underlying partitioned
         table with the provided right table or PartitionedTableProxy, and produces a new PartitionedTableProxy with
         the result tables as the constituents of its underlying partitioned table.
 
         In the case of the right table being another PartitionedTableProxy, the :meth:`~Table.raj` table operation
         is applied to the matching pairs of the constituent tables from both underlying partitioned tables.
 
         Args:
             table (Union[Table, PartitionedTableProxy]): the right table or PartitionedTableProxy of the join
-            on (Union[str, Sequence[str]]): the column(s) to match, can be a common name or an equal expression,
-                i.e. "col_a = col_b" for different column names
+            on (Union[str, Sequence[str]]): the column(s) to match, can be a common name or a match condition of two
+                columns, e.g. 'col_a = col_b'. The first 'N-1' matches are exact matches.  The final match is an inexact
+                match.  The inexact match can use either '>' or '>='.  If a common name is used for the inexact match,
+                '>=' is used for the comparison.
             joins (Union[str, Sequence[str]], optional): the column(s) to be added from the right table to the result
                 table, can be renaming expressions, i.e. "new_col = col"; default is None
-            match_rule (AsOfMatchRule): the inexact matching rule on the last column to match specified in 'on',
-                default is AsOfMatchRule.GREATER_THAN_EQUAL. The other valid value is AsOfMatchRule.GREATER_THAN.
         Returns:
             a new PartitionedTableProxy
 
         Raises:
             DHError
         """
         try:
-            on = to_sequence(on)
-            joins = to_sequence(joins)
-            if on:
-                on = [_JJoinMatch.parse(p) for p in on]
-            if joins:
-                joins = [_JJoinAddition.parse(p) for p in joins]
-
-            on = j_array_list(on)
-            joins = j_array_list(joins)
-            table_op = jpy.cast(table.j_object, _JTableOperations)
-            if match_rule is AsOfMatchRule.GREATER_THAN_EQUAL:
-                match_rule = _JReverseAsOfJoinRule.GREATER_THAN_EQUAL
-            elif match_rule is AsOfMatchRule.GREATER_THAN:
-                match_rule = _JReverseAsOfJoinRule.GREATER_THAN
-            else:
-                raise ValueError("invalid match_rule value")
+            on = ",".join(to_sequence(on))
+            joins = ",".join(to_sequence(joins))
+            table_op = jpy.cast(self.j_object, _JTableOperations)
+            r_table_op = jpy.cast(table.j_object, _JTableOperations)
 
             with auto_locking_ctx(self, table):
-                return PartitionedTableProxy(j_pt_proxy=self.j_pt_proxy.raj(table_op, on, joins, match_rule))
+                return PartitionedTableProxy(j_pt_proxy=table_op.raj(r_table_op, on, joins))
         except Exception as e:
             raise DHError(e, "reverse as-of join operation on the PartitionedTableProxy failed.") from e
 
     def group_by(self, by: Union[str, Sequence[str]] = None) -> PartitionedTableProxy:
         """Applies the :meth:`~Table.group_by` table operation to all constituent tables of the underlying
         partitioned table, and produces a new PartitionedTableProxy with the result tables as the constituents of its
         underlying partitioned table.
```

## deephaven/table_factory.py

```diff
@@ -22,14 +22,15 @@
 _JMutableInputTable = jpy.get_type("io.deephaven.engine.util.config.MutableInputTable")
 _JAppendOnlyArrayBackedMutableTable = jpy.get_type(
     "io.deephaven.engine.table.impl.util.AppendOnlyArrayBackedMutableTable")
 _JKeyedArrayBackedMutableTable = jpy.get_type("io.deephaven.engine.table.impl.util.KeyedArrayBackedMutableTable")
 _JTableDefinition = jpy.get_type("io.deephaven.engine.table.TableDefinition")
 _JTable = jpy.get_type("io.deephaven.engine.table.Table")
 _J_INPUT_TABLE_ATTRIBUTE = _JTable.INPUT_TABLE_ATTRIBUTE
+_JRingTableTools = jpy.get_type("io.deephaven.engine.table.impl.sources.ring.RingTableTools")
 
 
 def empty_table(size: int) -> Table:
     """Creates a table with rows but no columns.
 
     Args:
         size (int): the number of rows
@@ -287,7 +288,31 @@
     Returns:
         an InputTable
 
     Raises:
         DHError
     """
     return InputTable(col_defs=col_defs, init_table=init_table, key_cols=key_cols)
+
+
+def ring_table(parent: Table, capacity: int, initialize: bool = True) -> Table:
+    """Creates a ring table that retains the latest 'capacity' number of rows from the parent table.
+    Latest rows are determined solely by the new rows added to the parent table, deleted rows are ignored,
+    and updated rows are not expected and will raise an exception.
+
+    Ring table is mostly used with stream tables which do not retain their own data for more than an update cycle.
+
+    Args:
+        parent (Table): the parent table
+        capacity (int): the capacity of the ring table
+        initialize (bool): whether to initialize the ring table with a snapshot of the parent table, default is True
+
+    Returns:
+        a Table
+
+    Raises:
+        DHError
+    """
+    try:
+        return Table(j_table=_JRingTableTools.of(parent.j_table, capacity, initialize))
+    except Exception as e:
+        raise DHError(e, "failed to create a ring table.") from e
```

## deephaven/updateby.py

```diff
@@ -10,14 +10,15 @@
 from deephaven import DHError
 from deephaven._wrapper import JObjectWrapper
 from deephaven.jcompat import to_sequence
 
 _JUpdateByOperation = jpy.get_type("io.deephaven.api.updateby.UpdateByOperation")
 _JBadDataBehavior = jpy.get_type("io.deephaven.api.updateby.BadDataBehavior")
 _JOperationControl = jpy.get_type("io.deephaven.api.updateby.OperationControl")
+_JDeltaControl = jpy.get_type("io.deephaven.api.updateby.DeltaControl")
 _JMathContext = jpy.get_type("java.math.MathContext")
 _JDateTimeUtils = jpy.get_type("io.deephaven.time.DateTimeUtils")
 
 
 class MathContext(Enum):
     """An Enum for predefined precision and rounding settings in numeric calculation."""
     UNLIMITED = _JMathContext.UNLIMITED
@@ -45,14 +46,27 @@
     THROW = _JBadDataBehavior.THROW
     """Throw an exception and abort processing when bad data is encountered"""
 
     POISON = _JBadDataBehavior.POISON
     """Allow the bad data to poison the result. This is only valid for use with NaN"""
 
 
+class DeltaControl(Enum):
+    """An Enum defining ways to handle null values during update-by Delta operations where delta operations return the
+    difference between the current row and the previous row."""
+
+    NULL_DOMINATES = _JDeltaControl.NULL_DOMINATES
+    """A valid value following a null value returns null"""
+
+    VALUE_DOMINATES = _JDeltaControl.VALUE_DOMINATES
+    """A valid value following a null value returns the valid value"""
+
+    ZERO_DOMINATES = _JDeltaControl.ZERO_DOMINATES
+    """A valid value following a null value returns zero"""
+
 class OperationControl(JObjectWrapper):
     """A OperationControl represents control parameters for performing operations with the table
     UpdateByOperation. """
     j_object_type = _JOperationControl
 
     @property
     def j_object(self) -> jpy.JType:
@@ -90,15 +104,15 @@
         self.j_updateby_op = j_updateby_op
 
     @property
     def j_object(self) -> jpy.JType:
         return self.j_updateby_op
 
 
-def ema_tick_decay(time_scale_ticks: int, cols: Union[str, List[str]],
+def ema_tick(time_scale_ticks: float, cols: Union[str, List[str]],
                    op_control: OperationControl = None) -> UpdateByOperation:
     """Creates an EMA (exponential moving average) UpdateByOperation for the supplied column names, using ticks as
     the decay unit.
 
     The formula used is
         a = e^(-1 / time_scale_ticks)
         ema_next = a * ema_last + (1 - a) * value
@@ -123,26 +137,25 @@
         else:
             return UpdateByOperation(
                 j_updateby_op=_JUpdateByOperation.Ema(op_control.j_op_control, time_scale_ticks, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a tick-decay EMA UpdateByOperation.") from e
 
 
-def ema_time_decay(ts_col: str, time_scale: Union[int, str], cols: Union[str, List[str]],
+def ema_time(ts_col: str, time_scale: Union[int, str], cols: Union[str, List[str]],
                    op_control: OperationControl = None) -> UpdateByOperation:
     """Creates an EMA(exponential moving average) UpdateByOperation for the supplied column names, using time as the
     decay unit.
 
     The formula used is
-        a = e^(-dt / time_scale_nanos)
+        a = e^(-dt / time_scale)
         ema_next = a * ema_last + (1 - a) * value
 
      Args:
         ts_col (str): the column in the source table to use for timestamps
-
         time_scale (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
             interval string, e.g. "00:00:00.001"
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the ema operation on all columns.
         op_control (OperationControl): defines how special cases should behave,  when None, the default OperationControl
             settings as specified in :meth:`~OperationControl.__init__` will be used
 
@@ -160,14 +173,222 @@
         else:
             return UpdateByOperation(
                 j_updateby_op=_JUpdateByOperation.Ema(op_control.j_op_control, ts_col, time_scale, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a time-decay EMA UpdateByOperation.") from e
 
 
+def ems_tick(time_scale_ticks: float, cols: Union[str, List[str]],
+                   op_control: OperationControl = None) -> UpdateByOperation:
+    """Creates an EMS (exponential moving sum) UpdateByOperation for the supplied column names, using ticks as
+    the decay unit.
+
+    The formula used is
+        a = e^(-1 / time_scale_ticks)
+        ems_next = a * ems_last + value
+
+    Args:
+        time_scale_ticks (int): the decay rate in ticks
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the ems operation on all columns.
+        op_control (OperationControl): defines how special cases should behave, when None, the default OperationControl
+            settings as specified in :meth:`~OperationControl.__init__` will be used
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+    """
+    try:
+        cols = to_sequence(cols)
+        if op_control is None:
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.Ems(time_scale_ticks, *cols))
+        else:
+            return UpdateByOperation(
+                j_updateby_op=_JUpdateByOperation.Ems(op_control.j_op_control, time_scale_ticks, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a tick-decay EMS UpdateByOperation.") from e
+
+
+def ems_time(ts_col: str, time_scale: Union[int, str], cols: Union[str, List[str]],
+                   op_control: OperationControl = None) -> UpdateByOperation:
+    """Creates an EMS (exponential moving sum) UpdateByOperation for the supplied column names, using time as the
+    decay unit.
+
+    The formula used is
+        a = e^(-dt / time_scale)
+        eems_next = a * ems_last + value
+
+     Args:
+        ts_col (str): the column in the source table to use for timestamps
+        time_scale (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "00:00:00.001"
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the ems operation on all columns.
+        op_control (OperationControl): defines how special cases should behave,  when None, the default OperationControl
+            settings as specified in :meth:`~OperationControl.__init__` will be used
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+     """
+    try:
+        time_scale = _JDateTimeUtils.expressionToNanos(time_scale) if isinstance(time_scale, str) else time_scale
+        cols = to_sequence(cols)
+        if op_control is None:
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.Ems(ts_col, time_scale, *cols))
+        else:
+            return UpdateByOperation(
+                j_updateby_op=_JUpdateByOperation.Ems(op_control.j_op_control, ts_col, time_scale, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a time-decay EMS UpdateByOperation.") from e
+
+
+def emmin_tick(time_scale_ticks: float, cols: Union[str, List[str]],
+                   op_control: OperationControl = None) -> UpdateByOperation:
+    """Creates an EM Min (exponential moving minimum) UpdateByOperation for the supplied column names, using ticks as
+    the decay unit.
+
+    The formula used is
+        a = e^(-1 / time_scale_ticks)
+        em_val_next = min(a * em_val_last, value)
+
+    Args:
+        time_scale_ticks (int): the decay rate in ticks
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the operation on all columns.
+        op_control (OperationControl): defines how special cases should behave, when None, the default OperationControl
+            settings as specified in :meth:`~OperationControl.__init__` will be used
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+    """
+    try:
+        cols = to_sequence(cols)
+        if op_control is None:
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmMin(time_scale_ticks, *cols))
+        else:
+            return UpdateByOperation(
+                j_updateby_op=_JUpdateByOperation.EmMin(op_control.j_op_control, time_scale_ticks, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a tick-decay EM Min UpdateByOperation.") from e
+
+
+def emmin_time(ts_col: str, time_scale: Union[int, str], cols: Union[str, List[str]],
+                   op_control: OperationControl = None) -> UpdateByOperation:
+    """Creates an EM Min (exponential moving minimum) UpdateByOperation for the supplied column names, using time as the
+    decay unit.
+
+    The formula used is
+        a = e^(-dt / time_scale)
+        em_val_next = min(a * em_val_last, value)
+
+     Args:
+        ts_col (str): the column in the source table to use for timestamps
+        time_scale (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "00:00:00.001"
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the operation on all columns.
+        op_control (OperationControl): defines how special cases should behave,  when None, the default OperationControl
+            settings as specified in :meth:`~OperationControl.__init__` will be used
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+     """
+    try:
+        time_scale = _JDateTimeUtils.expressionToNanos(time_scale) if isinstance(time_scale, str) else time_scale
+        cols = to_sequence(cols)
+        if op_control is None:
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmMin(ts_col, time_scale, *cols))
+        else:
+            return UpdateByOperation(
+                j_updateby_op=_JUpdateByOperation.EmMin(op_control.j_op_control, ts_col, time_scale, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a time-decay EM Min UpdateByOperation.") from e
+
+
+def emmax_tick(time_scale_ticks: float, cols: Union[str, List[str]],
+                     op_control: OperationControl = None) -> UpdateByOperation:
+    """Creates an EM Max (exponential moving maximum) UpdateByOperation for the supplied column names, using ticks as
+    the decay unit.
+
+    The formula used is
+        a = e^(-1 / time_scale_ticks)
+        em_val_next = max(a * em_val_last, value)
+
+    Args:
+        time_scale_ticks (int): the decay rate in ticks
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the operation on all columns.
+        op_control (OperationControl): defines how special cases should behave, when None, the default OperationControl
+            settings as specified in :meth:`~OperationControl.__init__` will be used
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+    """
+    try:
+        cols = to_sequence(cols)
+        if op_control is None:
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmMax(time_scale_ticks, *cols))
+        else:
+            return UpdateByOperation(
+                j_updateby_op=_JUpdateByOperation.EmMax(op_control.j_op_control, time_scale_ticks, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a tick-decay EM Max UpdateByOperation.") from e
+
+
+def emmax_time(ts_col: str, time_scale: Union[int, str], cols: Union[str, List[str]],
+                     op_control: OperationControl = None) -> UpdateByOperation:
+    """Creates an EM Max (exponential moving maximum) UpdateByOperation for the supplied column names, using time as the
+    decay unit.
+
+    The formula used is
+        a = e^(-dt / time_scale)
+        em_val_next = max(a * em_val_last, value)
+
+     Args:
+        ts_col (str): the column in the source table to use for timestamps
+
+        time_scale (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "00:00:00.001"
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the operation on all columns.
+        op_control (OperationControl): defines how special cases should behave,  when None, the default OperationControl
+            settings as specified in :meth:`~OperationControl.__init__` will be used
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+     """
+    try:
+        time_scale = _JDateTimeUtils.expressionToNanos(time_scale) if isinstance(time_scale, str) else time_scale
+        cols = to_sequence(cols)
+        if op_control is None:
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmMax(ts_col, time_scale, *cols))
+        else:
+            return UpdateByOperation(
+                j_updateby_op=_JUpdateByOperation.EmMax(op_control.j_op_control, ts_col, time_scale, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a time-decay EM Max UpdateByOperation.") from e
+
+
 def cum_sum(cols: Union[str, List[str]]) -> UpdateByOperation:
     """Creates a cumulative sum UpdateByOperation for the supplied column names.
 
     Args:
 
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by performs the cumulative sum operation on all the applicable
@@ -266,14 +487,46 @@
     try:
         cols = to_sequence(cols)
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.Fill(cols))
     except Exception as e:
         raise DHError(e, "failed to create a forward fill UpdateByOperation.") from e
 
 
+def delta(cols: Union[str, List[str]], delta_control: DeltaControl = DeltaControl.NULL_DOMINATES) -> UpdateByOperation:
+    """Creates a delta UpdateByOperation for the supplied column names. The Delta operation produces values by computing
+    the difference between the current value and the previous value. When the current value is null, this operation
+    will output null. When the current value is valid, the output will depend on the DeltaControl provided.
+
+        When delta_control is not provided or set to NULL_DOMINATES, a value following a null value returns null.
+        When delta_control is set to VALUE_DOMINATES, a value following a null value returns the value.
+        When delta_control is set to ZERO_DOMINATES, a value following a null value returns zero.
+
+    Args:
+
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by performs the cumulative sum operation on all the applicable
+            columns.
+
+        delta_control (DeltaControl): defines how special cases should behave, when None, the default DeltaControl
+            settings of VALUE_DOMINATES will be used
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+    """
+    try:
+        cols = to_sequence(cols)
+        return UpdateByOperation(j_updateby_op=_JUpdateByOperation.Delta(delta_control.value, *cols))
+
+    except Exception as e:
+        raise DHError(e, "failed to create a delta UpdateByOperation.") from e
+
+
 def rolling_sum_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling sum UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks 
     are row counts, and you may specify the reverse and forward window in number of rows to include. The current row
     is considered to belong to the reverse window but not the forward window. Also, negative values are allowed and 
     can be used to generate completely forward or completely reverse windows. 
 
     Here are some examples of window values:
@@ -284,15 +537,14 @@
         rev_ticks = 10, fwd_ticks = -5 - contains 5 rows, beginning at 9 rows before, ending at 5 rows before  the
             current row (inclusive)
         rev_ticks = 11, fwd_ticks = -1 - contains 10 rows, beginning at 10 rows before, ending at 1 row before the
             current row (inclusive)
         rev_ticks = -5, fwd_ticks = 10 - contains 5 rows, beginning 5 rows following, ending at 10 rows  following the
             current row (inclusive)
 
-
     Args:
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling sum operation on all columns.
         rev_ticks (int): the look-behind window size (in rows/ticks)
         fwd_ticks (int): the look-forward window size (int rows/ticks), default is 0
 
     Returns:
@@ -309,16 +561,16 @@
 
 
 def rolling_sum_time(ts_col: str, cols: Union[str, List[str]], rev_time: Union[int, str],
                      fwd_time: Union[int, str] = 0) -> UpdateByOperation:
     """Creates a rolling sum UpdateByOperation for the supplied column names, using time as the windowing unit. This
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
-    the timestamp column belongs to no window and will not have a value computed or be considered in the windows of
-    other rows.
+    the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
+    be null.
      
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
         rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
@@ -348,14 +600,15 @@
         cols = to_sequence(cols)
         rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
         fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingSum(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling sum (time) UpdateByOperation.") from e
 
+
 def rolling_group_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling group UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks 
     are row counts, and you may specify the reverse and forward window in number of rows to include. The current row
     is considered to belong to the reverse window but not the forward window. Also, negative values are allowed and 
     can be used to generate completely forward or completely reverse windows. 
 
     Here are some examples of window values:
@@ -366,15 +619,14 @@
         rev_ticks = 10, fwd_ticks = -5 - contains 5 rows, beginning at 9 rows before, ending at 5 rows before  the
             current row (inclusive)
         rev_ticks = 11, fwd_ticks = -1 - contains 10 rows, beginning at 10 rows before, ending at 1 row before the
             current row (inclusive)
         rev_ticks = -5, fwd_ticks = 10 - contains 5 rows, beginning 5 rows following, ending at 10 rows  following the
             current row (inclusive)
 
-
     Args:
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling group operation on all columns.
         rev_ticks (int): the look-behind window size (in rows/ticks)
         fwd_ticks (int): the look-forward window size (int rows/ticks), default is 0
 
     Returns:
@@ -391,16 +643,16 @@
 
 
 def rolling_group_time(ts_col: str, cols: Union[str, List[str]], rev_time: Union[int, str],
                      fwd_time: Union[int, str] = 0) -> UpdateByOperation:
     """Creates a rolling group UpdateByOperation for the supplied column names, using time as the windowing unit. This
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
-    the timestamp column belongs to no window and will not have a value computed or be considered in the windows of
-    other rows.
+    the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
+    be null.
      
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
         rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
@@ -430,14 +682,15 @@
         cols = to_sequence(cols)
         rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
         fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingGroup(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling group (time) UpdateByOperation.") from e
 
+
 def rolling_avg_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling average UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
     are row counts, and you may specify the reverse and forward window in number of rows to include. The current row
     is considered to belong to the reverse window but not the forward window. Also, negative values are allowed and
     can be used to generate completely forward or completely reverse windows.
 
     Here are some examples of window values:
@@ -448,15 +701,14 @@
         rev_ticks = 10, fwd_ticks = -5 - contains 5 rows, beginning at 9 rows before, ending at 5 rows before  the
             current row (inclusive)
         rev_ticks = 11, fwd_ticks = -1 - contains 10 rows, beginning at 10 rows before, ending at 1 row before the
             current row (inclusive)
         rev_ticks = -5, fwd_ticks = 10 - contains 5 rows, beginning 5 rows following, ending at 10 rows  following the
             current row (inclusive)
 
-
     Args:
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling average operation on all columns.
         rev_ticks (int): the look-behind window size (in rows/ticks)
         fwd_ticks (int): the look-forward window size (int rows/ticks), default is 0
 
     Returns:
@@ -473,16 +725,16 @@
 
 
 def rolling_avg_time(ts_col: str, cols: Union[str, List[str]], rev_time: Union[int, str],
                        fwd_time: Union[int, str] = 0) -> UpdateByOperation:
     """Creates a rolling average UpdateByOperation for the supplied column names, using time as the windowing unit. This
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
-    the timestamp column belongs to no window and will not have a value computed or be considered in the windows of
-    other rows.
+    the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
+    be null.
      
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
         rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
@@ -512,14 +764,15 @@
         cols = to_sequence(cols)
         rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
         fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingAvg(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling average (time) UpdateByOperation.") from e
 
+
 def rolling_min_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling minimum UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
     are row counts, and you may specify the reverse and forward window in number of rows to include. The current row
     is considered to belong to the reverse window but not the forward window. Also, negative values are allowed and
     can be used to generate completely forward or completely reverse windows.
 
     Here are some examples of window values:
@@ -530,15 +783,14 @@
         rev_ticks = 10, fwd_ticks = -5 - contains 5 rows, beginning at 9 rows before, ending at 5 rows before  the
             current row (inclusive)
         rev_ticks = 11, fwd_ticks = -1 - contains 10 rows, beginning at 10 rows before, ending at 1 row before the
             current row (inclusive)
         rev_ticks = -5, fwd_ticks = 10 - contains 5 rows, beginning 5 rows following, ending at 10 rows  following the
             current row (inclusive)
 
-
     Args:
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling minimum operation on all columns.
         rev_ticks (int): the look-behind window size (in rows/ticks)
         fwd_ticks (int): the look-forward window size (int rows/ticks), default is 0
 
     Returns:
@@ -555,16 +807,16 @@
 
 
 def rolling_min_time(ts_col: str, cols: Union[str, List[str]], rev_time: Union[int, str],
                        fwd_time: Union[int, str] = 0) -> UpdateByOperation:
     """Creates a rolling minimum UpdateByOperation for the supplied column names, using time as the windowing unit. This
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
-    the timestamp column belongs to no window and will not have a value computed or be considered in the windows of
-    other rows.
+    the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
+    be null.
      
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
         rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
@@ -594,14 +846,15 @@
         cols = to_sequence(cols)
         rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
         fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingMin(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling minimum (time) UpdateByOperation.") from e
 
+
 def rolling_max_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling maximum UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
     are row counts, and you may specify the reverse and forward window in number of rows to include. The current row
     is considered to belong to the reverse window but not the forward window. Also, negative values are allowed and
     can be used to generate completely forward or completely reverse windows.
 
     Here are some examples of window values:
@@ -612,15 +865,14 @@
         rev_ticks = 10, fwd_ticks = -5 - contains 5 rows, beginning at 9 rows before, ending at 5 rows before  the
             current row (inclusive)
         rev_ticks = 11, fwd_ticks = -1 - contains 10 rows, beginning at 10 rows before, ending at 1 row before the
             current row (inclusive)
         rev_ticks = -5, fwd_ticks = 10 - contains 5 rows, beginning 5 rows following, ending at 10 rows  following the
             current row (inclusive)
 
-
     Args:
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling maximum operation on all columns.
         rev_ticks (int): the look-behind window size (in rows/ticks)
         fwd_ticks (int): the look-forward window size (int rows/ticks), default is 0
 
     Returns:
@@ -637,16 +889,16 @@
 
 
 def rolling_max_time(ts_col: str, cols: Union[str, List[str]], rev_time: Union[int, str],
                        fwd_time: Union[int, str] = 0) -> UpdateByOperation:
     """Creates a rolling maximum UpdateByOperation for the supplied column names, using time as the windowing unit. This
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
-    the timestamp column belongs to no window and will not have a value computed or be considered in the windows of
-    other rows.
+    the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
+    be null.
      
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
         rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
@@ -676,14 +928,15 @@
         cols = to_sequence(cols)
         rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
         fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingMax(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling maximum (time) UpdateByOperation.") from e
 
+
 def rolling_prod_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling product UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
     are row counts, and you may specify the reverse and forward window in number of rows to include. The current row
     is considered to belong to the reverse window but not the forward window. Also, negative values are allowed and
     can be used to generate completely forward or completely reverse windows.
 
     Here are some examples of window values:
@@ -694,15 +947,14 @@
         rev_ticks = 10, fwd_ticks = -5 - contains 5 rows, beginning at 9 rows before, ending at 5 rows before  the
             current row (inclusive)
         rev_ticks = 11, fwd_ticks = -1 - contains 10 rows, beginning at 10 rows before, ending at 1 row before the
             current row (inclusive)
         rev_ticks = -5, fwd_ticks = 10 - contains 5 rows, beginning 5 rows following, ending at 10 rows  following the
             current row (inclusive)
 
-
     Args:
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling product operation on all columns.
         rev_ticks (int): the look-behind window size (in rows/ticks)
         fwd_ticks (int): the look-forward window size (int rows/ticks), default is 0
 
     Returns:
@@ -713,21 +965,22 @@
     """
     try:
         cols = to_sequence(cols)
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingProduct(rev_ticks, fwd_ticks, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling product (tick) UpdateByOperation.") from e
 
+
 def rolling_prod_time(ts_col: str, cols: Union[str, List[str]], rev_time: Union[int, str],
                        fwd_time: Union[int, str] = 0) -> UpdateByOperation:
     """Creates a rolling product UpdateByOperation for the supplied column names, using time as the windowing unit. This
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
-    the timestamp column belongs to no window and will not have a value computed or be considered in the windows of
-    other rows.
+    the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
+    be null.
      
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
         rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
@@ -755,8 +1008,258 @@
     """
     try:
         cols = to_sequence(cols)
         rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
         fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingProduct(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
-        raise DHError(e, "failed to create a rolling product (time) UpdateByOperation.") from e
+        raise DHError(e, "failed to create a rolling product (time) UpdateByOperation.") from e
+
+
+def rolling_count_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
+    """Creates a rolling count UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
+    are row counts, and you may specify the reverse and forward window in number of rows to include. The current row
+    is considered to belong to the reverse window but not the forward window. Also, negative values are allowed and
+    can be used to generate completely forward or completely reverse windows.
+
+    Here are some examples of window values:
+        rev_ticks = 1, fwd_ticks = 0 - contains only the current row
+        rev_ticks = 10, fwd_ticks = 0 - contains 9 previous rows and the current row
+        rev_ticks = 0, fwd_ticks = 10 - contains the following 10 rows, excludes the current row
+        rev_ticks = 10, fwd_ticks = 10 - contains the previous 9 rows, the current row and the 10 rows following
+        rev_ticks = 10, fwd_ticks = -5 - contains 5 rows, beginning at 9 rows before, ending at 5 rows before the
+            current row (inclusive)
+        rev_ticks = 11, fwd_ticks = -1 - contains 10 rows, beginning at 10 rows before, ending at 1 row before the
+            current row (inclusive)
+        rev_ticks = -5, fwd_ticks = 10 - contains 5 rows, beginning 5 rows following, ending at 10 rows following the
+            current row (inclusive)
+
+    Args:
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the rolling count operation on all columns.
+        rev_ticks (int): the look-behind window size (in rows/ticks)
+        fwd_ticks (int): the look-forward window size (int rows/ticks), default is 0
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+    """
+    try:
+        cols = to_sequence(cols)
+        return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingCount(rev_ticks, fwd_ticks, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a rolling count (tick) UpdateByOperation.") from e
+
+
+def rolling_count_time(ts_col: str, cols: Union[str, List[str]], rev_time: Union[int, str],
+                      fwd_time: Union[int, str] = 0) -> UpdateByOperation:
+    """Creates a rolling count UpdateByOperation for the supplied column names, using time as the windowing unit. This
+    function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
+    allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
+    the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
+    be null.
+
+    Here are some examples of window values:
+        rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
+        rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
+            inclusive)
+        rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
+            current row timestamp (inclusive)
+        rev_time = "00:10:00", fwd_time = "00:10:00" - contains rows from 10m before through 10m following
+            the current row timestamp (inclusive)
+        rev_time = "00:10:00", fwd_time = "-00:05:00" - contains rows from 10m before through 5m before the
+            current row timestamp (inclusive), this is a purely backwards looking window
+        rev_time = "-00:05:00", fwd_time = "00:10:00"} - contains rows from 5m following through 10m
+            following the current row timestamp (inclusive), this is a purely forwards looking window
+
+    Args:
+        ts_col (str): the timestamp column for determining the window
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the rolling count operation on all columns.
+        rev_time (int): the look-behind window size, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "00:00:00.001"
+        fwd_time (int): the look-ahead window size, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "00:00:00.001", default is 0
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+    """
+    try:
+        cols = to_sequence(cols)
+        rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
+        fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
+        return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingCount(ts_col, rev_time, fwd_time, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a rolling count (time) UpdateByOperation.") from e
+
+
+def rolling_std_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
+    """Creates a rolling standard deviation UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
+    are row counts, and you may specify the reverse and forward window in number of rows to include. The current row
+    is considered to belong to the reverse window but not the forward window. Also, negative values are allowed and
+    can be used to generate completely forward or completely reverse windows.
+
+    Here are some examples of window values:
+        rev_ticks = 1, fwd_ticks = 0 - contains only the current row
+        rev_ticks = 10, fwd_ticks = 0 - contains 9 previous rows and the current row
+        rev_ticks = 0, fwd_ticks = 10 - contains the following 10 rows, excludes the current row
+        rev_ticks = 10, fwd_ticks = 10 - contains the previous 9 rows, the current row and the 10 rows following
+        rev_ticks = 10, fwd_ticks = -5 - contains 5 rows, beginning at 9 rows before, ending at 5 rows before the
+            current row (inclusive)
+        rev_ticks = 11, fwd_ticks = -1 - contains 10 rows, beginning at 10 rows before, ending at 1 row before the
+            current row (inclusive)
+        rev_ticks = -5, fwd_ticks = 10 - contains 5 rows, beginning 5 rows following, ending at 10 rows following the
+            current row (inclusive)
+
+    Args:
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the rolling standard deviation operation on all columns.
+        rev_ticks (int): the look-behind window size (in rows/ticks)
+        fwd_ticks (int): the look-forward window size (int rows/ticks), default is 0
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+    """
+    try:
+        cols = to_sequence(cols)
+        return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingStd(rev_ticks, fwd_ticks, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a rolling standard deviation (tick) UpdateByOperation.") from e
+
+
+def rolling_std_time(ts_col: str, cols: Union[str, List[str]], rev_time: Union[int, str],
+                       fwd_time: Union[int, str] = 0) -> UpdateByOperation:
+    """Creates a rolling standard deviation UpdateByOperation for the supplied column names, using time as the windowing unit. This
+    function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
+    allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
+    the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
+    be null.
+
+    Here are some examples of window values:
+        rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
+        rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
+            inclusive)
+        rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
+            current row timestamp (inclusive)
+        rev_time = "00:10:00", fwd_time = "00:10:00" - contains rows from 10m before through 10m following
+            the current row timestamp (inclusive)
+        rev_time = "00:10:00", fwd_time = "-00:05:00" - contains rows from 10m before through 5m before the
+            current row timestamp (inclusive), this is a purely backwards looking window
+        rev_time = "-00:05:00", fwd_time = "00:10:00"} - contains rows from 5m following through 10m
+            following the current row timestamp (inclusive), this is a purely forwards looking window
+
+    Args:
+        ts_col (str): the timestamp column for determining the window
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the rolling standard deviation operation on all columns.
+        rev_time (int): the look-behind window size, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "00:00:00.001"
+        fwd_time (int): the look-ahead window size, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "00:00:00.001", default is 0
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+    """
+    try:
+        cols = to_sequence(cols)
+        rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
+        fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
+        return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingStd(ts_col, rev_time, fwd_time, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a rolling standard deviation (time) UpdateByOperation.") from e
+
+
+def rolling_wavg_tick(weight_col: str, cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
+    """Creates a rolling weighted average UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
+    are row counts, and you may specify the reverse and forward window in number of rows to include. The current row
+    is considered to belong to the reverse window but not the forward window. Also, negative values are allowed and
+    can be used to generate completely forward or completely reverse windows.
+
+    Here are some examples of window values:
+        rev_ticks = 1, fwd_ticks = 0 - contains only the current row
+        rev_ticks = 10, fwd_ticks = 0 - contains 9 previous rows and the current row
+        rev_ticks = 0, fwd_ticks = 10 - contains the following 10 rows, excludes the current row
+        rev_ticks = 10, fwd_ticks = 10 - contains the previous 9 rows, the current row and the 10 rows following
+        rev_ticks = 10, fwd_ticks = -5 - contains 5 rows, beginning at 9 rows before, ending at 5 rows before  the
+            current row (inclusive)
+        rev_ticks = 11, fwd_ticks = -1 - contains 10 rows, beginning at 10 rows before, ending at 1 row before the
+            current row (inclusive)
+        rev_ticks = -5, fwd_ticks = 10 - contains 5 rows, beginning 5 rows following, ending at 10 rows  following the
+            current row (inclusive)
+
+    Args:
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the rolling weighted average operation on all columns.
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the rolling weighted average operation on all columns.
+        weight_col (str):  the column containing the weight values
+        rev_ticks (int): the look-behind window size (in rows/ticks)
+        fwd_ticks (int): the look-forward window size (int rows/ticks), default is 0
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+    """
+    try:
+        cols = to_sequence(cols)
+        return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingWAvg(rev_ticks, fwd_ticks, weight_col, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a rolling weighted average (tick) UpdateByOperation.") from e
+
+
+def rolling_wavg_time(ts_col: str, weight_col: str, cols: Union[str, List[str]], rev_time: Union[int, str],
+                      fwd_time: Union[int, str] = 0) -> UpdateByOperation:
+    """Creates a rolling weighted average UpdateByOperation for the supplied column names, using time as the windowing unit. This
+    function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
+    allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
+    the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
+    be null.
+
+    Here are some examples of window values:
+        rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
+        rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
+            inclusive)
+        rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
+            current row timestamp (inclusive)
+        rev_time = "00:10:00", fwd_time = "00:10:00" - contains rows from 10m before through 10m following
+            the current row timestamp (inclusive)
+        rev_time = "00:10:00", fwd_time = "-00:05:00" - contains rows from 10m before through 5m before the
+            current row timestamp (inclusive), this is a purely backwards looking window
+        rev_time = "-00:05:00", fwd_time = "00:10:00"} - contains rows from 5m following through 10m
+            following the current row timestamp (inclusive), this is a purely forwards looking window
+
+    Args:
+        ts_col (str): the timestamp column for determining the window
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the rolling weighted average operation on all columns.
+        weight_col (str):  the column containing the weight values
+        rev_time (int): the look-behind window size, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "00:00:00.001"
+        fwd_time (int): the look-ahead window size, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "00:00:00.001", default is 0
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+    """
+    try:
+        cols = to_sequence(cols)
+        rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
+        fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
+        return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingWAvg(ts_col, rev_time, fwd_time, weight_col, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a rolling weighted average (time) UpdateByOperation.") from e
```

## deephaven_internal/auto_completer/_completer.py

```diff
@@ -184,15 +184,15 @@
     ) -> list[list[Any]]:
         """ Gets signature help at the position
 
         Modeled after Jedi language server
         https://github.com/pappasam/jedi-language-server/blob/main/jedi_language_server/server.py#L255
         """
         if not self._versions[uri] == version:
-            # if you aren't the newest completion, you get nothing, quickly
+            # if you aren't the newest, you get nothing, quickly
             return []
 
         completer = self.get_completer(uri)
         signatures = completer.get_signatures(line, col)
 
         results: list = []
         for signature in signatures:
@@ -208,28 +208,28 @@
             ]
             results.append(result)
 
         return results
 
     def do_hover(
         self, uri: str, version: int, line: int, col: int
-    ) -> list[str]:
+    ) -> str:
         """ Gets hover help at the position
 
         Modeled after Jedi language server
         https://github.com/pappasam/jedi-language-server/blob/main/jedi_language_server/server.py#L366
         """
         if not self._versions[uri] == version:
-            # if you aren't the newest completion, you get nothing, quickly
-            return []
+            # if you aren't the newest, you get nothing, quickly
+            return ''
 
         completer = self.get_completer(uri)
         hovers = completer.help(line, col)
         if not hovers or hovers[0].type == "keyword":
-            return ['']
+            return ''
 
         # LSP doesn't support multiple hovers really. Not sure if/when Jedi would return multiple either
         hover = hovers[0]
         signatures = hover.get_signatures()
         kind = hover.type
 
         header = ""
@@ -246,8 +246,8 @@
                 header = hover.description
 
         hoverstring = wrap_python(header)
         raw_docstring = hover.docstring(raw=True)
         if raw_docstring:
             hoverstring += '\n---\n' + wrap_plaintext(raw_docstring)
 
-        return [hoverstring.strip()]
+        return hoverstring.strip()
```

## deephaven_internal/jvm/__init__.py

```diff
@@ -1,36 +1,68 @@
 _is_ready = False
 
+
 def ready():
     """Marks the JVM as ready. Should be called by Deephaven implementation code. In the case of the Java server process,
     this should be called right after Python has been initialized. In the case of the embedded Python server process,
     this should be called right after the JVM has been initialized."""
     global _is_ready
     _is_ready = True
 
+
 def check_ready():
     """Checks if the JVM is ready (ie, if ready() has been called). Should be called by Deephaven implementation code.
-    Raises an exception if the JVM is not ready."""
+    Raises a RuntimeError if the JVM is not ready.
+
+    Raises:
+        RuntimeError
+    """
     # Note: we might be tempted to store the source of truth for this in Java, but we aren't able to do that.
     # `import jpy` has potential side effects, and may improperly start the JVM.
     global _is_ready
     if not _is_ready:
-        raise Exception("The Deephaven Server has not been initialized. "
+        raise RuntimeError("The Deephaven Server has not been initialized. "
                         "Please ensure that deephaven_server.Server has been constructed before importing deephaven.")
 
+
+def check_py_env():
+    """Checks if the current Python environment is in good order and if not, raises a RuntimeError.
+
+    Raises:
+        RuntimeError
+    """
+    import importlib.metadata
+    try:
+        importlib.metadata.version("deephaven")
+    except:
+        pass
+    else:
+        # DH Enterprise deephaven package is installed by mistake
+        raise RuntimeError("The Deephaven Enterprise Python Package (name 'deephaven' on pypi) is installed in "
+                           "the current Python environment. It conflicts with the Deephaven Community Python "
+                           "Package (name 'deephaven-core' on pypi). Please uninstall the 'deephaven' package and "
+                           "reinstall the 'deephaven-core' package.")
+
+
 def preload_jvm_dll(*args, **kwargs):
     """A wrapper around jpyutil.preload_jvm_dll(...)."""
     import jpyutil
     result = jpyutil.preload_jvm_dll(*args, **kwargs)
     return result
 
+
 def init_jvm(*args, **kwargs):
     """A wrapper around jpyutil.init_jvm(...). Should be called by Deephaven implementation code.
-    Calls ready() after jpyutil.init_jvm(...)."""
+    Calls ready() after jpyutil.init_jvm(...).
+
+    Raises:
+        ImportError
+    """
     # Note: we might be able to use our own logic instead of jpyutil here in the future
     import jpyutil
     try:
         result = jpyutil.init_jvm(*args, **kwargs)
     except ImportError as e:
-        raise ImportError("Unable to initialize JVM, try setting the environment variable JAVA_HOME (JDK 11+ required)") from e
+        raise ImportError(
+            "Unable to initialize JVM, try setting the environment variable JAVA_HOME (JDK 11+ required)") from e
     ready()
     return result
```

## Comparing `deephaven_core-0.23.0.dist-info/METADATA` & `deephaven_core-0.24.0.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: deephaven-core
-Version: 0.23.0
+Version: 0.24.0
 Summary: Deephaven Engine Python Package
 Home-page: https://deephaven.io/
 Author: Deephaven Data Labs
 Author-email: python@deephaven.io
 License: Deephaven Community License
 Keywords: Deephaven Development
 Platform: UNKNOWN
@@ -22,15 +22,15 @@
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 Requires-Dist: jpy (>=0.13.0)
 Requires-Dist: deephaven-plugin
 Requires-Dist: numpy
 Requires-Dist: pandas
 Requires-Dist: pyarrow
-Requires-Dist: numba ; python_version < "3.11"
+Requires-Dist: numba ; python_version < "3.12"
 Provides-Extra: autocomplete
 Requires-Dist: jedi (==0.18.2) ; extra == 'autocomplete'
 
 
 # Deephaven Python Integration Package
 
 Deephaven Python Integration Package is created by Deephaven Data Labs. It allows Python developers, including data
```

## Comparing `deephaven_core-0.23.0.dist-info/RECORD` & `deephaven_core-0.24.0.dist-info/RECORD`

 * *Files 17% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-deephaven/__init__.py,sha256=YpL9-E4yAfy8TEbirgu-_Vmc1HaMFGB4cw6rw9uo55c,827
+deephaven/__init__.py,sha256=G4mP3h18Bna8qtXn9ikSgOsD1i0aNFJy4W27g3sVzxI,1120
 deephaven/_gc.py,sha256=Ej3zzkhgOEdT6lqj7LIzDH8eLgybW68716zpHjeNSgQ,970
 deephaven/_jpy.py,sha256=jczrjWcy2bMRCGpWS7UUFO2AWAG5cURUr4oTxq23EGw,676
 deephaven/_wrapper.py,sha256=R2TFX5myaVvl00Kj8LthRd9frMNnUO4TpK99vCpH-8M,4551
 deephaven/agg.py,sha256=oZRjNRqyyWGd4blkksdNmNoNf-YVrYU5NMfRA4nIdKk,10817
 deephaven/appmode.py,sha256=6r8L679hn7zRFfXCDXuYPUpgsLPMLqg8Zp0xzPy0QU4,1987
-deephaven/arrow.py,sha256=ogZDpLle23iJQNskn7BDH6eL55SCT77bIuBW8qQGMio,4727
+deephaven/arrow.py,sha256=G2FKr5ua9jt6XMqe97Ri9oCp-bhJdzqEbDwvrY7GrHU,4776
 deephaven/calendar.py,sha256=grvrTgPrTAYJz8DjxbgrGc5JHGHrHRkQtpPo8YEa-hk,14670
-deephaven/column.py,sha256=bxigQpaB0oq2k4v2Z3qDjFZkA2s6_zxwCGevub5dC6Y,7157
+deephaven/column.py,sha256=TylYMvChr7xYsBfhtwfwTWeglvu99fybGHamEuuYIDI,7276
 deephaven/constants.py,sha256=rhvNKqZoG0iwmjVdY0GPvycj1-c1pcSiXT0umzPahVo,3123
 deephaven/csv.py,sha256=5OQDIjc0_1DDvugaJHBN_T40PcmHh6Ik8tMOJHL1AOk,4515
 deephaven/dherror.py,sha256=QTHto3BTLGzThFsB2yaUvLrFG9b9slwM3DAdimmQm6A,2890
-deephaven/dtypes.py,sha256=CmumfZZEt2lOZwbgvcEPYUKktTDjaUfzxYXeNE-caGc,8142
+deephaven/dtypes.py,sha256=mu8e0nU6Mc1QwunmKZw9dRNAGJpRWU-JoxjMrjAIQv8,9049
 deephaven/execution_context.py,sha256=JvcJxKd3Zid-6uBEIUr9yVJY8tIT2H7v76vVrkz49fY,3517
-deephaven/filters.py,sha256=VuTRmbganstGil4BFtN6YwTxrAB9R44qPBnXsigcy3w,3186
+deephaven/filters.py,sha256=u5rPV0yDdh-Kx5jV_sNAKiBQqj-aPl0q_b0Jcptbu7c,4964
 deephaven/html.py,sha256=zJM2Qa7mwa-bmsRq91mJgdQ63nZ8zYbT6TgBmJPZx7g,650
 deephaven/jcompat.py,sha256=uriYEPqPhrhAO4V_L6qL85tCqdxO3i3B_YHD9oqydmI,4696
 deephaven/liveness_scope.py,sha256=LOTkbIjoMtfdPBNpZkEn-1pQS0MTeRZAwD6DDKDjXhc,2503
-deephaven/numpy.py,sha256=G77WhxF7fDR703DtIAnXLSZiU4V-9LHbMo1pLqcZfRM,6109
-deephaven/pandas.py,sha256=DuMkiNji8-sRCqqI5wLoX67EI3N_dHd-D8nvSKW_fio,5110
+deephaven/numpy.py,sha256=f_ouyNUO-x7-IN4TIgcT_cejRT6URVrHwV8CdZ9IrRA,6109
+deephaven/pandas.py,sha256=n3ll2RlfCuFfG45bf42u0vPgoXe-9K13Mlt6ic1x4Ro,8745
 deephaven/parquet.py,sha256=2swJSd98amMBqK8G1xEuMYSLli_sH2mYssx4TPuXwGw,9173
 deephaven/perfmon.py,sha256=8O98Rk0NviHG2XeInuYDxJTB2YKe7tRCif-GCoX1sjQ,8081
 deephaven/query_library.py,sha256=8z_SsylV4ZbANjiWCVqDgEOnjXHyKj9xlHfz3FMPT5Q,2690
 deephaven/replay.py,sha256=hVK8J79hnf57-7bKhf5vT85y71fKXWpc_QCa9W8Shyc,2471
-deephaven/table.py,sha256=4opAjEPqaB_EnIC6lxhQOWVYGtBk6abLiscrQtmoWMY,147667
-deephaven/table_factory.py,sha256=MClZOcvDdWIUfIizzMnWu41SD52gscIiv81JeuE_d00,10134
+deephaven/table.py,sha256=W7QGNWQZ57FjEomgB-wOsIqtKEy9qs4gyasMI5QT9rA,154549
+deephaven/table_factory.py,sha256=_aY6xjK82BduDwXNgxXJ7wYeNQ5ogWexrk1MYa3aAwo,11182
 deephaven/table_listener.py,sha256=66mXyKWwFKAHJ6ovEIDJUCk5tztzLyJMfnZlcuM3Km8,15976
 deephaven/time.py,sha256=HzafT8Kl5oKRdBFNtvxcYUxkO9LkfZ8Kp0-sCpzr3v4,21914
 deephaven/ugp.py,sha256=isoY2u4-whYCfj3nc7iumOPwAhJgDZSOINTeSd5RzB8,4712
-deephaven/updateby.py,sha256=aNNoO1PIpskFOqtneIRky7BFv8qiICFRhIWu1HUYHkk,40759
+deephaven/updateby.py,sha256=hSxslNqgQsmXotl24xyB8htlH1dOaSSp_RrtIit6KRg,67886
 deephaven/uri.py,sha256=lUD5LT_cIcKAkOVJd9bBmjVhxzB9OVJJowqIvBMPWDk,1155
 deephaven/config/__init__.py,sha256=IEgAvTwLORH5zKDkbDe0S-n6bdjt-T2wtUFUhQTvMFY,842
 deephaven/dbc/__init__.py,sha256=-kINZXfFQ_rjPKtE5yXXmV6SoxhFEkaPwPhhBdhoFiI,3200
 deephaven/dbc/adbc.py,sha256=bnl0nS-fgpaS1ZV8OLYph7WaqRs8uFv9GiE861sW9TA,1794
 deephaven/dbc/odbc.py,sha256=Sm54IOlEucnZk8N7XlOizjrKIJ1NHxmMbEsrJwv_V8U,1668
 deephaven/experimental/__init__.py,sha256=dJEBTCuRgxLJx45AEBpju6YkE9ShLAiNPJkDiTtEDeY,1316
 deephaven/experimental/ema.py,sha256=Ncdf1C4tbrI_Z8o1f9UZ0Lw_GFsH2HZDxR8XA8Y3sfI,4815
@@ -60,14 +60,14 @@
 deephaven/stream/kafka/cdc.py,sha256=P1DMVK1ZRdcr4TwAfO4-lc6O-Q6fdV4bNVMX_IO0Pcc,7082
 deephaven/stream/kafka/consumer.py,sha256=eHpGtXE7Fg56W45TP5KQCeLfojPPUKhk7adj-FAbydo,17567
 deephaven/stream/kafka/producer.py,sha256=obCfOKBYzXNg0VViy-h7qXcN-xvKcobrTvYWBP_qFOw,10818
 deephaven_internal/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 deephaven_internal/java_threads.py,sha256=xuCtrDHJNzPkbKoHBTAj7j-lAWNlaQJqleQ-LdP4aT8,1576
 deephaven_internal/stream.py,sha256=OTQuxFnJmok29-28XPd4Nmt6RNk_RnD5ppQkNddlKZY,3426
 deephaven_internal/auto_completer/__init__.py,sha256=2zPuxVzSsw5ZGpOFrvU3UY59s7qC13vCNTOWZoNKqiA,952
-deephaven_internal/auto_completer/_completer.py,sha256=x3AYrPWVEzrtKV8EMdpdeISjPSVDITHsXXoLXskRGl4,8508
-deephaven_internal/jvm/__init__.py,sha256=Zk8mWEKYR4YRROL7O6mmD62zX4CsCCiRewJNns-r51Y,1724
-deephaven_core-0.23.0.dist-info/METADATA,sha256=mOeMjd9jBr-hH3gRvX135Py5z2lbYH2LJ558t0wo4cA,2650
-deephaven_core-0.23.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-deephaven_core-0.23.0.dist-info/entry_points.txt,sha256=SdGFMIrwRwAlv0x_hKsuvQl176FayFeWUM64ujCbGnI,87
-deephaven_core-0.23.0.dist-info/top_level.txt,sha256=EshtqOS_YqliZnhyqyyQOCTJARwRdEW_XueIFQ9gJB4,29
-deephaven_core-0.23.0.dist-info/RECORD,,
+deephaven_internal/auto_completer/_completer.py,sha256=5gFt4Uu2dJQD_5VM8gE67DYhbKUzL6G2lI_piRh0jss,8476
+deephaven_internal/jvm/__init__.py,sha256=K0q4isvz9_4Ulq7hcH7dmxo5mLrdmeMIr99rKu5k1fs,2585
+deephaven_core-0.24.0.dist-info/METADATA,sha256=kEXTfeF5P6rrkpWvm6TQlZcxKssT5r-tfAhQQjAcRrU,2650
+deephaven_core-0.24.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+deephaven_core-0.24.0.dist-info/entry_points.txt,sha256=SdGFMIrwRwAlv0x_hKsuvQl176FayFeWUM64ujCbGnI,87
+deephaven_core-0.24.0.dist-info/top_level.txt,sha256=EshtqOS_YqliZnhyqyyQOCTJARwRdEW_XueIFQ9gJB4,29
+deephaven_core-0.24.0.dist-info/RECORD,,
```

