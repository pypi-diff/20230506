# Comparing `tmp/uss-0.0.1-py3-none-any.whl.zip` & `tmp/uss-0.0.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,44 +1,44 @@
-Zip file size: 51312 bytes, number of entries: 42
+Zip file size: 50486 bytes, number of entries: 42
 -rw-r--r--  2.0 unx        0 b- defN 23-May-06 05:43 uss/__init__.py
 -rw-r--r--  2.0 unx     2114 b- defN 23-May-06 02:59 uss/casa.py
--rw-r--r--  2.0 unx     1932 b- defN 23-May-06 07:15 uss/config.py
--rw-r--r--  2.0 unx     6021 b- defN 23-May-06 07:15 uss/evaluate.py
--rw-r--r--  2.0 unx     6652 b- defN 23-May-06 07:15 uss/evaluate_musdb18.py
--rw-r--r--  2.0 unx    23657 b- defN 23-May-06 07:15 uss/inference.py
+-rw-r--r--  2.0 unx     1922 b- defN 23-May-06 13:57 uss/config.py
+-rw-r--r--  2.0 unx     6022 b- defN 23-May-06 13:57 uss/evaluate.py
+-rw-r--r--  2.0 unx     6570 b- defN 23-May-06 09:03 uss/evaluate_musdb18.py
+-rw-r--r--  2.0 unx    23903 b- defN 23-May-06 13:57 uss/inference.py
 -rw-r--r--  2.0 unx     2295 b- defN 23-May-06 07:15 uss/losses.py
--rw-r--r--  2.0 unx     2941 b- defN 23-May-06 07:15 uss/parse_ontology.py
+-rw-r--r--  2.0 unx     2934 b- defN 23-May-06 13:57 uss/parse_ontology.py
 -rw-r--r--  2.0 unx     9028 b- defN 23-May-06 07:15 uss/test9.py
--rw-r--r--  2.0 unx    10772 b- defN 23-May-06 07:15 uss/train.py
+-rw-r--r--  2.0 unx    10445 b- defN 23-May-06 13:57 uss/train.py
 -rw-r--r--  2.0 unx     2114 b- defN 23-May-06 02:59 uss/uss.py
--rw-r--r--  2.0 unx     2114 b- defN 23-May-06 07:15 uss/uss_inference.py
--rw-r--r--  2.0 unx     6144 b- defN 23-May-06 07:15 uss/utils.py
+-rw-r--r--  2.0 unx     1977 b- defN 23-May-06 13:57 uss/uss_inference.py
+-rw-r--r--  2.0 unx     6234 b- defN 23-May-06 13:57 uss/utils.py
 -rw-r--r--  2.0 unx        0 b- defN 23-May-06 02:58 uss/callbacks/__init__.py
 -rw-r--r--  2.0 unx     1062 b- defN 23-May-06 07:15 uss/callbacks/base.py
--rw-r--r--  2.0 unx     3389 b- defN 23-May-06 07:15 uss/callbacks/evaluate.py
+-rw-r--r--  2.0 unx     3484 b- defN 23-May-06 13:57 uss/callbacks/evaluate.py
 -rw-r--r--  2.0 unx        0 b- defN 23-May-06 03:01 uss/data/__init__.py
--rw-r--r--  2.0 unx     7873 b- defN 23-May-06 07:15 uss/data/anchor_segment_detectors.py
--rw-r--r--  2.0 unx     2699 b- defN 23-May-06 07:15 uss/data/anchor_segment_mixers.py
--rw-r--r--  2.0 unx     2875 b- defN 23-May-06 07:15 uss/data/datamodules.py
--rw-r--r--  2.0 unx     1917 b- defN 23-May-06 07:15 uss/data/datasets.py
--rw-r--r--  2.0 unx     8330 b- defN 23-May-06 07:15 uss/data/samplers.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-06 03:02 uss/dataset_creation/__init__.py
+-rw-r--r--  2.0 unx     7896 b- defN 23-May-06 13:57 uss/data/anchor_segment_detectors.py
+-rw-r--r--  2.0 unx     2663 b- defN 23-May-06 13:57 uss/data/anchor_segment_mixers.py
+-rw-r--r--  2.0 unx     2855 b- defN 23-May-06 13:57 uss/data/datamodules.py
+-rw-r--r--  2.0 unx     1876 b- defN 23-May-06 13:57 uss/data/datasets.py
+-rw-r--r--  2.0 unx     5898 b- defN 23-May-06 13:57 uss/data/samplers.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-06 08:36 uss/dataset_creation/__init__.py
 -rw-r--r--  2.0 unx     4686 b- defN 23-May-06 03:02 uss/dataset_creation/_create_audioset_indexes.py
--rw-r--r--  2.0 unx     9928 b- defN 23-May-06 07:15 uss/dataset_creation/create_audioset_evaluation_meta.py
+-rw-r--r--  2.0 unx     9953 b- defN 23-May-06 13:57 uss/dataset_creation/create_audioset_evaluation_meta.py
 -rw-r--r--  2.0 unx        0 b- defN 23-May-06 03:03 uss/models/__init__.py
--rw-r--r--  2.0 unx     4485 b- defN 23-May-06 07:15 uss/models/base.py
--rw-r--r--  2.0 unx     4476 b- defN 23-May-06 07:15 uss/models/film.py
--rw-r--r--  2.0 unx     4965 b- defN 23-May-06 07:15 uss/models/pl_modules.py
--rw-r--r--  2.0 unx     8976 b- defN 23-May-06 07:15 uss/models/query_nets.py
--rw-r--r--  2.0 unx    17962 b- defN 23-May-06 07:15 uss/models/resunet.py
+-rw-r--r--  2.0 unx     4855 b- defN 23-May-06 13:57 uss/models/base.py
+-rw-r--r--  2.0 unx     4263 b- defN 23-May-06 13:57 uss/models/film.py
+-rw-r--r--  2.0 unx     4937 b- defN 23-May-06 13:57 uss/models/pl_modules.py
+-rw-r--r--  2.0 unx     9039 b- defN 23-May-06 13:57 uss/models/query_nets.py
+-rw-r--r--  2.0 unx    18238 b- defN 23-May-06 13:57 uss/models/resunet.py
 -rw-r--r--  2.0 unx        0 b- defN 23-May-06 03:04 uss/optimizers/__init__.py
--rw-r--r--  2.0 unx     2512 b- defN 23-May-06 07:15 uss/optimizers/lr_schedulers.py
+-rw-r--r--  2.0 unx     2498 b- defN 23-May-06 13:57 uss/optimizers/lr_schedulers.py
 -rw-r--r--  2.0 unx        0 b- defN 23-May-06 03:04 uss/plots/__init__.py
 -rw-r--r--  2.0 unx     1703 b- defN 23-May-06 07:15 uss/plots/plot.py
 -rw-r--r--  2.0 unx      597 b- defN 23-May-06 07:15 uss/plots/plot_paper_stats.py
--rw-r--r--  2.0 unx      550 b- defN 23-May-06 07:36 uss-0.0.1.dist-info/LICENSE
--rw-r--r--  2.0 unx      551 b- defN 23-May-06 07:36 uss-0.0.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-06 07:36 uss-0.0.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       47 b- defN 23-May-06 07:36 uss-0.0.1.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        4 b- defN 23-May-06 07:36 uss-0.0.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3289 b- defN 23-May-06 07:36 uss-0.0.1.dist-info/RECORD
-42 files, 168752 bytes uncompressed, 46144 bytes compressed:  72.7%
+-rw-r--r--  2.0 unx      550 b- defN 23-May-06 14:00 uss-0.0.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx      551 b- defN 23-May-06 14:00 uss-0.0.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-06 14:00 uss-0.0.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx       47 b- defN 23-May-06 14:00 uss-0.0.2.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        4 b- defN 23-May-06 14:00 uss-0.0.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     3289 b- defN 23-May-06 14:00 uss-0.0.2.dist-info/RECORD
+42 files, 166594 bytes uncompressed, 45318 bytes compressed:  72.8%
```

## zipnote {}

```diff
@@ -102,26 +102,26 @@
 
 Filename: uss/plots/plot.py
 Comment: 
 
 Filename: uss/plots/plot_paper_stats.py
 Comment: 
 
-Filename: uss-0.0.1.dist-info/LICENSE
+Filename: uss-0.0.2.dist-info/LICENSE
 Comment: 
 
-Filename: uss-0.0.1.dist-info/METADATA
+Filename: uss-0.0.2.dist-info/METADATA
 Comment: 
 
-Filename: uss-0.0.1.dist-info/WHEEL
+Filename: uss-0.0.2.dist-info/WHEEL
 Comment: 
 
-Filename: uss-0.0.1.dist-info/entry_points.txt
+Filename: uss-0.0.2.dist-info/entry_points.txt
 Comment: 
 
-Filename: uss-0.0.1.dist-info/top_level.txt
+Filename: uss-0.0.2.dist-info/top_level.txt
 Comment: 
 
-Filename: uss-0.0.1.dist-info/RECORD
+Filename: uss-0.0.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## uss/config.py

```diff
@@ -1,14 +1,13 @@
-import os
 from pathlib import Path
+
 import pandas as pd
 
 from uss.utils import get_path
 
-
 csv_paths_dict = {
     "class_labels_indices.csv": {
         "path": Path(Path.home(), ".cache/uss/metadata/class_labels_indices.csv"),
         "remote_path": "https://sandbox.zenodo.org/record/1186898/files/class_labels_indices.csv?download=1",
         "size": 14675,
     },
     "ontology.csv": {
```

## uss/evaluate.py

```diff
@@ -7,15 +7,16 @@
 import librosa
 import lightning.pytorch as pl
 import numpy as np
 import torch
 
 from uss.config import IX_TO_LB
 from uss.inference import load_ss_model
-from uss.utils import create_logging, calculate_sdr, parse_yaml, get_mean_sdr_from_dict
+from uss.utils import (calculate_sdr, create_logging, get_mean_sdr_from_dict,
+                       parse_yaml)
 
 
 class AudioSetEvaluator:
     def __init__(
         self,
         audios_dir: str,
         classes_num: int,
@@ -189,15 +190,15 @@
                 "{} {}: {:.3f}".format(
                     class_id,
                     IX_TO_LB[class_id],
                     median_sdris[class_id]))
 
         mean_sdri = get_mean_sdr_from_dict(median_sdris)
         # final_sdri = np.nanmean([mean_sdris[class_id]
-                                # for class_id in range(classes_num)])
+        # for class_id in range(classes_num)])
         print("--------")
         print("Average SDRi: {:.3f}".format(mean_sdri))
 
 
 if __name__ == "__main__":
 
     test_evaluate(
```

## uss/evaluate_musdb18.py

```diff
@@ -10,15 +10,15 @@
 import lightning.pytorch as pl
 import matplotlib.pyplot as plt
 import numpy as np
 import soundfile
 import torch
 import torch.nn as nn
 
-from uss.config import ID_TO_IX, LB_TO_IX, IX_TO_LB, csv_paths_dict, panns_paths_dict, model_paths_dict
+from uss.config import ID_TO_IX, LB_TO_IX, IX_TO_LB, csv_paths_dict, panns_paths_dict
 from uss.models.pl_modules import LitSeparation, get_model_class
 from uss.models.query_nets import initialize_query_net
 from uss.parse_ontology import Node, get_ontology_tree
 from uss.utils import (get_audioset632_id_to_lb, load_pretrained_panns,
                         parse_yaml, remove_silence, repeat_to_length, get_path)
 from uss.inference import load_ss_model, calculate_query_emb, separate_by_query_condition
 
@@ -79,16 +79,14 @@
     pickle_path = Path("./query_conditions", "config={}".format(Path(config_yaml).stem), "{}.pkl".format(Path(queries_dir).stem))
     
     pickle_path.parent.mkdir(parents=True, exist_ok=True)
 
     pickle.dump(query_condition, open(pickle_path, 'wb'))
     print("Write query condition to {}".format(pickle_path))
 
-    from IPython import embed; embed(using=False); os._exit(0)
-
 
 def evaluate(args):
     r"""Do separation for active sound classes."""
 
     # Arguments & parameters
     query_emb_path = args.query_emb_path
     config_yaml = args.config_yaml
```

## uss/inference.py

```diff
@@ -1,29 +1,31 @@
 import argparse
 import os
-import time
 import pickle
+import time
+import warnings
 from pathlib import Path
 from typing import Dict, List
-import warnings
 
 import librosa
 import lightning.pytorch as pl
 import matplotlib.pyplot as plt
 import numpy as np
 import soundfile
 import torch
 import torch.nn as nn
 
-from uss.config import ID_TO_IX, LB_TO_IX, IX_TO_LB, csv_paths_dict, panns_paths_dict
+from uss.config import (ID_TO_IX, IX_TO_LB, LB_TO_IX, csv_paths_dict,
+                        panns_paths_dict)
 from uss.models.pl_modules import LitSeparation, get_model_class
 from uss.models.query_nets import initialize_query_net
 from uss.parse_ontology import Node, get_ontology_tree
-from uss.utils import (get_audioset632_id_to_lb, load_pretrained_panns,
-                        parse_yaml, remove_silence, repeat_to_length, get_path)
+from uss.utils import (get_audioset632_id_to_lb, get_path,
+                       load_pretrained_panns, parse_yaml, remove_silence,
+                       repeat_to_length)
 
 
 def separate(args) -> None:
     r"""Do separation for active sound classes."""
 
     # Arguments & parameters
     audio_path = args.audio_path
@@ -51,15 +53,15 @@
         output_dir = os.path.join(
             "separated_results",
             Path(audio_path).stem)
 
     # Load pretrained universal source separation model
     print("Loading model ...")
 
-    warnings.filterwarnings("ignore", category=UserWarning) 
+    warnings.filterwarnings("ignore", category=UserWarning)
 
     pl_model = load_ss_model(
         configs=configs,
         checkpoint_path=checkpoint_path,
     ).to(device)
 
     # Load audio
@@ -71,53 +73,53 @@
     at_model = load_pretrained_panns(
         model_type=at_model_type,
         checkpoint_path=get_path(panns_paths_dict[at_model_type]),
         freeze=True,
     ).to(device)
 
     flag_sum = sum([
-        len(levels) > 0, 
-        len(class_ids) > 0, 
-        len(queries_dir) > 0, 
+        len(levels) > 0,
+        len(class_ids) > 0,
+        len(queries_dir) > 0,
         len(query_emb_path) > 0,
     ])
 
     assert flag_sum in [0, 1], "Please use only `levels` or `class_ids` or \
         `queries_dir` or `query_emb_path` arguments."
 
     if flag_sum == 0:
         levels = [1, 2, 3]
 
     print("Separating ...")
 
     # Separate by hierarchy
     if len(levels) > 0:
         separate_by_hierarchy(
-            audio=audio, 
-            sample_rate=sample_rate, 
-            segment_samples=segment_samples, 
-            at_model=at_model, 
+            audio=audio,
+            sample_rate=sample_rate,
+            segment_samples=segment_samples,
+            at_model=at_model,
             pl_model=pl_model,
-            device=device, 
-            levels=levels, 
-            ontology_path=ontology_path, 
-            non_sil_threshold=non_sil_threshold, 
+            device=device,
+            levels=levels,
+            ontology_path=ontology_path,
+            non_sil_threshold=non_sil_threshold,
             output_dir=output_dir
         )
 
     # Separate by class IDs
     elif len(class_ids) > 0:
         separate_by_class_ids(
-            audio=audio, 
-            sample_rate=sample_rate, 
-            segment_samples=segment_samples, 
-            at_model=at_model, 
+            audio=audio,
+            sample_rate=sample_rate,
+            segment_samples=segment_samples,
+            at_model=at_model,
             pl_model=pl_model,
-            device=device, 
-            class_ids=class_ids, 
+            device=device,
+            class_ids=class_ids,
             output_dir=output_dir
         )
 
     # Calculate query embedding and do separation
     elif len(queries_dir) > 0:
 
         print("Calculate query condition ...")
@@ -128,43 +130,52 @@
             pl_model=pl_model,
             sample_rate=sample_rate,
             remove_sil=True,
             segment_samples=segment_samples,
         )
 
         print("Time: {:.3f} s".format(time.time() - query_time))
-        
-        pickle_path = os.path.join("./query_conditions", "config={}".format(Path(config_yaml).stem), "{}.pkl".format(Path(queries_dir).stem))
-        
+
+        pickle_path = os.path.join(
+            "./query_conditions",
+            "config={}".format(
+                Path(config_yaml).stem),
+            "{}.pkl".format(
+                Path(queries_dir).stem))
+
         os.makedirs(os.path.dirname(pickle_path), exist_ok=True)
 
         pickle.dump(query_condition, open(pickle_path, 'wb'))
         print("Write query condition to {}".format(pickle_path))
 
-        output_path = os.path.join(output_dir, "query={}.wav".format(Path(queries_dir).stem))
+        output_path = os.path.join(
+            output_dir, "query={}.wav".format(
+                Path(queries_dir).stem))
 
         separate_by_query_condition(
-            audio=audio, 
-            segment_samples=segment_samples, 
+            audio=audio,
+            segment_samples=segment_samples,
             sample_rate=sample_rate,
             query_condition=query_condition,
             pl_model=pl_model,
             output_path=output_path,
         )
 
     # Load pre-calculated query embedding and do separation
     elif Path(query_emb_path).is_file():
-        
+
         query_condition = pickle.load(open(query_emb_path, 'rb'))
 
-        output_path = os.path.join(output_dir, "query={}.wav".format(Path('111').stem))
+        output_path = os.path.join(
+            output_dir, "query={}.wav".format(
+                Path('111').stem))
 
         separate_by_query_condition(
-            audio=audio, 
-            segment_samples=segment_samples, 
+            audio=audio,
+            segment_samples=segment_samples,
             sample_rate=sample_rate,
             query_condition=query_condition,
             pl_model=pl_model,
             output_path=output_path,
         )
 
 
@@ -210,29 +221,30 @@
         anchor_segment_detector=None,
         anchor_segment_mixer=None,
         query_net=query_net,
         loss_function=None,
         optimizer_type=None,
         learning_rate=None,
         lr_lambda_func=None,
+        map_location="cpu",
     )
 
     return pl_model
 
 
 def separate_by_hierarchy(
-    audio: np.ndarray, 
-    sample_rate: int, 
-    segment_samples: int, 
-    at_model: nn.Module, 
-    pl_model: pl.LightningModule, 
-    device: str, 
-    levels: List[int], 
-    ontology_path: str, 
-    non_sil_threshold: float, 
+    audio: np.ndarray,
+    sample_rate: int,
+    segment_samples: int,
+    at_model: nn.Module,
+    pl_model: pl.LightningModule,
+    device: str,
+    levels: List[int],
+    ontology_path: str,
+    non_sil_threshold: float,
     output_dir: str,
 ) -> None:
     r"""Separate by hierarchy."""
 
     audioset632_id_to_lb = get_audioset632_id_to_lb(
         ontology_path=ontology_path)
 
@@ -277,15 +289,15 @@
             )
             # sep_audio: (audio_samples,)
 
             # Write out separated audio
             label = audioset632_id_to_lb[class_id]
 
             output_name = "{}.wav".format(label)
-            
+
             # if label in LB_TO_IX.keys():
             #     output_name = "classid={}_{}.wav".format(LB_TO_IX[label], label)
             # else:
             #     output_name = "classid=unknown_{}.wav".format(label)
 
             output_path = os.path.join(
                 output_dir,
@@ -309,21 +321,21 @@
             origin="lower",
             aspect="auto",
             cmap="jet")
         plt.savefig("_zz_{}.pdf".format(level))
 
 
 def separate_by_class_ids(
-    audio: np.ndarray, 
-    sample_rate: int, 
-    segment_samples: int, 
-    at_model: nn.Module, 
-    pl_model: pl.LightningModule, 
-    device: str, 
-    class_ids: List[int], 
+    audio: np.ndarray,
+    sample_rate: int,
+    segment_samples: int,
+    at_model: nn.Module,
+    pl_model: pl.LightningModule,
+    device: str,
+    class_ids: List[int],
     output_dir: str,
 ) -> None:
     r"""Separate by class IDs."""
 
     at_probs = calculate_segment_at_probs(
         audio=audio,
         segment_samples=segment_samples,
@@ -339,15 +351,16 @@
         subclass_indexes=class_ids,
         pl_model=pl_model,
         device=device,
     )
     # sep_audio: (audio_samples,)
 
     # Write out separated audio
-    output_name = ";".join(["{}_{}".format(class_id, IX_TO_LB[class_id]) for class_id in class_ids])
+    output_name = ";".join(
+        ["{}_{}".format(class_id, IX_TO_LB[class_id]) for class_id in class_ids])
     output_name += ".wav"
 
     output_path = os.path.join(
         output_dir,
         output_name,
     )
 
@@ -355,23 +368,23 @@
         audio=sep_audio,
         output_path=output_path,
         sample_rate=sample_rate,
     )
 
 
 def calculate_query_emb(
-    queries_dir: str, 
-    pl_model: pl.LightningModule, 
-    sample_rate: int, 
-    remove_sil: bool, 
+    queries_dir: str,
+    pl_model: pl.LightningModule,
+    sample_rate: int,
+    remove_sil: bool,
     segment_samples: int,
     batch_size=8,
 ) -> np.ndarray:
     r"""Calculate the query embddings of audio files in a directory."""
-    
+
     audio_names = sorted(os.listdir(queries_dir))
 
     avg_query_conditions = []
 
     # Average query conditions of all audios
     for audio_index, audio_name in enumerate(audio_names):
 
@@ -382,27 +395,28 @@
         audio, fs = librosa.load(path=audio_path, sr=sample_rate, mono=True)
 
         # Remove silence
         if remove_sil:
             audio = remove_silence(audio=audio, sample_rate=sample_rate)
 
         audio_samples = audio.shape[0]
-        
+
         segments_num = int(np.ceil(audio_samples / segment_samples))
 
         segments = []
 
         # Get all segments
         for segment_index in range(segments_num):
 
             begin_sample = segment_index * segment_samples
             end_sample = begin_sample + segment_samples
 
-            segment = audio[begin_sample : end_sample]
-            segment = repeat_to_length(audio=segment, segment_samples=segment_samples)
+            segment = audio[begin_sample: end_sample]
+            segment = repeat_to_length(
+                audio=segment, segment_samples=segment_samples)
             segments.append(segment)
 
         if len(segments) == 0:
             continue
 
         segments = np.stack(segments, axis=0)
 
@@ -612,15 +626,15 @@
         device (str), e.g., "cpu" | "cuda"
 
     Returns:
         sep_audio (np.ndarray): separated audio
     """
 
     audio_samples = audio.shape[-1]
-    
+
     segments_num = int(np.ceil(audio_samples / segment_samples))
 
     segments = []
 
     # Collect active segments
     for segment_index in range(segments_num):
 
@@ -636,15 +650,15 @@
         segments.append(segment)
 
     segments = np.stack(segments, axis=0)
 
     # Do separation in mini-batch
     pointer = 0
     sep_segments = []
-    
+
     while pointer < len(segments):
 
         batch_segments = segments[pointer: pointer + batch_size]
 
         batch_sep_segments = _do_sep_by_query_in_minibatch(
             batch_segments=batch_segments,
             query_condition=query_condition,
@@ -655,15 +669,18 @@
 
     sep_segments = np.concatenate(sep_segments, axis=0)
 
     sep_audio = sep_segments.flatten()[0: audio_samples]
 
     if output_path:
         os.makedirs(os.path.dirname(output_path), exist_ok=True)
-        soundfile.write(file=output_path, data=sep_audio, samplerate=sample_rate)
+        soundfile.write(
+            file=output_path,
+            data=sep_audio,
+            samplerate=sample_rate)
         print("Write out separated file to {}".format(output_path))
 
     return sep_audio
 
 
 def _do_sep_by_id_in_minibatch(
     batch_segments: np.ndarray,
@@ -689,15 +706,17 @@
     with torch.no_grad():
         pl_model.query_net.eval()
 
         bottleneck = pl_model.query_net.forward_base(source=batch_segments)
         # bottleneck: (batch_size, bottleneck_dim)
 
         masked_bottleneck = torch.zeros_like(bottleneck)
-        masked_bottleneck[:, subclass_indexes] = bottleneck[:, subclass_indexes]
+        masked_bottleneck[:,
+                          subclass_indexes] = bottleneck[:,
+                                                         subclass_indexes]
 
         condition = pl_model.query_net.forward_adaptor(masked_bottleneck)
         # condition: (batch_size, condition_dim)
 
     input_dict = {
         "mixture": torch.Tensor(batch_segments.unsqueeze(1)),
         "condition": torch.Tensor(condition),
@@ -764,15 +783,15 @@
         pl_model (pl.LightningModule): universal separation model
 
     Returns:
         batch_condition (np.ndarray): mini-batch conditions.
     """
 
     device = next(query_net.parameters()).device
-    
+
     batch_segments = torch.Tensor(batch_segments).to(device)
     # shape: (batch_size, segment_samples)
 
     with torch.no_grad():
         query_net.eval()
 
         batch_condition = query_net.forward(source=batch_segments)["output"]
@@ -801,15 +820,15 @@
 
     os.makedirs(os.path.dirname(output_path), exist_ok=True)
     soundfile.write(file=output_path, data=audio, samplerate=sample_rate)
     print("Write out to {}".format(output_path))
 
 
 if __name__ == "__main__":
-    
+
     parser = argparse.ArgumentParser()
     parser.add_argument("--audio_path", type=str, required=True)
     parser.add_argument("--levels", nargs="*", type=int, default=[])
     parser.add_argument("--class_ids", nargs="*", type=int, default=[])
     parser.add_argument("--queries_dir", type=str, default="")
     parser.add_argument("--query_emb_path", type=str, default="")
     parser.add_argument("--config_yaml", type=str, default="")
```

## uss/parse_ontology.py

```diff
@@ -1,9 +1,9 @@
 import json
-from typing import List, Union
+from typing import List
 
 from uss.config import ID_TO_IX, IDS, ROOT_CLASS_ID_DICT
 
 
 class Node:
     def __init__(self, data, level):
         r"""Sound class Node.
```

## uss/train.py

```diff
@@ -16,15 +16,16 @@
 from uss.data.datamodules import DataModule
 from uss.data.datasets import Dataset
 from uss.data.samplers import BalancedSampler
 from uss.losses import get_loss_function
 from uss.models.pl_modules import LitSeparation, get_model_class
 from uss.models.query_nets import initialize_query_net
 from uss.optimizers.lr_schedulers import get_lr_lambda
-from uss.utils import create_logging, load_pretrained_panns, parse_yaml, get_path
+from uss.utils import (create_logging, get_path, load_pretrained_panns,
+                       parse_yaml)
 
 
 def train(args) -> None:
     r"""Train, evaluate, and save checkpoints.
 
     Args:
         workspace (str): directory of workspace
@@ -43,16 +44,14 @@
     devices_num = torch.cuda.device_count()
 
     # Read config file
     configs = parse_yaml(config_yaml)
 
     # Configurations of pretrained sound event detection model from PANNs
     sed_model_type = configs["sound_event_detection"]["model_type"]
-    # sed_checkpoint_path = configs["sound_event_detection"]["checkpoint_path"]
-    sed_freeze = configs["sound_event_detection"]["freeze"]
 
     # Configuration of data to train the universal source separation system
     clip_seconds = CLIP_SECONDS
     frames_per_second = FRAMES_PER_SECOND
     sample_rate = configs["data"]["sample_rate"]
     classes_num = configs["data"]["classes_num"]
     segment_seconds = configs["data"]["segment_seconds"]
@@ -96,20 +95,14 @@
     datamodule = get_datamodule(
         workspace=workspace,
         config_yaml=config_yaml,
         num_workers=num_workers,
         devices_num=devices_num,
     )
 
-    # Load pretrained sound event detection model
-    # sed_model = load_pretrained_panns(
-    #     model_type=sed_model_type,
-    #     checkpoint_path=sed_checkpoint_path,
-    #     freeze=sed_freeze,
-    # )
     sed_model = load_pretrained_panns(
         model_type=sed_model_type,
         checkpoint_path=get_path(panns_paths_dict[sed_model_type]),
         freeze=True,
     )
 
     # Initialize query net
```

## uss/uss_inference.py

```diff
@@ -1,63 +1,58 @@
 import argparse
-import os
-import time
-import pickle
-import pathlib
-from typing import Dict, List
-
-import librosa
-import lightning.pytorch as pl
-import matplotlib.pyplot as plt
-import numpy as np
-import soundfile
-import torch
-import torch.nn as nn
 from pathlib import Path
 
 from uss.inference import separate
 from uss.utils import get_path
 
-
 model_paths_dict = {
     "at_soft": {
         "config_yaml": {
             "path": Path(Path.home(), ".cache/uss/scripts/ss_model=resunet30,querynet=at_soft,data=full.yaml"),
-            "remote_path": "https://sandbox.zenodo.org/record/1187598/files/ss_model%3Dresunet30%2Cquerynet%3Dat_soft%2Cdata%3Dfull.yaml?download=1",
+            "remote_path": "https://sandbox.zenodo.org/record/1196562/files/ss_model%3Dresunet30%2Cquerynet%3Dat_soft%2Cdata%3Dfull.yaml?download=1",
             "size": 1558,
         },
         "checkpoint": {
-            "path": Path(Path.home(), ".cache/uss/checkpoints/ss_model=resunet30,querynet=at_soft,data=full,devices=8,step=100000.ckpt"),
-            "remote_path": "https://sandbox.zenodo.org/record/1186898/files/ss_model%3Dresunet30%2Cquerynet%3Dat_soft%2Cdata%3Dfull%2Cdevices%3D8%2Cstep%3D100000.ckpt?download=1",
+            "path": Path(Path.home(), ".cache/uss/checkpoints/ss_model=resunet30,querynet=at_soft,data=full,devices=8,step=1000000.ckpt"),
+            "remote_path": "https://sandbox.zenodo.org/record/1196562/files/ss_model%3Dresunet30%2Cquerynet%3Dat_soft%2Cdata%3Dfull%2Cdevices%3D8%2Cstep%3D1000000.ckpt?download=1",
             "size": 1121024828,
         },
     }
 }
 
 
 def main():
 
     parser = argparse.ArgumentParser()
     parser.add_argument("-i", "--audio_path", type=str, required=True)
-    parser.add_argument("-c", "--condition_type", type=str, default="at_soft", choices=["at_soft", "embedding"])
+    parser.add_argument(
+        "-c",
+        "--condition_type",
+        type=str,
+        default="at_soft",
+        choices=[
+            "at_soft",
+            "embedding"])
     parser.add_argument("--levels", nargs="*", type=int, default=[])
     parser.add_argument("--class_ids", nargs="*", type=int, default=[])
     parser.add_argument("--queries_dir", type=str, default="")
     parser.add_argument("--query_emb_path", type=str, default="")
     parser.add_argument("--output_dir", type=str, default="")
 
     args = parser.parse_args()
 
     condition_type = args.condition_type
 
     # Use default pretrained models
     if condition_type == "at_soft":
-        args.config_yaml = get_path(meta=model_paths_dict[condition_type]["config_yaml"])
-        args.checkpoint_path = get_path(meta=model_paths_dict[condition_type]["checkpoint"])
+        args.config_yaml = get_path(
+            meta=model_paths_dict[condition_type]["config_yaml"])
+        args.checkpoint_path = get_path(
+            meta=model_paths_dict[condition_type]["checkpoint"])
 
     elif condition_type == "embedding":
         pass
 
     else:
         raise NotImplementedError
 
-    separate(args)
+    separate(args)
```

## uss/utils.py

```diff
@@ -1,16 +1,16 @@
 import datetime
 import json
 import logging
 import os
-from pathlib import Path
-import librosa
 import pickle
+from pathlib import Path
 from typing import Dict
 
+import librosa
 import numpy as np
 import torch
 import torch.nn as nn
 import yaml
 from panns_inference.models import Cnn14, Cnn14_DecisionLevelMax
 
 
@@ -61,15 +61,15 @@
 
     with open(config_yaml, "r") as fr:
         return yaml.load(fr, Loader=yaml.FullLoader)
 
 
 def get_audioset632_id_to_lb(ontology_path: str) -> Dict:
     r"""Get AudioSet 632 classes ID to label mapping."""
-    
+
     audioset632_id_to_lb = {}
 
     with open(ontology_path) as f:
         data_list = json.load(f)
 
     for e in data_list:
         audioset632_id_to_lb[e["id"]] = e["name"]
@@ -175,30 +175,37 @@
         self.statistics_dict[split].append(statistics)
 
         if flush:
             self.flush()
 
     def flush(self):
         pickle.dump(self.statistics_dict, open(self.statistics_path, "wb"))
-        pickle.dump(self.statistics_dict, open(self.backup_statistics_path, "wb"))
+        pickle.dump(
+            self.statistics_dict, open(
+                self.backup_statistics_path, "wb"))
         logging.info("    Dump statistics to {}".format(self.statistics_path))
-        logging.info("    Dump statistics to {}".format(self.backup_statistics_path))
+        logging.info(
+            "    Dump statistics to {}".format(
+                self.backup_statistics_path))
 
 
 def get_mean_sdr_from_dict(sdris_dict):
     mean_sdr = np.nanmean(list(sdris_dict.values()))
     return mean_sdr
 
 
 def remove_silence(audio: np.ndarray, sample_rate: int) -> np.ndarray:
     r"""Remove silent frames."""
     window_size = int(sample_rate * 0.1)
     threshold = 0.02
 
-    frames = librosa.util.frame(x=audio, frame_length=window_size, hop_length=window_size).T
+    frames = librosa.util.frame(
+        x=audio,
+        frame_length=window_size,
+        hop_length=window_size).T
     # shape: (frames_num, window_size)
 
     new_frames = get_active_frames(frames, threshold)
     # shape: (new_frames_num, window_size)
 
     new_audio = new_frames.flatten()
     # shape: (new_audio_samples,)
@@ -219,27 +226,28 @@
     # shape: (new_frames_num,)
 
     return new_frames
 
 
 def repeat_to_length(audio: np.ndarray, segment_samples: int) -> np.ndarray:
     r"""Repeat audio to length."""
-    
+
     repeats_num = (segment_samples // audio.shape[-1]) + 1
-    audio = np.tile(audio, repeats_num)[0 : segment_samples]
+    audio = np.tile(audio, repeats_num)[0: segment_samples]
 
     return audio
 
 
 def get_path(meta, re_download=False):
 
     path = meta["path"]
     remote_path = meta["remote_path"]
     size = meta["size"]
 
-    if not Path(path).is_file() or Path(path).stat().st_size != size or re_download:
+    if not Path(path).is_file() or Path(
+            path).stat().st_size != size or re_download:
 
         Path(path).parents[0].mkdir(parents=True, exist_ok=True)
         os.system("wget -O {} {}".format(path, remote_path))
         print("Download to {}".format(path))
 
-    return path
+    return path
```

## uss/callbacks/evaluate.py

```diff
@@ -1,17 +1,15 @@
-import os
+import logging
+
 import lightning.pytorch as pl
 from lightning.pytorch.utilities import rank_zero_only
+from torch.utils.tensorboard import SummaryWriter
 
 from uss.evaluate import AudioSetEvaluator
 from uss.utils import StatisticsContainer, get_mean_sdr_from_dict
-from torch.utils.tensorboard import SummaryWriter
-
-import numpy as np
-import logging
 
 
 class EvaluateCallback(pl.Callback):
     def __init__(
         self,
         pl_model: pl.LightningModule,
         balanced_train_eval_dir: str,
@@ -33,25 +31,25 @@
             evaluate_step_frequency (int): evaluate every N steps
             summary_writer (SummaryWriter): used to write TensorBoard logs
             statistics_path (str): path to write statistics
 
         Returns:
             None
         """
-        
+
         # Evaluators
         self.balanced_train_evaluator = AudioSetEvaluator(
-            audios_dir=balanced_train_eval_dir, 
-            classes_num=classes_num, 
+            audios_dir=balanced_train_eval_dir,
+            classes_num=classes_num,
             max_eval_per_class=max_eval_per_class,
         )
 
         self.test_evaluator = AudioSetEvaluator(
-            audios_dir=test_eval_dir, 
-            classes_num=classes_num, 
+            audios_dir=test_eval_dir,
+            classes_num=classes_num,
             max_eval_per_class=max_eval_per_class,
         )
 
         self.pl_model = pl_model
 
         self.evaluate_step_frequency = evaluate_step_frequency
 
@@ -66,31 +64,37 @@
 
         trainer = args[0]
         epoch = trainer.current_epoch
         global_step = trainer.global_step
 
         if global_step == 1 or global_step % self.evaluate_step_frequency == 0:
 
-            for split, evaluator in zip(["balanced_train", "test"], [self.balanced_train_evaluator, self.test_evaluator]):
+            for split, evaluator in zip(["balanced_train", "test"], [
+                                        self.balanced_train_evaluator, self.test_evaluator]):
 
                 logging.info("------ {} ------".format(split))
 
                 stats_dict = evaluator(pl_model=self.pl_model)
 
                 median_sdris_dict = AudioSetEvaluator.get_median_metrics(
-                    stats_dict=stats_dict, 
+                    stats_dict=stats_dict,
                     metric_type="sdris_dict",
                 )
 
                 median_sdri = get_mean_sdr_from_dict(median_sdris_dict)
                 logging.info("Average SDRi: {:.3f}".format(median_sdri))
 
-                self.summary_writer.add_scalar("SDRi/{}".format(split), global_step=global_step, scalar_value=median_sdri)
-
-                logging.info("    Flush tensorboard logs to {}".format(self.summary_writer.log_dir))
+                self.summary_writer.add_scalar(
+                    "SDRi/{}".format(split),
+                    global_step=global_step,
+                    scalar_value=median_sdri)
+
+                logging.info(
+                    "    Flush tensorboard logs to {}".format(
+                        self.summary_writer.log_dir))
 
                 self.statistics_container.append(
-                    steps=global_step, 
-                    statistics={"sdri_dict": median_sdris_dict}, 
+                    steps=global_step,
+                    statistics={"sdri_dict": median_sdris_dict},
                     split=split,
                     flush=True,
-                )         
+                )
```

## uss/data/anchor_segment_detectors.py

```diff
@@ -3,23 +3,23 @@
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 
 class AnchorSegmentDetector(nn.Module):
-    def __init__(self, 
-        sed_model: nn.Module, 
-        clip_seconds: float,
-        segment_seconds: float,
-        frames_per_second: int,
-        sample_rate: float,
-        detect_mode: str,
-    ) -> None:
-        r"""The anchor segment detector is used to detect 2-second anchor 
+    def __init__(self,
+                 sed_model: nn.Module,
+                 clip_seconds: float,
+                 segment_seconds: float,
+                 frames_per_second: int,
+                 sample_rate: float,
+                 detect_mode: str,
+                 ) -> None:
+        r"""The anchor segment detector is used to detect 2-second anchor
         segments from 10-second weakly labelled audio clips during training.
 
         Args:
             sed_model (nn.Module): pretrained sound event detection model
             clip_seconds (float): audio clip duration, e.g., 10.
             segment_seconds (float): anchor segment duration, e.g., 2.
             frames_per_second (int):, e.g., 100
@@ -35,27 +35,27 @@
 
         self.sed_model = sed_model
         self.clip_frames = int(clip_seconds * frames_per_second)
         self.segment_frames = int(segment_seconds * frames_per_second + 1)
         self.hop_samples = sample_rate // frames_per_second
         self.sample_rate = sample_rate
         self.detect_mode = detect_mode
-        
+
         # Used to the area under the probability curve of anchor segments
         self.anchor_segment_scorer = AnchorSegmentScorer(
             segment_frames=self.segment_frames,
         )
 
     def __call__(
-        self, 
-        waveforms: torch.Tensor, 
+        self,
+        waveforms: torch.Tensor,
         class_ids: List,
         debug: bool = False,
     ) -> Dict:
-        r"""Input a batch of 10-second audio clips. Mine 2-second anchor 
+        r"""Input a batch of 10-second audio clips. Mine 2-second anchor
         segments.
 
         Args:
             waveforms (torch.Tensor): (batch_size, clip_samples)
             class_ids (List): (batch_size,)
             debug (bool)
 
@@ -71,28 +71,28 @@
         batch_size, _ = waveforms.shape
 
         # Sound event detection
         with torch.no_grad():
             self.sed_model.eval()
 
             framewise_output = self.sed_model(
-                input=waveforms, 
+                input=waveforms,
             )['framewise_output']
             # (batch_size, clip_frames, classes_num)
 
         segments = []
         bgn_samples = []
         end_samples = []
 
         # Detect the anchor segments of a mini-batch of audio clips one by one
         for n in range(batch_size):
 
             # Class ID of the current 10-second clip.
             class_id = class_ids[n]
-            # There can be multiple tags in the 10-second clip. We only detect 
+            # There can be multiple tags in the 10-second clip. We only detect
             # the anchor segment of #class_id
 
             prob_array = framewise_output[n, :, class_id]
             # shape: (segment_frames,)
 
             if self.detect_mode == "max_area":
 
@@ -101,33 +101,33 @@
                     prob_array=prob_array,
                 )
 
                 anchor_index = torch.argmax(anchor_segment_scores)
 
                 if debug:
                     _debug_plot_anchor_segment(
-                        waveform=waveforms[0], 
-                        anchor_segment_scores=anchor_segment_scores, 
+                        waveform=waveforms[0],
+                        anchor_segment_scores=anchor_segment_scores,
                         class_id=class_id,
                     )
 
             elif self.detect_mode == "random":
                 anchor_index = random.randint(0, self.segment_frames - 1)
                 anchor_index = torch.tensor(anchor_index).to(waveforms.device)
-                
+
             else:
                 raise NotImplementedError
-            
+
             # Get begin and end samples of an anchor segment.
             bgn_sample, end_sample = self.get_segment_bgn_end_samples(
                 anchor_index=anchor_index,
                 clip_frames=self.clip_frames,
             )
 
-            segment = waveforms[n, bgn_sample : end_sample]
+            segment = waveforms[n, bgn_sample: end_sample]
 
             segments.append(segment)
             bgn_samples.append(bgn_sample)
             end_samples.append(end_sample)
 
         segments = torch.stack(segments, dim=0)
         # (batch_size, segment_samples)
@@ -137,124 +137,128 @@
 
         segments_dict = {
             'waveform': segments,
             'class_id': class_ids,
             'bgn_sample': bgn_samples,
             'end_sample': end_samples,
         }
-        
+
         return segments_dict
 
     def get_segment_bgn_end_samples(
-        self, 
-        anchor_index: int, 
+        self,
+        anchor_index: int,
         clip_frames: int
     ) -> Tuple[int, int]:
         r"""Get begin and end samples of an anchor segment.
 
         Args:
             anchor_index (torch.int): e.g., 155
 
         Returns:
             bgn_sample (torch.int), e.g., 17600
             end_sample (torch.int): e.g., 81600
         """
-        
+
         anchor_index = torch.clamp(
-            input=anchor_index, 
+            input=anchor_index,
             min=self.segment_frames // 2,
             max=clip_frames - self.segment_frames // 2,
         )
 
         bgn_frame = anchor_index - self.segment_frames // 2
         end_frame = anchor_index + self.segment_frames // 2
 
         bgn_sample = bgn_frame * self.hop_samples
         end_sample = end_frame * self.hop_samples
 
         return bgn_sample, end_sample
 
 
 def _debug_plot_anchor_segment(
-    waveform: torch.Tensor, 
-    anchor_segment_scores: torch.Tensor, 
+    waveform: torch.Tensor,
+    anchor_segment_scores: torch.Tensor,
     class_id: int,
 ) -> None:
     r"""For debug only. Plot anchor segment prediction."""
-    
+
     import os
-    import soundfile
+
     import matplotlib.pyplot as plt
+    import soundfile
+
     from uss.config import IX_TO_LB
     sample_rate = 32000
 
     n = 0
     audio_path = os.path.join("_debug_anchor_segment{}.wav".format(n))
     fig_path = os.path.join("_debug_anchor_segment{}.pdf".format(n))
-    
+
     soundfile.write(
-        file=audio_path, 
-        data=waveform.data.cpu().numpy(), 
+        file=audio_path,
+        data=waveform.data.cpu().numpy(),
         samplerate=sample_rate,
     )
     print("Write out to {}".format(audio_path))
-    
+
     plt.figure()
     plt.plot(anchor_segment_scores.data.cpu().numpy())
     plt.title(IX_TO_LB[class_id])
     plt.ylim(0, 1)
     plt.savefig(fig_path)
     print("Write out to {}".format(fig_path))
 
     os._exit(0)
 
 
 class AnchorSegmentScorer(nn.Module):
-    def __init__(self, 
-        segment_frames: int,
-    ) -> None:
+    def __init__(self,
+                 segment_frames: int,
+                 ) -> None:
         r"""Calculate the area under the probabiltiy curve of an anchor segment.
-    
+
         Args:
             segment_frames (int)
 
         Returns:
             None
         """
 
         super(AnchorSegmentScorer, self).__init__()
 
         self.segment_frames = segment_frames
 
         filter_len = self.segment_frames
-        self.register_buffer('smooth_filter', torch.ones(1, 1, filter_len) / filter_len)
+        self.register_buffer(
+            'smooth_filter', torch.ones(
+                1, 1, filter_len) / filter_len)
 
     def __call__(self, prob_array: torch.Tensor):
         r"""Calculate the area under the probabiltiy curve of an anchor segment.
 
         Args:
-            prob_array (torch.Tensor): (clip_frames,), sound event 
+            prob_array (torch.Tensor): (clip_frames,), sound event
                 detection probability of a specific sound class.
 
         Returns:
-            output: (clip_frames,), smoothed probability, equivalent to the 
+            output: (clip_frames,), smoothed probability, equivalent to the
                 area of probability of anchor segments.
         """
-        
+
         x = F.pad(
-            input=prob_array[None, None, :], 
-            pad=(self.segment_frames // 2,self.segment_frames // 2),
+            input=prob_array[None, None, :],
+            pad=(self.segment_frames // 2, self.segment_frames // 2),
             mode='replicate'
         )
         # shape: (1, 1, clip_frames)
 
         output = torch.conv1d(
-            input=x, 
-            weight=self.smooth_filter, 
+            input=x,
+            weight=self.smooth_filter,
             padding=0,
         )
         # shape: (1, 1, clip_frames)
 
         output = output.squeeze(dim=(0, 1))
         # (clip_frames,)
 
-        return output
+        return output
```

## uss/data/anchor_segment_mixers.py

```diff
@@ -1,25 +1,24 @@
 from typing import Dict
 
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
 
 class AnchorSegmentMixer(nn.Module):
     def __init__(
-        self, 
+        self,
         mix_num: int,
         match_energy: bool,
     ) -> None:
         r"""Anchor segment mixer. Used to mix multiple sources into a mixture.
 
         Args:
             mix_num (int): the number of sources to mix
-            match_energy (bool): set to `True` to rescale segments to have the 
+            match_energy (bool): set to `True` to rescale segments to have the
                 same energy before mixing
 
         Returns:
             None
         """
 
         super(AnchorSegmentMixer, self).__init__()
@@ -49,31 +48,32 @@
             mixture = segment.clone()
 
             for i in range(1, self.mix_num):
 
                 next_segment = waveforms[(n + i) % batch_size].clone()
 
                 if self.match_energy:
-                    # Rescale the energy of the next_segment to match the energy of 
+                    # Rescale the energy of the next_segment to match the energy of
                     # the segment
-                    next_segment = rescale_to_match_energy(segment, next_segment)
-                
+                    next_segment = rescale_to_match_energy(
+                        segment, next_segment)
+
                 mixture += next_segment
 
             targets.append(segment)
             mixtures.append(mixture)
 
         targets = torch.stack(targets, dim=0)
         mixtures = torch.stack(mixtures, dim=0)
-        
+
         return mixtures, targets
 
 
 def rescale_to_match_energy(
-    segment1: torch.Tensor, 
+    segment1: torch.Tensor,
     segment2: torch.Tensor,
 ) -> torch.Tensor:
     r"""Rescale segment2 to match the energy of segment1.
 
     Args:
         segment1 (torch.Tensor), signal
         segment2 (torch.Tensor), signal
@@ -87,19 +87,19 @@
 def get_energy(x: torch.Tensor) -> torch.float:
     r"""Calculate the energy of a signal."""
 
     return torch.mean(x ** 2)
 
 
 def get_energy_ratio(
-    segment1: torch.Tensor, 
-    segment2: torch.Tensor, 
+    segment1: torch.Tensor,
+    segment2: torch.Tensor,
     eps=1e-10
 ) -> float:
     r"""Calculate ratio = sqrt(E(s1) / E(s2))."""
 
     energy1 = get_energy(segment1)
     energy2 = get_energy(segment2)
     ratio = (energy1 / max(energy2, eps)) ** 0.5
     ratio = torch.clamp(ratio, 0.02, 50)
 
-    return ratio
+    return ratio
```

## uss/data/datamodules.py

```diff
@@ -1,14 +1,12 @@
-import os
-from typing import Dict, List, Optional, NoReturn
+from typing import Dict, List, Optional
 
+import lightning.pytorch as pl
 import numpy as np
-import h5py
 import torch
-import lightning.pytorch as pl
 from torch.utils.data import DataLoader
 
 from uss.data.samplers import DistributedSamplerWrapper
 
 
 class DataModule(pl.LightningDataModule):
     def __init__(
@@ -44,15 +42,15 @@
     def setup(self, stage: Optional[str] = None) -> None:
         r"""called on every GPU."""
 
         self.train_dataset = self._train_dataset
 
         # The sampler yields a part of mini-batch meta on each device
         self.train_sampler = DistributedSamplerWrapper(self._train_sampler)
-        
+
     def train_dataloader(self) -> torch.utils.data.DataLoader:
         r"""Get train loader."""
 
         if self.num_workers > 0:
             persistent_workers = True
         else:
             persistent_workers = False
@@ -65,39 +63,40 @@
             pin_memory=True,
             persistent_workers=persistent_workers,
         )
 
         return train_loader
 
     def val_dataloader(self):
-        r"""We use `uss.callbacks.evaluate` to evaluate on the train / test 
+        r"""We use `uss.callbacks.evaluate` to evaluate on the train / test
         dataset"""
         pass
 
 
 def collate_fn(list_data_dict: List[Dict]) -> Dict:
     r"""Collate a mini-batch of data.
 
     Args:
         list_data_dict (List[Dict]): e.g., [
-            {"hdf5_path": "xx/balanced_train.h5", 
+            {"hdf5_path": "xx/balanced_train.h5",
              "index_in_hdf5": 11072,
              ...},
         ...]
 
     Returns:
         data_dict (Dict): e.g., {
             "hdf5_path": ["xx/balanced_train.h5", "xx/balanced_train.h5", ...]
             "index_in_hdf5": [11072, 17251, ...],
         }
     """
-    
+
     data_dict = {}
 
     for key in list_data_dict[0].keys():
-    
-        data_dict[key] = np.array([data_dict[key] for data_dict in list_data_dict])
-    
+
+        data_dict[key] = np.array([data_dict[key]
+                                  for data_dict in list_data_dict])
+
         if str(data_dict[key].dtype) in ["float32"]:
             data_dict[key] = torch.Tensor(data_dict[key])
 
     return data_dict
```

## uss/data/datasets.py

```diff
@@ -1,67 +1,68 @@
+from typing import Dict
+
 import h5py
 import numpy as np
-from typing import Dict
 
 from uss.utils import int16_to_float32
 
 
 class Dataset:
     def __init__(self, steps_per_epoch) -> None:
-        r"""This class takes the meta as input, and return the waveform, target 
-        and other information of the audio clip. This class is used by 
+        r"""This class takes the meta as input, and return the waveform, target
+        and other information of the audio clip. This class is used by
         DataLoader.
 
         Args:
             steps_per_epoch (int): the number of steps in an epoch
         """
         self.steps_per_epoch = steps_per_epoch
-    
+
     def __getitem__(self, meta) -> Dict:
         """Load waveform, target and other information of an audio clip.
-        
+
         Args:
             meta (Dict): {
                 "hdf5_path": str,
                 "index_in_hdf5": int,
                 "class_id": int,
             }
 
-        Returns: 
+        Returns:
             data_dict (Dict): {
                 "hdf5_path": str,
                 "index_in_hdf5": int,
-                "audio_name": str, 
-                "waveform": (clip_samples,), 
+                "audio_name": str,
+                "waveform": (clip_samples,),
                 "target": (classes_num,),
                 "class_id": int,
             }
         """
-        
+
         hdf5_path = meta["hdf5_path"]
         index_in_hdf5 = meta["index_in_hdf5"]
         class_id = meta["class_id"]
 
         with h5py.File(hdf5_path, 'r') as hf:
 
             audio_name = hf["audio_name"][index_in_hdf5].decode()
 
             waveform = int16_to_float32(hf["waveform"][index_in_hdf5])
-            waveform = waveform  
+            waveform = waveform
             # shape: (clip_samples,)
-            
+
             target = hf["target"][index_in_hdf5].astype(np.float32)
             # shape: (classes_num,)
 
         data_dict = {
             "hdf5_path": hdf5_path,
             "index_in_hdf5": index_in_hdf5,
-            "audio_name": audio_name, 
-            "waveform": waveform, 
-            "target": target, 
-            "class_id": class_id, 
+            "audio_name": audio_name,
+            "waveform": waveform,
+            "target": target,
+            "class_id": class_id,
         }
 
         return data_dict
 
     def __len__(self) -> int:
-        return self.steps_per_epoch
+        return self.steps_per_epoch
```

## uss/data/samplers.py

```diff
@@ -1,30 +1,26 @@
 import logging
-import os
-import pathlib
-import pickle
-from typing import Dict, List, Union, Iterable, Iterator
 import time
+from typing import Dict, List
 
-import numpy as np
 import h5py
-from pytorch_lightning.utilities import rank_zero_only
+import numpy as np
 import torch.distributed as dist
 from torch.utils.data.sampler import Sampler
 
 
 class Base:
-    def __init__(self, 
-        indexes_hdf5_path: str, 
-        batch_size: int, 
-        steps_per_epoch: int,
-        random_seed: int, 
-    ):
+    def __init__(self,
+                 indexes_hdf5_path: str,
+                 batch_size: int,
+                 steps_per_epoch: int,
+                 random_seed: int,
+                 ):
         r"""Base class of train samplers.
-        
+
         Args:
             indexes_hdf5_path (str)
             batch_size (int)
             steps_per_epoch (int)
             random_seed (int)
 
         Returns:
@@ -35,71 +31,75 @@
         self.random_state = np.random.RandomState(random_seed)
         self.steps_per_epoch = steps_per_epoch
 
         # Load targets of training data
         load_time = time.time()
 
         with h5py.File(indexes_hdf5_path, 'r') as hf:
-            self.audio_names = [audio_name.decode() for audio_name in hf['audio_name'][:]]
-            self.hdf5_paths = [hdf5_path.decode() for hdf5_path in hf['hdf5_path'][:]]
+            self.audio_names = [audio_name.decode()
+                                for audio_name in hf['audio_name'][:]]
+            self.hdf5_paths = [hdf5_path.decode()
+                               for hdf5_path in hf['hdf5_path'][:]]
             self.indexes_in_hdf5 = hf['index_in_hdf5'][:]
             self.targets = hf['target'][:].astype(np.float32)
             # self.targets: (audios_num, classes_num)
 
         self.audios_num, self.classes_num = self.targets.shape
 
         logging.info('Training number: {}'.format(self.audios_num))
-        logging.info('Load target time: {:.3f} s'.format(time.time() - load_time))
+        logging.info(
+            'Load target time: {:.3f} s'.format(
+                time.time() - load_time))
 
         # Number of training samples of different sound classes
         self.samples_num_per_class = np.sum(self.targets, axis=0)
-        
+
         logging.info('samples_num_per_class: {}'.format(
             self.samples_num_per_class.astype(np.int32)))
 
 
 class BalancedSampler(Base, Sampler):
-    def __init__(self, 
-        indexes_hdf5_path: str, 
-        batch_size: int, 
-        steps_per_epoch: int, 
-        random_seed: int = 1234, 
-    ) -> None:
-        r"""Balanced sampler. Generate mini-batches meta for training. Data are 
+    def __init__(self,
+                 indexes_hdf5_path: str,
+                 batch_size: int,
+                 steps_per_epoch: int,
+                 random_seed: int = 1234,
+                 ) -> None:
+        r"""Balanced sampler. Generate mini-batches meta for training. Data are
         evenly sampled from different sound classes.
-        
+
         Args:
             indexes_hdf5_path (str)
             batch_size (int)
             steps_per_epoch (int)
             random_seed (int)
 
         Returns:
             None
         """
 
         super(BalancedSampler, self).__init__(
-            indexes_hdf5_path=indexes_hdf5_path, 
-            batch_size=batch_size, 
+            indexes_hdf5_path=indexes_hdf5_path,
+            batch_size=batch_size,
             steps_per_epoch=steps_per_epoch,
             random_seed=random_seed,
         )
 
-        # Training indexes of all sound classes. E.g.: 
+        # Training indexes of all sound classes. E.g.:
         # [[0, 11, 12, ...], [3, 4, 15, 16, ...], [7, 8, ...], ...]
         self.indexes_per_class = []
-        
+
         for k in range(self.classes_num):
             self.indexes_per_class.append(
                 np.where(self.targets[:, k] == 1)[0])
-            
+
         # Shuffle indexes
         for k in range(self.classes_num):
             self.random_state.shuffle(self.indexes_per_class[k])
-        
+
         self.queue = []  # Contains sound class IDs
 
         self.pointers_of_classes = [0] * self.classes_num
 
     def expand_queue(self, queue) -> List:
         r"""Append more class IDs to the queue.
 
@@ -113,24 +113,24 @@
         classes_set = np.arange(self.classes_num).tolist()
         self.random_state.shuffle(classes_set)
         queue.extend(classes_set)
         return queue
 
     def __iter__(self) -> List[Dict]:
         r"""Yield mini-batch meta.
-        
+
         Args:
             None
 
         Returns:
             batch_meta: e.g.: [
-                {"audio_name": "YfWBzCRl6LUs.wav", 
-                 "hdf5_path": "xx/balanced_train.h5", 
-                 "index_in_hdf5": 15734, 
-                 "target": [0, 1, 0, 0, ...]}, 
+                {"audio_name": "YfWBzCRl6LUs.wav",
+                 "hdf5_path": "xx/balanced_train.h5",
+                 "index_in_hdf5": 15734,
+                 "target": [0, 1, 0, 0, ...]},
             ...]
         """
 
         batch_size = self.batch_size
 
         while True:
 
@@ -142,110 +142,32 @@
                     self.queue = self.expand_queue(self.queue)
 
                 # Pop a class ID and get the audio index
                 class_id = self.queue.pop(0)
                 pointer = self.pointers_of_classes[class_id]
                 self.pointers_of_classes[class_id] += 1
                 index = self.indexes_per_class[class_id][pointer]
-                
-                # When finish one epoch of a sound class, then shuffle its 
+
+                # When finish one epoch of a sound class, then shuffle its
                 # indexes and reset pointer.
                 if self.pointers_of_classes[class_id] >= self.samples_num_per_class[class_id]:
                     self.pointers_of_classes[class_id] = 0
                     self.random_state.shuffle(self.indexes_per_class[class_id])
 
                 batch_meta.append({
-                    'hdf5_path': self.hdf5_paths[index], 
-                    'index_in_hdf5': self.indexes_in_hdf5[index], 
+                    'hdf5_path': self.hdf5_paths[index],
+                    'index_in_hdf5': self.indexes_in_hdf5[index],
                     'class_id': class_id})
 
             yield batch_meta
 
     def __len__(self) -> int:
         return self.steps_per_epoch
 
 
-'''
-class UnBalancedSampler(Base):
-    def __init__(self, indexes_hdf5_path, batch_size, steps_per_epoch, black_list_csv=None, 
-        random_seed=1234):
-        """Balanced sampler. Generate batch meta for training. Data are equally 
-        sampled from different sound classes.
-        
-        Args:
-          indexes_hdf5_path: string
-          batch_size: int
-          black_list_csv: string
-          random_seed: int
-        """
-        super(UnBalancedSampler, self).__init__(indexes_hdf5_path, 
-            batch_size, black_list_csv, random_seed)
-
-        self.steps_per_epoch = steps_per_epoch
-        self.audios_num = self.targets.shape[0]
-
-        self.indexes = np.arange(self.audios_num)
-            
-        # Shuffle indexes
-        self.random_state.shuffle(self.indexes)
-        
-        self.pointer = 0
-
-    def __iter__(self):
-        """Generate batch meta for training. 
-        
-        Returns:
-          batch_meta: e.g.: [
-            {'audio_name': 'YfWBzCRl6LUs.wav', 
-             'hdf5_path': 'xx/balanced_train.h5', 
-             'index_in_hdf5': 15734, 
-             'target': [0, 1, 0, 0, ...]}, 
-            ...]
-        """
-
-        batch_size = self.batch_size
-
-        while True:
-            batch_meta = []
-            i = 0
-            while i < batch_size:
-                index = self.indexes[self.pointer]
-                self.pointer += 1
-
-                # Shuffle indexes and reset pointer
-                if self.pointer >= self.audios_num:
-                    self.pointer = 0
-                    self.random_state.shuffle(self.indexes)
-                
-                tmp = np.where(self.targets[index]==1)[0]
-
-                if len(tmp) == 0:
-                    continue
-
-                class_id = self.random_state.choice(tmp)
-                # from IPython import embed; embed(using=False); os._exit(0)
-
-                # If audio in black list then continue
-                if self.audio_names[index] in self.black_list_names:
-                    continue
-                else:
-                    batch_meta.append({
-                        'hdf5_path': self.hdf5_paths[index], 
-                        'index_in_hdf5': self.indexes_in_hdf5[index],
-                        'class_id': class_id,
-                    })
-                    i += 1
-
-            yield batch_meta
-
-    def __len__(self):
-        return self.steps_per_epoch
-'''
-
-
 class DistributedSamplerWrapper:
     def __init__(self, sampler: object) -> None:
         r"""Distributed wrapper of sampler.
 
         Args:
             sampler (Sampler object)
 
@@ -270,11 +192,11 @@
             rank = dist.get_rank()
 
         else:
             num_replicas = 1
             rank = 0
 
         for list_meta in self.sampler:
-            yield list_meta[rank :: num_replicas]
+            yield list_meta[rank:: num_replicas]
 
     def __len__(self) -> int:
         return len(self.sampler)
```

## uss/dataset_creation/create_audioset_evaluation_meta.py

```diff
@@ -1,34 +1,32 @@
 import argparse
 import os
-import soundfile
 import pathlib
-import logging
+
 import numpy as np
+import soundfile
 from torch.utils.data import DataLoader
 
-import torch
-from panns_inference.models import Cnn14, Cnn14_DecisionLevelMax
-
-from uss.data.datasets import Dataset
-from uss.data.samplers import BalancedSampler
-from uss.data.datamodules import collate_fn
-from uss.utils import load_pretrained_model
+from uss.config import (CLASSES_NUM, CLIP_SECONDS, FRAMES_PER_SECOND,
+                        SAMPLE_RATE, panns_paths_dict)
 from uss.data.anchor_segment_detectors import AnchorSegmentDetector
 from uss.data.anchor_segment_mixers import AnchorSegmentMixer
-from uss.config import SAMPLE_RATE, FRAMES_PER_SEC, CLASSES_NUM
+from uss.data.datamodules import collate_fn
+from uss.data.datasets import Dataset
+from uss.data.samplers import BalancedSampler
+from uss.utils import get_path, load_pretrained_panns
 
 
 def create_evaluation_meta(args):
-    r"""Create csv containing information of anchor segments for creating 
-    mixtures. For each sound class k, we select M anchor segments that will be 
-    randomly mixed with anchor segments that do not contain sound class k. In 
+    r"""Create csv containing information of anchor segments for creating
+    mixtures. For each sound class k, we select M anchor segments that will be
+    randomly mixed with anchor segments that do not contain sound class k. In
     total, there are classes_num x M mixtures to separate. Anchor segments are
-    short segments (such as 2 s) detected by a pretrained sound event detection 
-    system on 10-second audio clips from AudioSet. All time stamps of anchor 
+    short segments (such as 2 s) detected by a pretrained sound event detection
+    system on 10-second audio clips from AudioSet. All time stamps of anchor
     segments are written into a csv file. E.g.,
 
     .. code-block:: csv
         index_in_hdf5   audio_name  bgn_sample  end_sample  class_id    mix_rank
         4768    YC0j69NCIKfw.wav    140480  204480  347 0
         15640   Yip4ZCCgoVXc.wav    81920   145920  496 1
         10614   YTRxF5y6hFbE.wav    130240  194240  270 0
@@ -50,54 +48,46 @@
     # arguments & parameters
     workspace = args.workspace
     split = args.split
     output_audios_dir = args.output_audios_dir
     output_meta_csv_path = args.output_meta_csv_path
 
     sample_rate = SAMPLE_RATE
-    frames_per_second = FRAMES_PER_SEC
+    frames_per_second = FRAMES_PER_SECOND
+    clip_seconds = CLIP_SECONDS
     classes_num = CLASSES_NUM
 
-    sed_checkpoint_path = "./downloaded_checkpoints/Cnn14_DecisionLevelMax_mAP=0.385.pth"
-    at_checkpoint_path = "./downloaded_checkpoints/Cnn14_mAP=0.431.pth"
-
-    channels = 1
-    segment_seconds = 2.0
-    eval_segments_per_class = 100 
+    eval_segments_per_class = 100
+    segment_seconds = 2.
+    anchor_segment_detect_mode = "max_area"
+    match_energy = True
     mix_num = 2
 
     batch_size = 32
     steps_per_epoch = 10000
     num_workers = 16
-    device = 'cuda'
+    device = "cuda"
+    sed_model_type = "Cnn14_DecisionLevelMax"
 
     if split == 'balanced_train':
-        indexes_hdf5_path = os.path.join(workspace, "hdf5s/indexes/balanced_train.h5")
+        indexes_hdf5_path = os.path.join(
+            workspace, "hdf5s/indexes/balanced_train.h5")
 
     elif split == 'test':
         indexes_hdf5_path = os.path.join(workspace, "hdf5s/indexes/eval.h5")
     # E.g., indexes_hdf5 looks like: {
     #     'audio_name': (audios_num,),
     #     'hdf5_path': (audios_num,),
     #     'index_in_hdf5': (audios_num,),
     #     'target': (audios_num, classes_num)
     # }
 
-    segment_frames = int(segment_seconds * frames_per_second)
-    hop_samples = int(sample_rate / frames_per_second)
-
-    sed_model = load_pretrained_model(
-        model_name="Cnn14_DecisionLevelMax",
-        checkpoint_path=sed_checkpoint_path,
-        freeze=True,
-    ).to(device)
-
-    at_model = load_pretrained_model(
-        model_name="Cnn14",
-        checkpoint_path=at_checkpoint_path,
+    sed_model = load_pretrained_panns(
+        model_type=sed_model_type,
+        checkpoint_path=get_path(panns_paths_dict[sed_model_type]),
         freeze=True,
     ).to(device)
 
     # dataset
     dataset = Dataset(
         steps_per_epoch=steps_per_epoch,
     )
@@ -116,127 +106,140 @@
         num_workers=num_workers,
         pin_memory=True,
         persistent_workers=False,
     )
 
     anchor_segment_detector = AnchorSegmentDetector(
         sed_model=sed_model,
-        clip_seconds=10.,
-        segment_seconds=2.,
-        frames_per_second=100,
+        clip_seconds=clip_seconds,
+        segment_seconds=segment_seconds,
+        frames_per_second=frames_per_second,
         sample_rate=sample_rate,
+        detect_mode=anchor_segment_detect_mode,
     ).to(device)
 
     anchor_segment_mixer = AnchorSegmentMixer(
         mix_num=mix_num,
+        match_energy=match_energy,
     ).to(device)
 
-    eval_meta_dict = {
-        'index_in_hdf5': [],
-        'audio_name': [], 
-        'class_id': [], 
-        'bgn_sample': [], 
-        'end_sample': [],
-        'mix_rank': []
-    }
-    
     count_dict = {class_id: 0 for class_id in range(classes_num)}
 
     meta_dict = {
         'audio_name': [],
     }
 
     for i in range(mix_num):
         meta_dict['source{}_name'.format(i + 1)] = []
         meta_dict['source{}_class_id'.format(i + 1)] = []
         meta_dict['source{}_onset'.format(i + 1)] = []
 
     for class_id in range(classes_num):
-        sub_dir = os.path.join(output_audios_dir, "class_id={}".format(class_id))
+        sub_dir = os.path.join(
+            output_audios_dir,
+            "class_id={}".format(class_id))
         os.makedirs(sub_dir, exist_ok=True)
-        
+
     for batch_index, batch_data_dict in enumerate(dataloader):
 
         batch_data_dict['waveform'] = batch_data_dict['waveform'].to(device)
         # (batch_size, clip_samples)
 
         segments_dict = anchor_segment_detector(
             waveforms=batch_data_dict['waveform'],
             class_ids=batch_data_dict['class_id'],
         )
-        
+
         mixtures, segments = anchor_segment_mixer(
             waveforms=segments_dict['waveform'],
         )
 
         mixtures = mixtures.data.cpu().numpy()
         segments = segments.data.cpu().numpy()
 
         source_names = batch_data_dict['audio_name']
         class_ids = segments_dict['class_id']
         bgn_samples = segments_dict['bgn_sample'].data.cpu().numpy()
-        end_samples = segments_dict['end_sample'].data.cpu().numpy()
-        
+
         for n in range(batch_size):
 
             class_id = class_ids[n]
 
             if count_dict[class_id] < eval_segments_per_class:
 
-                mixture_name = "class_id={},index={:03d},mixture.wav".format(class_id, count_dict[class_id])
-                source_name = "class_id={},index={:03d},source.wav".format(class_id, count_dict[class_id])
-
-                mixture_path = os.path.join(output_audios_dir, "class_id={}".format(class_id), mixture_name)
-                source_path = os.path.join(output_audios_dir, "class_id={}".format(class_id), source_name)
-
-                soundfile.write(file=mixture_path, data=mixtures[n], samplerate=sample_rate)
-                soundfile.write(file=source_path, data=segments[n], samplerate=sample_rate)
+                mixture_name = "class_id={},index={:03d},mixture.wav".format(
+                    class_id, count_dict[class_id])
+                source_name = "class_id={},index={:03d},source.wav".format(
+                    class_id, count_dict[class_id])
+
+                mixture_path = os.path.join(
+                    output_audios_dir,
+                    "class_id={}".format(class_id),
+                    mixture_name)
+                source_path = os.path.join(
+                    output_audios_dir,
+                    "class_id={}".format(class_id),
+                    source_name)
+
+                soundfile.write(
+                    file=mixture_path,
+                    data=mixtures[n],
+                    samplerate=sample_rate)
+                soundfile.write(
+                    file=source_path,
+                    data=segments[n],
+                    samplerate=sample_rate)
 
                 print("Write out to {}".format(mixture_path))
                 print("Write out to {}".format(source_path))
 
                 # Write mixing information into a csv file.
                 meta_dict['audio_name'].append(mixture_name)
-                
+
                 for i in range(mix_num):
-                    meta_dict['source{}_name'.format(i + 1)].append(source_names[(n + i) % batch_size])
-                    meta_dict['source{}_onset'.format(i + 1)].append(bgn_samples[(n + i) % batch_size] / sample_rate)
-                    meta_dict['source{}_class_id'.format(i + 1)].append(class_ids[(n + i) % batch_size])
+                    meta_dict['source{}_name'.format(
+                        i + 1)].append(source_names[(n + i) % batch_size])
+                    meta_dict['source{}_onset'.format(
+                        i + 1)].append(bgn_samples[(n + i) % batch_size] / sample_rate)
+                    meta_dict['source{}_class_id'.format(
+                        i + 1)].append(class_ids[(n + i) % batch_size])
 
                 ###
                 meta_dict['audio_name'].append(source_name)
                 meta_dict['source1_name'].append(source_names[n])
                 meta_dict['source1_onset'].append(bgn_samples[n] / sample_rate)
                 meta_dict['source1_class_id'].append(class_ids[n])
 
                 for i in range(1, mix_num):
                     meta_dict['source{}_name'.format(i + 1)].append("")
                     meta_dict['source{}_onset'.format(i + 1)].append("")
                     meta_dict['source{}_class_id'.format(i + 1)].append("")
 
                 count_dict[class_id] += 1
 
-        finished_n = np.sum([count_dict[class_id] for class_id in range(classes_num)])
-        print('Finished: {} / {}'.format(finished_n, eval_segments_per_class * classes_num))
+        finished_n = np.sum([count_dict[class_id]
+                            for class_id in range(classes_num)])
+        print('Finished: {} / {}'.format(finished_n,
+              eval_segments_per_class * classes_num))
 
         if all_classes_finished(count_dict, eval_segments_per_class):
             break
 
     write_meta_dict_to_csv(meta_dict, output_meta_csv_path)
     print("Write csv to {}".format(output_meta_csv_path))
 
 
 def all_classes_finished(count_dict, segments_per_class):
-    r"""Check if all sound classes have #segments_per_class segments in 
+    r"""Check if all sound classes have #segments_per_class segments in
     count_dict.
 
     Args:
         count_dict: dict, e.g., {
-            0: 12, 
-            1: 4, 
+            0: 12,
+            1: 4,
             ...,
             526: 33,
         }
         segments_per_class: int
 
     Returns:
         bool
@@ -244,22 +247,23 @@
 
     for class_id in count_dict.keys():
         if count_dict[class_id] < segments_per_class:
             return False
 
     return True
 
+
 def write_meta_dict_to_csv(meta_dict, output_meta_csv_path):
     r"""Write meta dict into a csv file.
 
     Args:
         meta_dict: dict, e.g., {
             'index_in_hdf5': (segments_num,),
-            'audio_name': (segments_num,), 
-            'class_id': (segments_num,), 
+            'audio_name': (segments_num,),
+            'class_id': (segments_num,),
         }
         output_csv_path: str
     """
 
     keys = list(meta_dict.keys())
 
     items_num = len(meta_dict[keys[0]])
@@ -280,17 +284,26 @@
 if __name__ == "__main__":
 
     parser = argparse.ArgumentParser(description="")
     subparsers = parser.add_subparsers(dest="mode")
 
     parser_train = subparsers.add_parser("create_evaluation_meta")
     parser_train.add_argument("--workspace", type=str, required=True)
-    parser_train.add_argument("--split", type=str, required=True, choices=['balanced_train', 'test'])
+    parser_train.add_argument(
+        "--split",
+        type=str,
+        required=True,
+        choices=[
+            'balanced_train',
+            'test'])
     parser_train.add_argument("--output_audios_dir", type=str, required=True)
-    parser_train.add_argument("--output_meta_csv_path", type=str, required=True)
+    parser_train.add_argument(
+        "--output_meta_csv_path",
+        type=str,
+        required=True)
 
     args = parser.parse_args()
     args.filename = pathlib.Path(__file__).stem
 
     if args.mode == "create_evaluation_meta":
         create_evaluation_meta(args)
```

## uss/models/base.py

```diff
@@ -1,13 +1,13 @@
-import torch.nn as nn
-import torch
-import numpy as np
-import torch.nn.functional as F
 import math
 
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
 from torchlibrosa.stft import magphase
 
 
 def init_layer(layer):
     """Initialize a Linear or Convolutional layer. """
     nn.init.xavier_uniform_(layer.weight)
 
@@ -21,29 +21,29 @@
     bn.bias.data.fill_(0.0)
     bn.weight.data.fill_(1.0)
 
 
 def init_embedding(layer):
     """Initialize a Linear or Convolutional layer. """
     nn.init.uniform_(layer.weight, -1., 1.)
- 
+
     if hasattr(layer, 'bias'):
         if layer.bias is not None:
             layer.bias.data.fill_(0.)
 
 
 def init_gru(rnn):
     """Initialize a GRU layer. """
 
     def _concat_init(tensor, init_funcs):
         (length, fan_out) = tensor.shape
         fan_in = length // len(init_funcs)
 
         for (i, init_func) in enumerate(init_funcs):
-            init_func(tensor[i * fan_in : (i + 1) * fan_in, :])
+            init_func(tensor[i * fan_in: (i + 1) * fan_in, :])
 
     def _inner_uniform(tensor):
         fan_in = nn.init._calculate_correct_fan(tensor, "fan_in")
         nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))
 
     for i in range(rnn.num_layers):
         _concat_init(
@@ -84,30 +84,30 @@
     def spectrogram_phase(self, input, eps=0.):
         (real, imag) = self.stft(input)
         mag = torch.clamp(real ** 2 + imag ** 2, eps, np.inf) ** 0.5
         cos = real / mag
         sin = imag / mag
         return mag, cos, sin
 
-
     def wav_to_spectrogram_phase(self, input, eps=1e-10):
         """Waveform to spectrogram.
 
         Args:
           input: (batch_size, segment_samples, channels_num)
 
         Outputs:
           output: (batch_size, channels_num, time_steps, freq_bins)
         """
         sp_list = []
         cos_list = []
         sin_list = []
         channels_num = input.shape[1]
         for channel in range(channels_num):
-            mag, cos, sin = self.spectrogram_phase(input[:, channel, :], eps=eps)
+            mag, cos, sin = self.spectrogram_phase(
+                input[:, channel, :], eps=eps)
             sp_list.append(mag)
             cos_list.append(cos)
             sin_list.append(sin)
 
         sps = torch.cat(sp_list, dim=1)
         coss = torch.cat(cos_list, dim=1)
         sins = torch.cat(sin_list, dim=1)
@@ -126,15 +126,14 @@
         channels_num = input.shape[1]
         for channel in range(channels_num):
             sp_list.append(self.spectrogram(input[:, channel, :], eps=eps))
 
         output = torch.cat(sp_list, dim=1)
         return output
 
-
     def spectrogram_to_wav(self, input, spectrogram, length=None):
         """Spectrogram to waveform.
 
         Args:
           input: (batch_size, segment_samples, channels_num)
           spectrogram: (batch_size, channels_num, time_steps, freq_bins)
 
@@ -142,12 +141,19 @@
           output: (batch_size, segment_samples, channels_num)
         """
         channels_num = input.shape[1]
         wav_list = []
         for channel in range(channels_num):
             (real, imag) = self.stft(input[:, channel, :])
             (_, cos, sin) = magphase(real, imag)
-            wav_list.append(self.istft(spectrogram[:, channel : channel + 1, :, :] * cos, 
-                spectrogram[:, channel : channel + 1, :, :] * sin, length))
-        
+            wav_list.append(self.istft(spectrogram[:,
+                                                   channel: channel + 1,
+                                                   :,
+                                                   :] * cos,
+                                       spectrogram[:,
+                                                   channel: channel + 1,
+                                                   :,
+                                                   :] * sin,
+                                       length))
+
         output = torch.stack(wav_list, dim=1)
         return output
```

## uss/models/film.py

```diff
@@ -1,21 +1,18 @@
-from einops import rearrange
-import numpy as np
-from typing import Dict, List, NoReturn, Tuple
+from typing import Dict, List
+
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
-from torchlibrosa.stft import STFT, ISTFT, magphase
 
-from uss.models.base import Base, init_layer, init_bn, act
+from uss.models.base import init_layer
 
 
 def get_film_meta(module: nn.Module) -> Dict:
     r"""Get FiLM meta dict of a module.
-    
+
     Args:
         module (nn.Module), the module to extract meta dict
 
     Returns:
         film_meta (Dict), FiLM meta dict
     """
 
@@ -33,22 +30,22 @@
     # Pre-order traversal of modules
     for child_name, child_module in module.named_children():
 
         child_meta = get_film_meta(child_module)
 
         if len(child_meta) > 0:
             film_meta[child_name] = child_meta
-    
+
     return film_meta
 
 
 class FiLM(nn.Module):
     def __init__(
-        self, 
-        film_meta: Dict, 
+        self,
+        film_meta: Dict,
         condition_size: int,
     ) -> None:
         r"""Create FiLM modules from film meta dict.
 
         Args:
             film_meta (Dict), e.g.,
                 {'encoder_block1': {'conv_block1': {'beta1': 32, 'beta2': 32}},
@@ -60,64 +57,64 @@
         """
 
         super(FiLM, self).__init__()
 
         self.condition_size = condition_size
 
         self.modules, _ = self._create_film_modules(
-            film_meta=film_meta, 
+            film_meta=film_meta,
             prefix_names=[],
         )
-        
+
     def _create_film_modules(
-        self, 
-        film_meta: Dict, 
+        self,
+        film_meta: Dict,
         prefix_names: List[str],
     ):
         r"""Create FiLM modules.
 
         Args:
             film_meta (Dict), e.g.,
                 {"encoder_block1": {"conv_block1": {"beta1": 32, "beta2": 32}},
                  ...}
-            prefix_names (str), only used to get correct module name, e.g., 
+            prefix_names (str), only used to get correct module name, e.g.,
                 ["encoder_block1", "conv_block1"]
         """
 
         modules = {}
-       
+
         # Pre-order traversal of modules
         for module_name, value in film_meta.items():
 
             if isinstance(value, dict):
 
                 prefix_names.append(module_name)
-                
+
                 modules[module_name], _ = self._create_film_modules(
-                    film_meta=value, 
+                    film_meta=value,
                     prefix_names=prefix_names,
                 )
 
             elif isinstance(value, int):
 
                 prefix_names.append(module_name)
                 unique_module_name = '->'.join(prefix_names)
 
                 modules[module_name] = self._add_film_layer_to_module(
-                    num_features=value, 
+                    num_features=value,
                     unique_module_name=unique_module_name,
                 )
 
             prefix_names.pop()
 
         return modules, prefix_names
 
     def _add_film_layer_to_module(
-        self, 
-        num_features: int, 
+        self,
+        num_features: int,
         unique_module_name: str,
     ) -> nn.Module:
         r"""Add a FiLM layer."""
 
         layer = nn.Linear(self.condition_size, num_features)
         init_layer(layer)
         self.add_module(name=unique_module_name, module=layer)
@@ -128,40 +125,41 @@
 
         film_data = {}
 
         # Pre-order traversal of modules
         for module_name, module in modules.items():
 
             if isinstance(module, dict):
-                film_data[module_name] = self._calculate_film_data(conditions, module)
+                film_data[module_name] = self._calculate_film_data(
+                    conditions, module)
 
             elif isinstance(module, nn.Module):
                 film_data[module_name] = module(conditions)[:, :, None, None]
 
         return film_data
 
     def forward(self, conditions: torch.Tensor) -> Dict:
         r"""Forward conditions to all FiLM layers to get FiLM data.
 
         Args:
-            conditions (torch.Tensor): query net outputs, 
+            conditions (torch.Tensor): query net outputs,
                 (batch_size, condition_dim)
 
         Returns:
             film_dict (Dict): e.g., {
                 "encoder_block1": {
                     "conv_block1": {
                         "beta1": (16, 32, 1, 1),
                         "beta2": (16, 32, 1, 1),
                     },
                     ...,
                 },
                 ...,
             }
         """
-        
+
         film_dict = self._calculate_film_data(
-            conditions=conditions, 
+            conditions=conditions,
             modules=self.modules,
         )
-        
-        return film_dict
+
+        return film_dict
```

## uss/models/pl_modules.py

```diff
@@ -1,8 +1,8 @@
-from typing import Any, Callable, Dict
+from typing import Callable, Dict
 
 import lightning.pytorch as pl
 import torch
 import torch.nn as nn
 import torch.optim as optim
 from torch.optim.lr_scheduler import LambdaLR
 
@@ -20,15 +20,15 @@
         lr_lambda_func: Callable,
     ) -> None:
         r"""Pytorch Lightning wrapper of PyTorch model, including forward,
         optimization of model, etc.
 
         Args:
             ss_model (nn.Module): universal source separation module
-            anchor_segment_detector (nn.Module): used to detect anchor segments 
+            anchor_segment_detector (nn.Module): used to detect anchor segments
                 from audio clips
             anchor_segment_mixer (nn.Module): used to mix segments into mixtures
             query_net (nn.Module): used to extract conditions for separation
             loss_function (Callable): loss function to train the separation model
             optimizer_type (str): e.g., "AdamW"
             learning_rate (float)
             lr_lambda_func (Callable), learning rate scaler
@@ -41,16 +41,16 @@
         self.query_net = query_net
         self.loss_function = loss_function
         self.optimizer_type = optimizer_type
         self.learning_rate = learning_rate
         self.lr_lambda_func = lr_lambda_func
 
     def training_step(
-        self, 
-        batch_data_dict: Dict, 
+        self,
+        batch_data_dict: Dict,
         batch_idx: int
     ) -> torch.float:
         r"""Forward a mini-batch data to model, calculate loss function, and
         train for one step. A mini-batch data is evenly distributed on multiple
         devices (if there are) for parallel training.
 
         Args:
@@ -75,28 +75,28 @@
         )
         # segments_dict: {
         #     "waveform": (batch_size, segment_samples),
         #     "class_id": (batch_size,),
         #     "bgn_sample": (batch_size,),
         #     "end_sample": (batch_size,),
         # }
-        
+
         # Mix segments into mixtures and execute energy augmentation
         mixtures, segments = self.anchor_segment_mixer(
             waveforms=segments_dict['waveform'],
         )
         # mixtures: (batch_size, segment_samples)
         # segments: (batch_size, segment_samples)
 
         # Use query net to calculate conditional embedding
         conditions = self.query_net(
             source=segments,
         )['output']
         # conditions: (batch_size, condition_dim)
-        
+
         input_dict = {
             'mixture': mixtures[:, None, :],
             'condition': conditions,
         }
 
         target_dict = {
             'segment': segments[:, None, :],
@@ -140,15 +140,15 @@
                 'scheduler': scheduler,
                 'interval': 'step',
                 'frequency': 1,
             }
         }
 
         return output_dict
-    
+
 
 def get_model_class(model_type: str) -> nn.Module:
     r"""Get separation module by model_type."""
 
     if model_type == 'ResUNet30':
         from uss.models.resunet import ResUNet30
         return ResUNet30
```

## uss/models/query_nets.py

```diff
@@ -1,15 +1,16 @@
 from typing import Dict
+
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
-from uss.utils import load_pretrained_panns, get_path
-from uss.models.base import init_layer
 from uss.config import panns_paths_dict
+from uss.models.base import init_layer
+from uss.utils import get_path, load_pretrained_panns
 
 
 def initialize_query_net(configs):
     r"""Initialize query net.
 
     Args:
         configs (Dict)
@@ -46,15 +47,15 @@
         )
 
     elif model_type == "YourOwn_QueryNet":
         model = YourOwn_QueryNet(outputs_num=outputs_num)
 
     else:
         raise NotImplementedError
-    
+
     return model
 
 
 def get_panns_bottleneck_type(bottleneck_type: str) -> str:
     r"""Get PANNs bottleneck name.
 
     Args:
@@ -70,20 +71,20 @@
     else:
         panns_bottleneck_type = bottleneck_type
 
     return panns_bottleneck_type
 
 
 class Cnn14_Wrapper(nn.Module):
-    def __init__(self, 
-        bottleneck_type: str, 
-        base_checkpoint_path: str, 
-        freeze_base: bool,
-    ) -> None:
-        r"""Query Net based on Cnn14 of PANNs. There are no extra learnable 
+    def __init__(self,
+                 bottleneck_type: str,
+                 base_checkpoint_path: str,
+                 freeze_base: bool,
+                 ) -> None:
+        r"""Query Net based on Cnn14 of PANNs. There are no extra learnable
         parameters.
 
         Args:
             bottleneck_type (str), "at_soft" | "embedding"
             base_checkpoint_path (str), Cnn14 checkpoint path
             freeze_base (bool), whether to freeze the parameters of the Cnn14
         """
@@ -153,34 +154,34 @@
 
         output = self.forward_adaptor(bottleneck=bottleneck)
 
         output_dict = {
             "bottleneck": bottleneck,
             "output": output,
         }
-        
+
         return output_dict
 
 
 class AdaptiveCnn14_Wrapper(nn.Module):
-    def __init__(self, 
-        bottleneck_type: str, 
-        base_checkpoint_path: str, 
-        freeze_base: bool,
-        freeze_adaptor: bool, 
-        outputs_num: int,
-    ) -> None:
-        r"""Query Net based on Cnn14 of PANNs. There are no extra learnable 
+    def __init__(self,
+                 bottleneck_type: str,
+                 base_checkpoint_path: str,
+                 freeze_base: bool,
+                 freeze_adaptor: bool,
+                 outputs_num: int,
+                 ) -> None:
+        r"""Query Net based on Cnn14 of PANNs. There are no extra learnable
         parameters.
 
         Args:
             bottleneck_type (str), "at_soft" | "embedding"
             base_checkpoint_path (str), Cnn14 checkpoint path
             freeze_base (bool), whether to freeze the parameters of the Cnn14
-            freeze_adaptor (bool), whether to freeze the parameters of the 
+            freeze_adaptor (bool), whether to freeze the parameters of the
                 adaptor
             outputs_num (int), output dimension
         """
 
         super(AdaptiveCnn14_Wrapper, self).__init__()
 
         self.freeze_base = freeze_base
@@ -189,30 +190,31 @@
 
         self.base = load_pretrained_panns(
             model_type="Cnn14",
             checkpoint_path=base_checkpoint_path,
             freeze=freeze_base,
         )
 
-        bottleneck_units = self._get_bottleneck_units(self.panns_bottleneck_type)
-        
+        bottleneck_units = self._get_bottleneck_units(
+            self.panns_bottleneck_type)
+
         self.fc1 = nn.Linear(bottleneck_units, 2048)
         self.fc2 = nn.Linear(2048, outputs_num)
 
         if freeze_adaptor:
             for param in self.fc1.parameters():
                 param.requires_grad = False
 
             for param in self.fc2.parameters():
                 param.requires_grad = False
 
         self.init_weights()
 
     def _get_bottleneck_units(self, panns_bottleneck_type) -> int:
-        
+
         if panns_bottleneck_type == "embedding":
             bottleneck_hid_units = self.base.fc_audioset.in_features
 
         elif panns_bottleneck_type == "clipwise_output":
             bottleneck_hid_units = self.base.fc_audioset.out_features
 
         else:
@@ -220,15 +222,15 @@
 
         return bottleneck_hid_units
 
     def init_weights(self):
         r"""Initialize weights."""
         init_layer(self.fc1)
         init_layer(self.fc2)
-    
+
     def forward_base(self, source: torch.Tensor) -> torch.Tensor:
         r"""Forward a source into a the base part of the query net.
 
         Args:
             source (torch.Tensor), (batch_size, audio_samples)
 
         Returns:
@@ -314,9 +316,9 @@
         x = torch.mean(source, dim=-1, keepdim=True)
         bottleneck = self.fc1(x)
 
         output_dict = {
             "bottleneck": bottleneck,
             "output": bottleneck,
         }
-        
-        return output_dict
+
+        return output_dict
```

## uss/models/resunet.py

```diff
@@ -1,19 +1,20 @@
-from einops import rearrange
+from typing import Dict, Tuple
+
 import numpy as np
-from typing import Dict, List, NoReturn, Tuple
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
-from torchlibrosa.stft import STFT, ISTFT, magphase
+from einops import rearrange
+from torchlibrosa.stft import ISTFT, STFT, magphase
+
+from uss.models.base import Base, init_bn, init_layer
+from uss.models.film import FiLM, get_film_meta
 
-from uss.models.base import Base, init_layer, init_bn, act
-from uss.models.film import get_film_meta, FiLM
 
-    
 class ConvBlockRes(nn.Module):
     def __init__(
         self,
         in_channels: int,
         out_channels: int,
         kernel_size: Tuple,
         momentum: float,
@@ -71,32 +72,35 @@
         init_bn(self.bn2)
         init_layer(self.conv1)
         init_layer(self.conv2)
 
         if self.is_shortcut:
             init_layer(self.shortcut)
 
-    def forward(self, 
-        input_tensor: torch.Tensor, 
-        film_dict: Dict
-    ) -> torch.Tensor:
+    def forward(self,
+                input_tensor: torch.Tensor,
+                film_dict: Dict
+                ) -> torch.Tensor:
         r"""Forward input feature maps to the encoder block.
 
         Args:
             input_tensor (torch.Tensor), (B, C, T, F)
             film_dict (Dict)
 
         Returns:
             output (torch.Tensor), (B, C, T, F)
         """
 
         b1 = film_dict['beta1']
         b2 = film_dict['beta2']
 
-        x = self.conv1(F.leaky_relu_(self.bn1(input_tensor) + b1, negative_slope=0.01))
+        x = self.conv1(
+            F.leaky_relu_(
+                self.bn1(input_tensor) + b1,
+                negative_slope=0.01))
         x = self.conv2(F.leaky_relu_(self.bn2(x) + b2, negative_slope=0.01))
 
         if self.is_shortcut:
             output = self.shortcut(input_tensor) + x
         else:
             output = input_tensor + x
 
@@ -118,18 +122,18 @@
         super(EncoderBlockRes1B, self).__init__()
 
         self.conv_block1 = ConvBlockRes(
             in_channels, out_channels, kernel_size, momentum, has_film,
         )
         self.downsample = downsample
 
-    def forward(self, 
-        input_tensor: torch.Tensor, 
-        film_dict: Dict
-    ) -> torch.Tensor:
+    def forward(self,
+                input_tensor: torch.Tensor,
+                film_dict: Dict
+                ) -> torch.Tensor:
         r"""Forward input feature maps to the encoder block.
 
         Args:
             input_tensor (torch.Tensor), (B, C_in, T, F)
             film_dict (Dict)
 
         Returns:
@@ -159,60 +163,60 @@
         super(DecoderBlockRes1B, self).__init__()
 
         self.kernel_size = kernel_size
         self.stride = upsample
 
         self.bn1 = nn.BatchNorm2d(in_channels, momentum=momentum)
         self.bn2 = nn.BatchNorm2d(in_channels, momentum=momentum)
-        # Do not delate the dummy self.bn2. FiLM need self.bn2 to parse the 
+        # Do not delate the dummy self.bn2. FiLM need self.bn2 to parse the
         # FiLM meta correctly.
 
         self.conv1 = torch.nn.ConvTranspose2d(
             in_channels=in_channels,
             out_channels=out_channels,
             kernel_size=self.stride,
             stride=self.stride,
             padding=(0, 0),
             bias=False,
             dilation=(1, 1),
         )
 
         self.conv_block2 = ConvBlockRes(
-            in_channels=out_channels * 2, 
-            out_channels=out_channels, 
-            kernel_size=kernel_size, 
-            momentum=momentum, 
+            in_channels=out_channels * 2,
+            out_channels=out_channels,
+            kernel_size=kernel_size,
+            momentum=momentum,
             has_film=has_film,
         )
-        
+
         self.has_film = has_film
 
         self.init_weights()
 
     def init_weights(self):
         r"""Initialize weights."""
         init_bn(self.bn1)
         init_layer(self.conv1)
 
     def forward(
-        self, 
-        input_tensor: torch.Tensor, 
-        concat_tensor: torch.Tensor, 
+        self,
+        input_tensor: torch.Tensor,
+        concat_tensor: torch.Tensor,
         film_dict: Dict,
     ) -> torch.Tensor:
         r"""Forward input feature maps to the decoder block.
 
         Args:
             input_tensor (torch.Tensor), (B, C_in, T, F)
             film_dict (Dict)
 
         Returns:
             output (torch.Tensor): (B, C_out, T * upsample, F * upsample)
         """
-        
+
         b1 = film_dict['beta1']
 
         x = self.conv1(F.leaky_relu_(self.bn1(input_tensor) + b1))
         # (B, C_out, T * upsample, F * upsample)
 
         x = torch.cat((x, concat_tensor), dim=1)
         # (B, C_out * 2, T * upsample, F * upsample)
@@ -220,18 +224,18 @@
         output = self.conv_block2(x, film_dict['conv_block2'])
         # output: (B, C_out, T * upsample, F * upsample)
 
         return output
 
 
 class ResUNet30_Base(nn.Module, Base):
-    def __init__(self, 
-        input_channels: int, 
-        output_channels: int,
-    ) -> None:
+    def __init__(self,
+                 input_channels: int,
+                 output_channels: int,
+                 ) -> None:
         r"""Base separation model.
 
         Args:
             input_channels (int), audio channels, e.g., 1 | 2
             output_channels (int), audio channels, e.g., 1 | 2
         """
 
@@ -243,16 +247,17 @@
         pad_mode = "reflect"
         window = "hann"
         momentum = 0.01
 
         self.output_channels = output_channels
 
         self.K = 3  # mag, cos, sin
-        
-        self.time_downsample_ratio = 2 ** 5  # This number equals 2^{#encoder_blcoks}
+
+        # This number equals 2^{#encoder_blcoks}
+        self.time_downsample_ratio = 2 ** 5
 
         self.stft = STFT(
             n_fft=window_size,
             hop_length=hop_size,
             win_length=window_size,
             window=window,
             center=center,
@@ -269,19 +274,19 @@
             pad_mode=pad_mode,
             freeze_parameters=True,
         )
 
         self.bn0 = nn.BatchNorm2d(window_size // 2 + 1, momentum=momentum)
 
         self.pre_conv = nn.Conv2d(
-            in_channels=input_channels, 
-            out_channels=32, 
-            kernel_size=(1, 1), 
-            stride=(1, 1), 
-            padding=(0, 0), 
+            in_channels=input_channels,
+            out_channels=32,
+            kernel_size=(1, 1),
+            stride=(1, 1),
+            padding=(0, 0),
             bias=True,
         )
 
         self.encoder_block1 = EncoderBlockRes1B(
             in_channels=32,
             out_channels=32,
             kernel_size=(3, 3),
@@ -421,20 +426,23 @@
 
             (There is input_channels == output_channels for the source separation task.)
 
         Outputs:
             waveform: (B, output_channels, audio_samples)
         """
 
-        x = rearrange(input_tensor, 'b (c k) t f -> b c k t f', c=self.output_channels)
+        x = rearrange(
+            input_tensor,
+            'b (c k) t f -> b c k t f',
+            c=self.output_channels)
 
         mask_mag = torch.sigmoid(x[:, :, 0, :, :])
         mask_real = torch.tanh(x[:, :, 1, :, :])
         mask_imag = torch.tanh(x[:, :, 2, :, :])
-        
+
         _, mask_cos, mask_sin = magphase(mask_real, mask_imag)
         # mask_cos, mask_sin: (B, output_channels, T, F)
 
         # Y = |Y|cosY + j|Y|sinY
         #   = |Y|cos(X + M) + j|Y|sin(X + M)
         #   = |Y|(cosX cosM - sinX sinM) + j|Y|(sinX cosM + cosX sinM)
         out_cos = (
@@ -464,15 +472,14 @@
         # (B * output_channels, audio_samples)
 
         # Reshape to (B, output_channels, audio_samples)
         waveform = rearrange(x, '(b c) t -> b c t', c=self.output_channels)
 
         return waveform
 
-
     def forward(self, mixtures, film_dict):
         r"""Forward mixtures and conditions to separate target sources.
 
         Args:
             input (torch.Tensor): (batch_size, output_channels, segment_samples)
 
         Outputs:
@@ -484,43 +491,56 @@
         mag, cos_in, sin_in = self.wav_to_spectrogram_phase(mixtures)
         x = mag
 
         # Batch normalization
         x = x.transpose(1, 3)
         x = self.bn0(x)
         x = x.transpose(1, 3)   # shape: (B, input_channels, T, F)
-        
+
         # Pad spectrogram to be evenly divided by downsample ratio
         origin_len = x.shape[2]
-        pad_len = (
-            int(np.ceil(x.shape[2] / self.time_downsample_ratio)) * self.time_downsample_ratio
-            - origin_len
-        )
+        pad_len = (int(np.ceil(x.shape[2] /
+                               self.time_downsample_ratio)) *
+                   self.time_downsample_ratio -
+                   origin_len)
         x = F.pad(x, pad=(0, 0, 0, pad_len))
         # x: (B, input_channels, T, F)
 
         # Let frequency bins be evenly divided by 2, e.g., 513 -> 512
-        x = x[..., 0 : x.shape[-1] - 1]  # (B, input_channels, T, F)
+        x = x[..., 0: x.shape[-1] - 1]  # (B, input_channels, T, F)
 
         # UNet
         x = self.pre_conv(x)
 
-        x1_pool, x1 = self.encoder_block1(x, film_dict['encoder_block1'])  # x1_pool: (B, 32, T / 2, F / 2)
-        x2_pool, x2 = self.encoder_block2(x1_pool, film_dict['encoder_block2'])  # x2_pool: (B, 64, T / 4, F / 4)
-        x3_pool, x3 = self.encoder_block3(x2_pool, film_dict['encoder_block3'])  # x3_pool: (B, 128, T / 8, F / 8)
-        x4_pool, x4 = self.encoder_block4(x3_pool, film_dict['encoder_block4'])  # x4_pool: (B, 256, T / 16, F / 16)
-        x5_pool, x5 = self.encoder_block5(x4_pool, film_dict['encoder_block5'])  # x5_pool: (B, 384, T / 32, F / 32)
-        x6_pool, x6 = self.encoder_block6(x5_pool, film_dict['encoder_block6'])  # x6_pool: (B, 384, T / 32, F / 64)
-        x_center, _ = self.conv_block7a(x6_pool, film_dict['conv_block7a'])  # (B, 384, T / 32, F / 64)
-        x7 = self.decoder_block1(x_center, x6, film_dict['decoder_block1'])  # (B, 384, T / 32, F / 32)
-        x8 = self.decoder_block2(x7, x5, film_dict['decoder_block2'])  # (B, 384, T / 16, F / 16)
-        x9 = self.decoder_block3(x8, x4, film_dict['decoder_block3'])  # (B, 256, T / 8, F / 8)
-        x10 = self.decoder_block4(x9, x3, film_dict['decoder_block4'])  # (B, 128, T / 4, F / 4)
-        x11 = self.decoder_block5(x10, x2, film_dict['decoder_block5'])  # (B, 64, T / 2, F / 2)
-        x12 = self.decoder_block6(x11, x1, film_dict['decoder_block6'])  # (B, 32, T, F)
+        x1_pool, x1 = self.encoder_block1(
+            x, film_dict['encoder_block1'])  # x1_pool: (B, 32, T / 2, F / 2)
+        x2_pool, x2 = self.encoder_block2(
+            x1_pool, film_dict['encoder_block2'])  # x2_pool: (B, 64, T / 4, F / 4)
+        x3_pool, x3 = self.encoder_block3(
+            x2_pool, film_dict['encoder_block3'])  # x3_pool: (B, 128, T / 8, F / 8)
+        # x4_pool: (B, 256, T / 16, F / 16)
+        x4_pool, x4 = self.encoder_block4(x3_pool, film_dict['encoder_block4'])
+        # x5_pool: (B, 384, T / 32, F / 32)
+        x5_pool, x5 = self.encoder_block5(x4_pool, film_dict['encoder_block5'])
+        # x6_pool: (B, 384, T / 32, F / 64)
+        x6_pool, x6 = self.encoder_block6(x5_pool, film_dict['encoder_block6'])
+        x_center, _ = self.conv_block7a(
+            x6_pool, film_dict['conv_block7a'])  # (B, 384, T / 32, F / 64)
+        # (B, 384, T / 32, F / 32)
+        x7 = self.decoder_block1(x_center, x6, film_dict['decoder_block1'])
+        # (B, 384, T / 16, F / 16)
+        x8 = self.decoder_block2(x7, x5, film_dict['decoder_block2'])
+        x9 = self.decoder_block3(
+            x8, x4, film_dict['decoder_block3'])  # (B, 256, T / 8, F / 8)
+        x10 = self.decoder_block4(
+            x9, x3, film_dict['decoder_block4'])  # (B, 128, T / 4, F / 4)
+        x11 = self.decoder_block5(
+            x10, x2, film_dict['decoder_block5'])  # (B, 64, T / 2, F / 2)
+        x12 = self.decoder_block6(
+            x11, x1, film_dict['decoder_block6'])  # (B, 32, T, F)
 
         x = self.after_conv(x12)
 
         # Recover shape
         x = F.pad(x, pad=(0, 1))
         x = x[:, :, 0:origin_len, :]
 
@@ -538,40 +558,40 @@
 
         output_dict = {'waveform': separated_audio}
 
         return output_dict
 
 
 class ResUNet30(nn.Module):
-    def __init__(self, 
-        input_channels: int, 
-        output_channels: int, 
-        condition_size: int,
-    ) -> None:
+    def __init__(self,
+                 input_channels: int,
+                 output_channels: int,
+                 condition_size: int,
+                 ) -> None:
         r"""Universal separation model.
 
         Args:
             input_channels (int), audio channels, e.g., 1 | 2
             output_channels (int), audio channels, e.g., 1 | 2
             condition_size (int), FiLM condition size, e.g., 527 | 2048
         """
 
         super(ResUNet30, self).__init__()
 
         self.base = ResUNet30_Base(
-            input_channels=input_channels, 
+            input_channels=input_channels,
             output_channels=output_channels,
         )
-        
+
         self.film_meta = get_film_meta(
             module=self.base,
         )
-        
+
         self.film = FiLM(
-            film_meta=self.film_meta, 
+            film_meta=self.film_meta,
             condition_size=condition_size
         )
 
     def forward(self, input_dict: Dict) -> Dict:
         r"""Forward mixtures and conditions to separate target sources.
 
         Args:
@@ -581,21 +601,21 @@
             }
 
         Returns:
             output_dict (Dict): {
                 "waveform": (batch_size, audio_channels, audio_samples)
             }
         """
-        
+
         mixtures = input_dict['mixture']
         conditions = input_dict['condition']
 
         film_dict = self.film(
             conditions=conditions,
         )
 
         output_dict = self.base(
-            mixtures=mixtures, 
+            mixtures=mixtures,
             film_dict=film_dict,
         )
 
         return output_dict
```

## uss/optimizers/lr_schedulers.py

```diff
@@ -1,14 +1,14 @@
 from functools import partial
 from typing import Callable
 
 
 def linear_warm_up(
-    step: int, 
-    warm_up_steps: int, 
+    step: int,
+    warm_up_steps: int,
     reduce_lr_steps: int
 ) -> float:
     r"""Get linear warm up scheduler for LambdaLR.
 
     Args:
         step (int): global step
         warm_up_steps (int): steps for warm up
@@ -27,18 +27,17 @@
         lr_scale = step / warm_up_steps
     else:
         lr_scale = 0.9 ** (step // reduce_lr_steps)
 
     return lr_scale
 
 
-
 def constant_warm_up(
-    step: int, 
-    warm_up_steps: int, 
+    step: int,
+    warm_up_steps: int,
     reduce_lr_steps: int
 ) -> float:
     r"""Get constant warm up scheduler for LambdaLR.
 
     Args:
         step (int): global step
         warm_up_steps (int): steps for warm up
@@ -48,15 +47,15 @@
         >>> lr_lambda = partial(constant_warm_up, warm_up_steps=1000, reduce_lr_steps=10000)
         >>> from torch.optim.lr_scheduler import LambdaLR
         >>> LambdaLR(optimizer, lr_lambda)
 
     Returns:
         lr_scale (float): learning rate scaler
     """
-    
+
     if 0 <= step < warm_up_steps:
         lr_scale = 0.001
 
     elif warm_up_steps <= step < 2 * warm_up_steps:
         lr_scale = 0.01
 
     elif 2 * warm_up_steps <= step < 3 * warm_up_steps:
@@ -65,38 +64,38 @@
     else:
         lr_scale = 1
 
     return lr_scale
 
 
 def get_lr_lambda(
-    lr_lambda_type: str, 
+    lr_lambda_type: str,
     **kwargs
 ) -> Callable:
     r"""Get learning scheduler.
 
     Args:
         lr_lambda_type (str), e.g., "constant_warm_up" | "linear_warm_up"
 
     Returns:
         lr_lambda_func (Callable)
     """
     if lr_lambda_type == "constant_warm_up":
 
         lr_lambda_func = partial(
-            constant_warm_up, 
-            warm_up_steps=kwargs["warm_up_steps"], 
+            constant_warm_up,
+            warm_up_steps=kwargs["warm_up_steps"],
             reduce_lr_steps=kwargs["reduce_lr_steps"],
         )
 
     elif lr_lambda_type == "linear_warm_up":
 
         lr_lambda_func = partial(
-            linear_warm_up, 
-            warm_up_steps=kwargs["warm_up_steps"], 
+            linear_warm_up,
+            warm_up_steps=kwargs["warm_up_steps"],
             reduce_lr_steps=kwargs["reduce_lr_steps"],
         )
 
     else:
         raise NotImplementedError
 
     return lr_lambda_func
```

## Comparing `uss-0.0.1.dist-info/LICENSE` & `uss-0.0.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `uss-0.0.1.dist-info/METADATA` & `uss-0.0.2.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: uss
-Version: 0.0.1
+Version: 0.0.2
 Summary: Universal source separation (USS) with weakly labelled data.
 Home-page: https://github.com/bytedance/uss
 Author: Qiuqiang Kong
 Author-email: qiuqiangkong@gmail.com
 License: Apache2.0
 Requires-Python: >=3.8
 License-File: LICENSE
```

## Comparing `uss-0.0.1.dist-info/RECORD` & `uss-0.0.2.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,42 +1,42 @@
 uss/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 uss/casa.py,sha256=KlJexLSPChyKwBy0_nz_8JZNqkdDgf8DNnOEMhTpz1A,2114
-uss/config.py,sha256=mPJyfQhgq79w_a3ErDkmGbwWQo6XyMTLOaaliWn_umY,1932
-uss/evaluate.py,sha256=nRu8UbELFirm2PiBw5t649UoAI4dolLtjCGzURzat24,6021
-uss/evaluate_musdb18.py,sha256=_ARbasRNS9otM4NX9okUnbWS8DanJyShjm6rN2aGBUc,6652
-uss/inference.py,sha256=0EHYAOgLbm2Z-eOzLXcjYoPC2Kv-v3BXp3T001KKefg,23657
+uss/config.py,sha256=u4rVIj7achn3gnW5BtzJ0BAJ956GwjqbXMHbFyRVy6Q,1922
+uss/evaluate.py,sha256=jN8ZZVBX4RdyKCae7N30vqUb1VBC5D6itrM4QdWqQJw,6022
+uss/evaluate_musdb18.py,sha256=yVAgOSPbub4Rw7u5ewKbcCOEdYcR0e09MBC0QUIVlCo,6570
+uss/inference.py,sha256=3tb7YtvO7T3Pjy5sotRUNKAMnGPm_2j9pwD0Fcu4S_o,23903
 uss/losses.py,sha256=gkNIddm9g7fWwQBKSOzGRI-mED6yOfNx6OOzN8mUz4g,2295
-uss/parse_ontology.py,sha256=0_EpMwLkYRZwxiEZxd6rFS-wylyIRkhldfK21zJVwSk,2941
+uss/parse_ontology.py,sha256=LZurDVFUcsLiiFvDXEPBHBzzCak9w8Y827MOZCJKXVc,2934
 uss/test9.py,sha256=Cv1MBLoclXSE2WfvUF-cgmqgexjrvKoGH-vhA-Cg01U,9028
-uss/train.py,sha256=PLl1SdlzNhaC75R4gUpxSHrbqVw9aOxQ1LX9PS2UPuw,10772
+uss/train.py,sha256=6WXlmLpP54A0vPIsP0_yixt-4MrEr4oAIR4Aml2_-9U,10445
 uss/uss.py,sha256=KlJexLSPChyKwBy0_nz_8JZNqkdDgf8DNnOEMhTpz1A,2114
-uss/uss_inference.py,sha256=KlJexLSPChyKwBy0_nz_8JZNqkdDgf8DNnOEMhTpz1A,2114
-uss/utils.py,sha256=mR4taR5JAk5seEAa-iToUYzGs1_bg1j-pKzYXbOzDoU,6144
+uss/uss_inference.py,sha256=e4JTZBhafIakqg8iRfn4OpNWTQDp9YmLqPcdzdAkZ60,1977
+uss/utils.py,sha256=NYzCRzqTldOz0gvKtd2jMWD1E2T-uTMO6JjFUhM10DQ,6234
 uss/callbacks/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 uss/callbacks/base.py,sha256=rkb_dHZaPZnohsd4G34qP-9VnBNrwuKw7Dc6GjWMiwA,1062
-uss/callbacks/evaluate.py,sha256=q_ETWimHVhqLqryxMqVrnQcIrvvIVCO1L1YUPj2bfBE,3389
+uss/callbacks/evaluate.py,sha256=3k6RMWSjaQsO36cH_AgTW1P5V4NUKE1KRHQIrkC1x6E,3484
 uss/data/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-uss/data/anchor_segment_detectors.py,sha256=o1lNgQa45YGlZ6MbFPyOG2R0XjcgHowJMQGulUp1S1U,7873
-uss/data/anchor_segment_mixers.py,sha256=I3a1SbaEWNSzA4FNJ3wScpHZcbzOPl6qV4-CDp4y8Yc,2699
-uss/data/datamodules.py,sha256=hgaGrfKS3TsZV2FNkohysdqVHUxOXKCgwwZeHLVjn60,2875
-uss/data/datasets.py,sha256=ywrvHMUqBNwT0y_LDxjnynDy8F1EtDPAyCvGymB9xjY,1917
-uss/data/samplers.py,sha256=8xsbgzO6F4RAyyx0tJY1Ot3j1kkmZ2xJo3bsk8X2RxM,8330
+uss/data/anchor_segment_detectors.py,sha256=-oFkNH_fnMMFHDrqS7QvQN1w2Q3PUQcSOE2leaJX4zQ,7896
+uss/data/anchor_segment_mixers.py,sha256=lHy1t3-L6ha_I2329HT-8F4f8Ck7wKt4gpw0ruw5GTQ,2663
+uss/data/datamodules.py,sha256=kMqM1sFR3wmP4QvqcZOJ_PenXs_HzC-I9_MlQZMDR08,2855
+uss/data/datasets.py,sha256=LAtqp4dATGc1jAadoKZ2K8n6aT_JJHe2uMikws4B9LI,1876
+uss/data/samplers.py,sha256=j2MtE2SQkcKu7mGHZ8wT8-JnA3umMcQvGkHKQmNxpxU,5898
 uss/dataset_creation/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 uss/dataset_creation/_create_audioset_indexes.py,sha256=XBgR17KgYxeWeABfGOtM9rE18NOFcHTk6kJB5Z1C6qY,4686
-uss/dataset_creation/create_audioset_evaluation_meta.py,sha256=p4HpOSdkYCd-76FY9YYCP-XjlZs-GwK_Z2akk0Yf45Y,9928
+uss/dataset_creation/create_audioset_evaluation_meta.py,sha256=mMLFTbtR5J8yPn2R-cxyqUJpAM7dr0N98e9Ly0mNVuI,9953
 uss/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-uss/models/base.py,sha256=Mtem6HTWiLkxFvnCqzvKoorkcl0_zMfaE_LEa6FDWfo,4485
-uss/models/film.py,sha256=iGYp5Rz6wNqhpkVH_KJY7O6hjBtTQBo1X7UhAiISFnU,4476
-uss/models/pl_modules.py,sha256=M8Kuw3aw7COhG7mRIKyfFomJieYTGnnlqWcvP8zjDDs,4965
-uss/models/query_nets.py,sha256=dRH8OEacNgbtAXF4M7Q9C4thLuth7zLyZDR5MrhC8d4,8976
-uss/models/resunet.py,sha256=enXW2yupJEMutHVYq47lPk1O3iGd5fZk4RAPK6LVAWU,17962
+uss/models/base.py,sha256=ZhwpPebSaruibkwKtrnMqdbVt8se3djjZgEeC43l8vA,4855
+uss/models/film.py,sha256=zBYVym3eO9dxwN4u0vr_vTDzK2G9TLz5PDannlbJ4S4,4263
+uss/models/pl_modules.py,sha256=6bgLwfqlExf0s5RZNCUGErETjH4Cnu4zYixBI116JYM,4937
+uss/models/query_nets.py,sha256=w_1hp5-hpWYBPbJ-5TSj4u6BgVh3ugJRwBvH2_WvHdI,9039
+uss/models/resunet.py,sha256=vfxdEHJvMMFixnfxlAlQevflWsrg2IjQ1PxpyJOLfVs,18238
 uss/optimizers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-uss/optimizers/lr_schedulers.py,sha256=VjykVBwJxt3x6abWEk7bYM9TOQIbNkv1_qd9SXdBnmI,2512
+uss/optimizers/lr_schedulers.py,sha256=snXRI6znqpfqjF8VAnOauvmupiT7ipFHfwUuuBvouE8,2498
 uss/plots/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 uss/plots/plot.py,sha256=ngjl3tIdBAnja_vi0SBaU8DGE1tu2AUBZCO3es9BOpM,1703
 uss/plots/plot_paper_stats.py,sha256=4U7ERQpCEjpH4uazr8uRrNq9vyTgGW4JZjAkxAN5RZM,597
-uss-0.0.1.dist-info/LICENSE,sha256=kQerw4uTF-mBnGDidjv9wkC98aqFZLZS3zGP2i65B24,550
-uss-0.0.1.dist-info/METADATA,sha256=KE9grK50MCtK72RbSbB3noANCuMzUq-1MO-dXuNgy6c,551
-uss-0.0.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-uss-0.0.1.dist-info/entry_points.txt,sha256=1BqS8TZGkntJezk3ZgP1BvHEK_6M-GsO4bMJcREvjQo,47
-uss-0.0.1.dist-info/top_level.txt,sha256=MI_r_zJ6Pj7xHOc4ZJDXYhqj95zy2egBnKDK0pGmQhY,4
-uss-0.0.1.dist-info/RECORD,,
+uss-0.0.2.dist-info/LICENSE,sha256=kQerw4uTF-mBnGDidjv9wkC98aqFZLZS3zGP2i65B24,550
+uss-0.0.2.dist-info/METADATA,sha256=L8ZnED5cETj0jZITbDDve5D74jVGmWyS1Egu1Q8MUko,551
+uss-0.0.2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+uss-0.0.2.dist-info/entry_points.txt,sha256=1BqS8TZGkntJezk3ZgP1BvHEK_6M-GsO4bMJcREvjQo,47
+uss-0.0.2.dist-info/top_level.txt,sha256=MI_r_zJ6Pj7xHOc4ZJDXYhqj95zy2egBnKDK0pGmQhY,4
+uss-0.0.2.dist-info/RECORD,,
```

