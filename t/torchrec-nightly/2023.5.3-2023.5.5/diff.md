# Comparing `tmp/torchrec_nightly-2023.5.3-py39-none-any.whl.zip` & `tmp/torchrec_nightly-2023.5.5-py39-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,141 +1,141 @@
-Zip file size: 339500 bytes, number of entries: 139
--rw-r--r--  2.0 unx      811 b- defN 23-May-03 11:23 torchrec/__init__.py
--rw-r--r--  2.0 unx     1638 b- defN 23-May-03 11:23 torchrec/streamable.py
--rw-r--r--  2.0 unx      854 b- defN 23-May-03 11:23 torchrec/types.py
--rw-r--r--  2.0 unx     1153 b- defN 23-May-03 11:23 torchrec/datasets/__init__.py
--rw-r--r--  2.0 unx    41469 b- defN 23-May-03 11:23 torchrec/datasets/criteo.py
--rw-r--r--  2.0 unx     4548 b- defN 23-May-03 11:23 torchrec/datasets/movielens.py
--rw-r--r--  2.0 unx     6539 b- defN 23-May-03 11:23 torchrec/datasets/random.py
--rw-r--r--  2.0 unx    10909 b- defN 23-May-03 11:23 torchrec/datasets/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/datasets/scripts/__init__.py
--rw-r--r--  2.0 unx     2448 b- defN 23-May-03 11:23 torchrec/datasets/scripts/contiguous_preproc_criteo.py
--rw-r--r--  2.0 unx     2847 b- defN 23-May-03 11:23 torchrec/datasets/scripts/npy_preproc_criteo.py
--rw-r--r--  2.0 unx     3077 b- defN 23-May-03 11:23 torchrec/datasets/scripts/shuffle_preproc_criteo.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/datasets/test_utils/__init__.py
--rw-r--r--  2.0 unx     5308 b- defN 23-May-03 11:23 torchrec/datasets/test_utils/criteo_test_utils.py
--rw-r--r--  2.0 unx     1912 b- defN 23-May-03 11:23 torchrec/distributed/__init__.py
--rw-r--r--  2.0 unx    37254 b- defN 23-May-03 11:23 torchrec/distributed/batched_embedding_kernel.py
--rw-r--r--  2.0 unx     2069 b- defN 23-May-03 11:23 torchrec/distributed/collective_utils.py
--rw-r--r--  2.0 unx     4925 b- defN 23-May-03 11:23 torchrec/distributed/comm.py
--rw-r--r--  2.0 unx    55820 b- defN 23-May-03 11:23 torchrec/distributed/comm_ops.py
--rw-r--r--  2.0 unx    35443 b- defN 23-May-03 11:23 torchrec/distributed/dist_data.py
--rw-r--r--  2.0 unx    29817 b- defN 23-May-03 11:23 torchrec/distributed/embedding.py
--rw-r--r--  2.0 unx     4443 b- defN 23-May-03 11:23 torchrec/distributed/embedding_kernel.py
--rw-r--r--  2.0 unx    27279 b- defN 23-May-03 11:23 torchrec/distributed/embedding_lookup.py
--rw-r--r--  2.0 unx    14990 b- defN 23-May-03 11:23 torchrec/distributed/embedding_sharding.py
--rw-r--r--  2.0 unx    37089 b- defN 23-May-03 11:23 torchrec/distributed/embedding_tower_sharding.py
--rw-r--r--  2.0 unx    15021 b- defN 23-May-03 11:23 torchrec/distributed/embedding_types.py
--rw-r--r--  2.0 unx    34625 b- defN 23-May-03 11:23 torchrec/distributed/embeddingbag.py
--rw-r--r--  2.0 unx     7373 b- defN 23-May-03 11:23 torchrec/distributed/fbgemm_qcomm_codec.py
--rw-r--r--  2.0 unx     5273 b- defN 23-May-03 11:23 torchrec/distributed/fused_embedding.py
--rw-r--r--  2.0 unx     5110 b- defN 23-May-03 11:23 torchrec/distributed/fused_embeddingbag.py
--rw-r--r--  2.0 unx     1699 b- defN 23-May-03 11:23 torchrec/distributed/fused_params.py
--rw-r--r--  2.0 unx     3807 b- defN 23-May-03 11:23 torchrec/distributed/grouped_position_weighted.py
--rw-r--r--  2.0 unx    19528 b- defN 23-May-03 11:23 torchrec/distributed/model_parallel.py
--rw-r--r--  2.0 unx    13636 b- defN 23-May-03 11:23 torchrec/distributed/quant_embedding.py
--rw-r--r--  2.0 unx    14006 b- defN 23-May-03 11:23 torchrec/distributed/quant_embedding_kernel.py
--rw-r--r--  2.0 unx    10833 b- defN 23-May-03 11:23 torchrec/distributed/quant_embeddingbag.py
--rw-r--r--  2.0 unx     9261 b- defN 23-May-03 11:23 torchrec/distributed/shard.py
--rw-r--r--  2.0 unx    19218 b- defN 23-May-03 11:23 torchrec/distributed/sharding_plan.py
--rw-r--r--  2.0 unx    22330 b- defN 23-May-03 11:23 torchrec/distributed/train_pipeline.py
--rw-r--r--  2.0 unx    24927 b- defN 23-May-03 11:23 torchrec/distributed/types.py
--rw-r--r--  2.0 unx    11373 b- defN 23-May-03 11:23 torchrec/distributed/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/distributed/composable/__init__.py
--rw-r--r--  2.0 unx     3207 b- defN 23-May-03 11:23 torchrec/distributed/composable/table_batched_embedding_slice.py
--rw-r--r--  2.0 unx     1025 b- defN 23-May-03 11:23 torchrec/distributed/planner/__init__.py
--rw-r--r--  2.0 unx     3135 b- defN 23-May-03 11:23 torchrec/distributed/planner/constants.py
--rw-r--r--  2.0 unx    10318 b- defN 23-May-03 11:23 torchrec/distributed/planner/enumerators.py
--rw-r--r--  2.0 unx    12485 b- defN 23-May-03 11:23 torchrec/distributed/planner/partitioners.py
--rw-r--r--  2.0 unx      824 b- defN 23-May-03 11:23 torchrec/distributed/planner/perf_models.py
--rw-r--r--  2.0 unx    12224 b- defN 23-May-03 11:23 torchrec/distributed/planner/planners.py
--rw-r--r--  2.0 unx    11094 b- defN 23-May-03 11:23 torchrec/distributed/planner/proposers.py
--rw-r--r--  2.0 unx    40173 b- defN 23-May-03 11:23 torchrec/distributed/planner/shard_estimators.py
--rw-r--r--  2.0 unx    21410 b- defN 23-May-03 11:23 torchrec/distributed/planner/stats.py
--rw-r--r--  2.0 unx     9125 b- defN 23-May-03 11:23 torchrec/distributed/planner/storage_reservations.py
--rw-r--r--  2.0 unx    12879 b- defN 23-May-03 11:23 torchrec/distributed/planner/types.py
--rw-r--r--  2.0 unx     1119 b- defN 23-May-03 11:23 torchrec/distributed/planner/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/distributed/sharding/__init__.py
--rw-r--r--  2.0 unx     2539 b- defN 23-May-03 11:23 torchrec/distributed/sharding/cw_sequence_sharding.py
--rw-r--r--  2.0 unx     9519 b- defN 23-May-03 11:23 torchrec/distributed/sharding/cw_sharding.py
--rw-r--r--  2.0 unx     2802 b- defN 23-May-03 11:23 torchrec/distributed/sharding/dp_sequence_sharding.py
--rw-r--r--  2.0 unx     7452 b- defN 23-May-03 11:23 torchrec/distributed/sharding/dp_sharding.py
--rw-r--r--  2.0 unx     5041 b- defN 23-May-03 11:23 torchrec/distributed/sharding/rw_sequence_sharding.py
--rw-r--r--  2.0 unx    12850 b- defN 23-May-03 11:23 torchrec/distributed/sharding/rw_sharding.py
--rw-r--r--  2.0 unx     3114 b- defN 23-May-03 11:23 torchrec/distributed/sharding/sequence_sharding.py
--rw-r--r--  2.0 unx     7609 b- defN 23-May-03 11:23 torchrec/distributed/sharding/tw_sequence_sharding.py
--rw-r--r--  2.0 unx    16061 b- defN 23-May-03 11:23 torchrec/distributed/sharding/tw_sharding.py
--rw-r--r--  2.0 unx     1284 b- defN 23-May-03 11:23 torchrec/distributed/sharding/twcw_sharding.py
--rw-r--r--  2.0 unx    19840 b- defN 23-May-03 11:23 torchrec/distributed/sharding/twrw_sharding.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/__init__.py
--rw-r--r--  2.0 unx    10125 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/infer_utils.py
--rw-r--r--  2.0 unx     4868 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/multi_process.py
--rw-r--r--  2.0 unx    34114 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/test_model.py
--rw-r--r--  2.0 unx    11193 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/test_model_parallel.py
--rw-r--r--  2.0 unx    25075 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/test_model_parallel_base.py
--rw-r--r--  2.0 unx    15367 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/test_sharding.py
--rw-r--r--  2.0 unx      422 b- defN 23-May-03 11:23 torchrec/fx/__init__.py
--rw-r--r--  2.0 unx     6477 b- defN 23-May-03 11:23 torchrec/fx/tracer.py
--rw-r--r--  2.0 unx     4401 b- defN 23-May-03 11:23 torchrec/fx/utils.py
--rw-r--r--  2.0 unx     1223 b- defN 23-May-03 11:23 torchrec/inference/__init__.py
--rw-r--r--  2.0 unx     3614 b- defN 23-May-03 11:23 torchrec/inference/client.py
--rw-r--r--  2.0 unx     3957 b- defN 23-May-03 11:23 torchrec/inference/model_packager.py
--rw-r--r--  2.0 unx     7834 b- defN 23-May-03 11:23 torchrec/inference/modules.py
--rw-r--r--  2.0 unx     3797 b- defN 23-May-03 11:23 torchrec/inference/state_dict_transform.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/metrics/__init__.py
--rw-r--r--  2.0 unx    12728 b- defN 23-May-03 11:23 torchrec/metrics/auc.py
--rw-r--r--  2.0 unx     3703 b- defN 23-May-03 11:23 torchrec/metrics/calibration.py
--rw-r--r--  2.0 unx     3465 b- defN 23-May-03 11:23 torchrec/metrics/ctr.py
--rw-r--r--  2.0 unx     3836 b- defN 23-May-03 11:23 torchrec/metrics/mae.py
--rw-r--r--  2.0 unx    17573 b- defN 23-May-03 11:23 torchrec/metrics/metric_module.py
--rw-r--r--  2.0 unx     6615 b- defN 23-May-03 11:23 torchrec/metrics/metrics_config.py
--rw-r--r--  2.0 unx     3643 b- defN 23-May-03 11:23 torchrec/metrics/metrics_namespace.py
--rw-r--r--  2.0 unx     3904 b- defN 23-May-03 11:23 torchrec/metrics/model_utils.py
--rw-r--r--  2.0 unx     4631 b- defN 23-May-03 11:23 torchrec/metrics/mse.py
--rw-r--r--  2.0 unx     5605 b- defN 23-May-03 11:23 torchrec/metrics/multiclass_recall.py
--rw-r--r--  2.0 unx     6811 b- defN 23-May-03 11:23 torchrec/metrics/ne.py
--rw-r--r--  2.0 unx    30635 b- defN 23-May-03 11:23 torchrec/metrics/rec_metric.py
--rw-r--r--  2.0 unx    10490 b- defN 23-May-03 11:23 torchrec/metrics/recall_session.py
--rw-r--r--  2.0 unx     6057 b- defN 23-May-03 11:23 torchrec/metrics/throughput.py
--rw-r--r--  2.0 unx    10622 b- defN 23-May-03 11:23 torchrec/metrics/tower_qps.py
--rw-r--r--  2.0 unx     2867 b- defN 23-May-03 11:23 torchrec/metrics/weighted_avg.py
--rw-r--r--  2.0 unx    16441 b- defN 23-May-03 11:23 torchrec/metrics/test_utils/__init__.py
--rw-r--r--  2.0 unx      913 b- defN 23-May-03 11:23 torchrec/models/__init__.py
--rw-r--r--  2.0 unx    11410 b- defN 23-May-03 11:23 torchrec/models/deepfm.py
--rw-r--r--  2.0 unx    30000 b- defN 23-May-03 11:23 torchrec/models/dlrm.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/models/experimental/__init__.py
--rw-r--r--  2.0 unx     9823 b- defN 23-May-03 11:23 torchrec/models/experimental/test_transformerdlrm.py
--rw-r--r--  2.0 unx     7434 b- defN 23-May-03 11:23 torchrec/models/experimental/transformerdlrm.py
--rw-r--r--  2.0 unx     1179 b- defN 23-May-03 11:23 torchrec/modules/__init__.py
--rw-r--r--  2.0 unx     1456 b- defN 23-May-03 11:23 torchrec/modules/activation.py
--rw-r--r--  2.0 unx    15163 b- defN 23-May-03 11:23 torchrec/modules/crossnet.py
--rw-r--r--  2.0 unx     8415 b- defN 23-May-03 11:23 torchrec/modules/deepfm.py
--rw-r--r--  2.0 unx     5131 b- defN 23-May-03 11:23 torchrec/modules/embedding_configs.py
--rw-r--r--  2.0 unx    12822 b- defN 23-May-03 11:23 torchrec/modules/embedding_modules.py
--rw-r--r--  2.0 unx     4858 b- defN 23-May-03 11:23 torchrec/modules/embedding_tower.py
--rw-r--r--  2.0 unx    12360 b- defN 23-May-03 11:23 torchrec/modules/feature_processor.py
--rw-r--r--  2.0 unx    31184 b- defN 23-May-03 11:23 torchrec/modules/fused_embedding_modules.py
--rw-r--r--  2.0 unx    10696 b- defN 23-May-03 11:23 torchrec/modules/lazy_extension.py
--rw-r--r--  2.0 unx     6309 b- defN 23-May-03 11:23 torchrec/modules/mlp.py
--rw-r--r--  2.0 unx     4022 b- defN 23-May-03 11:23 torchrec/modules/utils.py
--rw-r--r--  2.0 unx     1639 b- defN 23-May-03 11:23 torchrec/optim/__init__.py
--rw-r--r--  2.0 unx     2012 b- defN 23-May-03 11:23 torchrec/optim/apply_optimizer_in_backward.py
--rw-r--r--  2.0 unx     1569 b- defN 23-May-03 11:23 torchrec/optim/clipping.py
--rw-r--r--  2.0 unx     1353 b- defN 23-May-03 11:23 torchrec/optim/fused.py
--rw-r--r--  2.0 unx    16069 b- defN 23-May-03 11:23 torchrec/optim/keyed.py
--rw-r--r--  2.0 unx     4420 b- defN 23-May-03 11:23 torchrec/optim/optimizers.py
--rw-r--r--  2.0 unx     7405 b- defN 23-May-03 11:23 torchrec/optim/rowwise_adagrad.py
--rw-r--r--  2.0 unx     4865 b- defN 23-May-03 11:23 torchrec/optim/warmup.py
--rw-r--r--  2.0 unx      560 b- defN 23-May-03 11:23 torchrec/optim/test_utils/__init__.py
--rw-r--r--  2.0 unx     1140 b- defN 23-May-03 11:23 torchrec/quant/__init__.py
--rw-r--r--  2.0 unx    23109 b- defN 23-May-03 11:23 torchrec/quant/embedding_modules.py
--rw-r--r--  2.0 unx     3691 b- defN 23-May-03 11:23 torchrec/quant/utils.py
--rw-r--r--  2.0 unx     1163 b- defN 23-May-03 11:23 torchrec/sparse/__init__.py
--rw-r--r--  2.0 unx    52763 b- defN 23-May-03 11:23 torchrec/sparse/jagged_tensor.py
--rw-r--r--  2.0 unx     1430 b- defN 23-May-03 11:23 torchrec/sparse/test_utils/__init__.py
--rw-r--r--  2.0 unx     5661 b- defN 23-May-03 11:23 torchrec/test_utils/__init__.py
--rw-r--r--  2.0 unx     1530 b- defN 23-May-03 11:28 torchrec_nightly-2023.5.3.dist-info/LICENSE
--rw-r--r--  2.0 unx     5011 b- defN 23-May-03 11:28 torchrec_nightly-2023.5.3.dist-info/METADATA
--rw-r--r--  2.0 unx       93 b- defN 23-May-03 11:28 torchrec_nightly-2023.5.3.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 23-May-03 11:28 torchrec_nightly-2023.5.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    12974 b- defN 23-May-03 11:28 torchrec_nightly-2023.5.3.dist-info/RECORD
-139 files, 1381525 bytes uncompressed, 318740 bytes compressed:  76.9%
+Zip file size: 339209 bytes, number of entries: 139
+-rw-r--r--  2.0 unx      811 b- defN 23-May-05 11:18 torchrec/__init__.py
+-rw-r--r--  2.0 unx     1638 b- defN 23-May-05 11:18 torchrec/streamable.py
+-rw-r--r--  2.0 unx      854 b- defN 23-May-05 11:18 torchrec/types.py
+-rw-r--r--  2.0 unx     1153 b- defN 23-May-05 11:18 torchrec/datasets/__init__.py
+-rw-r--r--  2.0 unx    41469 b- defN 23-May-05 11:18 torchrec/datasets/criteo.py
+-rw-r--r--  2.0 unx     4548 b- defN 23-May-05 11:18 torchrec/datasets/movielens.py
+-rw-r--r--  2.0 unx     6539 b- defN 23-May-05 11:18 torchrec/datasets/random.py
+-rw-r--r--  2.0 unx    10909 b- defN 23-May-05 11:18 torchrec/datasets/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-05 11:18 torchrec/datasets/scripts/__init__.py
+-rw-r--r--  2.0 unx     2448 b- defN 23-May-05 11:18 torchrec/datasets/scripts/contiguous_preproc_criteo.py
+-rw-r--r--  2.0 unx     2847 b- defN 23-May-05 11:18 torchrec/datasets/scripts/npy_preproc_criteo.py
+-rw-r--r--  2.0 unx     3077 b- defN 23-May-05 11:18 torchrec/datasets/scripts/shuffle_preproc_criteo.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-05 11:18 torchrec/datasets/test_utils/__init__.py
+-rw-r--r--  2.0 unx     5308 b- defN 23-May-05 11:18 torchrec/datasets/test_utils/criteo_test_utils.py
+-rw-r--r--  2.0 unx     1912 b- defN 23-May-05 11:18 torchrec/distributed/__init__.py
+-rw-r--r--  2.0 unx    36972 b- defN 23-May-05 11:18 torchrec/distributed/batched_embedding_kernel.py
+-rw-r--r--  2.0 unx     2069 b- defN 23-May-05 11:18 torchrec/distributed/collective_utils.py
+-rw-r--r--  2.0 unx     4925 b- defN 23-May-05 11:18 torchrec/distributed/comm.py
+-rw-r--r--  2.0 unx    55820 b- defN 23-May-05 11:18 torchrec/distributed/comm_ops.py
+-rw-r--r--  2.0 unx    35443 b- defN 23-May-05 11:18 torchrec/distributed/dist_data.py
+-rw-r--r--  2.0 unx    29817 b- defN 23-May-05 11:18 torchrec/distributed/embedding.py
+-rw-r--r--  2.0 unx     3872 b- defN 23-May-05 11:18 torchrec/distributed/embedding_kernel.py
+-rw-r--r--  2.0 unx    27189 b- defN 23-May-05 11:18 torchrec/distributed/embedding_lookup.py
+-rw-r--r--  2.0 unx    14990 b- defN 23-May-05 11:18 torchrec/distributed/embedding_sharding.py
+-rw-r--r--  2.0 unx    37089 b- defN 23-May-05 11:18 torchrec/distributed/embedding_tower_sharding.py
+-rw-r--r--  2.0 unx    15021 b- defN 23-May-05 11:18 torchrec/distributed/embedding_types.py
+-rw-r--r--  2.0 unx    34625 b- defN 23-May-05 11:18 torchrec/distributed/embeddingbag.py
+-rw-r--r--  2.0 unx     7373 b- defN 23-May-05 11:18 torchrec/distributed/fbgemm_qcomm_codec.py
+-rw-r--r--  2.0 unx     5273 b- defN 23-May-05 11:18 torchrec/distributed/fused_embedding.py
+-rw-r--r--  2.0 unx     5110 b- defN 23-May-05 11:18 torchrec/distributed/fused_embeddingbag.py
+-rw-r--r--  2.0 unx     1699 b- defN 23-May-05 11:18 torchrec/distributed/fused_params.py
+-rw-r--r--  2.0 unx     3807 b- defN 23-May-05 11:18 torchrec/distributed/grouped_position_weighted.py
+-rw-r--r--  2.0 unx    19528 b- defN 23-May-05 11:18 torchrec/distributed/model_parallel.py
+-rw-r--r--  2.0 unx    13316 b- defN 23-May-05 11:18 torchrec/distributed/quant_embedding.py
+-rw-r--r--  2.0 unx    13385 b- defN 23-May-05 11:18 torchrec/distributed/quant_embedding_kernel.py
+-rw-r--r--  2.0 unx    10510 b- defN 23-May-05 11:18 torchrec/distributed/quant_embeddingbag.py
+-rw-r--r--  2.0 unx     9261 b- defN 23-May-05 11:18 torchrec/distributed/shard.py
+-rw-r--r--  2.0 unx    19218 b- defN 23-May-05 11:18 torchrec/distributed/sharding_plan.py
+-rw-r--r--  2.0 unx    22330 b- defN 23-May-05 11:18 torchrec/distributed/train_pipeline.py
+-rw-r--r--  2.0 unx    24927 b- defN 23-May-05 11:18 torchrec/distributed/types.py
+-rw-r--r--  2.0 unx    11373 b- defN 23-May-05 11:18 torchrec/distributed/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-05 11:18 torchrec/distributed/composable/__init__.py
+-rw-r--r--  2.0 unx     3207 b- defN 23-May-05 11:18 torchrec/distributed/composable/table_batched_embedding_slice.py
+-rw-r--r--  2.0 unx     1025 b- defN 23-May-05 11:18 torchrec/distributed/planner/__init__.py
+-rw-r--r--  2.0 unx     3135 b- defN 23-May-05 11:18 torchrec/distributed/planner/constants.py
+-rw-r--r--  2.0 unx    10318 b- defN 23-May-05 11:18 torchrec/distributed/planner/enumerators.py
+-rw-r--r--  2.0 unx    12485 b- defN 23-May-05 11:18 torchrec/distributed/planner/partitioners.py
+-rw-r--r--  2.0 unx      824 b- defN 23-May-05 11:18 torchrec/distributed/planner/perf_models.py
+-rw-r--r--  2.0 unx    12224 b- defN 23-May-05 11:18 torchrec/distributed/planner/planners.py
+-rw-r--r--  2.0 unx    11094 b- defN 23-May-05 11:18 torchrec/distributed/planner/proposers.py
+-rw-r--r--  2.0 unx    40173 b- defN 23-May-05 11:18 torchrec/distributed/planner/shard_estimators.py
+-rw-r--r--  2.0 unx    21410 b- defN 23-May-05 11:18 torchrec/distributed/planner/stats.py
+-rw-r--r--  2.0 unx     9125 b- defN 23-May-05 11:18 torchrec/distributed/planner/storage_reservations.py
+-rw-r--r--  2.0 unx    12879 b- defN 23-May-05 11:18 torchrec/distributed/planner/types.py
+-rw-r--r--  2.0 unx     1119 b- defN 23-May-05 11:18 torchrec/distributed/planner/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-05 11:18 torchrec/distributed/sharding/__init__.py
+-rw-r--r--  2.0 unx     2539 b- defN 23-May-05 11:18 torchrec/distributed/sharding/cw_sequence_sharding.py
+-rw-r--r--  2.0 unx     9519 b- defN 23-May-05 11:18 torchrec/distributed/sharding/cw_sharding.py
+-rw-r--r--  2.0 unx     2802 b- defN 23-May-05 11:18 torchrec/distributed/sharding/dp_sequence_sharding.py
+-rw-r--r--  2.0 unx     7452 b- defN 23-May-05 11:18 torchrec/distributed/sharding/dp_sharding.py
+-rw-r--r--  2.0 unx     5041 b- defN 23-May-05 11:18 torchrec/distributed/sharding/rw_sequence_sharding.py
+-rw-r--r--  2.0 unx    12850 b- defN 23-May-05 11:18 torchrec/distributed/sharding/rw_sharding.py
+-rw-r--r--  2.0 unx     3114 b- defN 23-May-05 11:18 torchrec/distributed/sharding/sequence_sharding.py
+-rw-r--r--  2.0 unx     7609 b- defN 23-May-05 11:18 torchrec/distributed/sharding/tw_sequence_sharding.py
+-rw-r--r--  2.0 unx    16061 b- defN 23-May-05 11:18 torchrec/distributed/sharding/tw_sharding.py
+-rw-r--r--  2.0 unx     1284 b- defN 23-May-05 11:18 torchrec/distributed/sharding/twcw_sharding.py
+-rw-r--r--  2.0 unx    19840 b- defN 23-May-05 11:18 torchrec/distributed/sharding/twrw_sharding.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-05 11:18 torchrec/distributed/test_utils/__init__.py
+-rw-r--r--  2.0 unx    10125 b- defN 23-May-05 11:18 torchrec/distributed/test_utils/infer_utils.py
+-rw-r--r--  2.0 unx     4868 b- defN 23-May-05 11:18 torchrec/distributed/test_utils/multi_process.py
+-rw-r--r--  2.0 unx    34114 b- defN 23-May-05 11:18 torchrec/distributed/test_utils/test_model.py
+-rw-r--r--  2.0 unx    11193 b- defN 23-May-05 11:18 torchrec/distributed/test_utils/test_model_parallel.py
+-rw-r--r--  2.0 unx    25075 b- defN 23-May-05 11:18 torchrec/distributed/test_utils/test_model_parallel_base.py
+-rw-r--r--  2.0 unx    15367 b- defN 23-May-05 11:18 torchrec/distributed/test_utils/test_sharding.py
+-rw-r--r--  2.0 unx      422 b- defN 23-May-05 11:18 torchrec/fx/__init__.py
+-rw-r--r--  2.0 unx     6477 b- defN 23-May-05 11:18 torchrec/fx/tracer.py
+-rw-r--r--  2.0 unx     4401 b- defN 23-May-05 11:18 torchrec/fx/utils.py
+-rw-r--r--  2.0 unx     1223 b- defN 23-May-05 11:18 torchrec/inference/__init__.py
+-rw-r--r--  2.0 unx     3614 b- defN 23-May-05 11:18 torchrec/inference/client.py
+-rw-r--r--  2.0 unx     3957 b- defN 23-May-05 11:18 torchrec/inference/model_packager.py
+-rw-r--r--  2.0 unx     7834 b- defN 23-May-05 11:18 torchrec/inference/modules.py
+-rw-r--r--  2.0 unx     3797 b- defN 23-May-05 11:18 torchrec/inference/state_dict_transform.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-05 11:18 torchrec/metrics/__init__.py
+-rw-r--r--  2.0 unx    12728 b- defN 23-May-05 11:18 torchrec/metrics/auc.py
+-rw-r--r--  2.0 unx     3703 b- defN 23-May-05 11:18 torchrec/metrics/calibration.py
+-rw-r--r--  2.0 unx     3465 b- defN 23-May-05 11:18 torchrec/metrics/ctr.py
+-rw-r--r--  2.0 unx     3836 b- defN 23-May-05 11:18 torchrec/metrics/mae.py
+-rw-r--r--  2.0 unx    17812 b- defN 23-May-05 11:18 torchrec/metrics/metric_module.py
+-rw-r--r--  2.0 unx     6615 b- defN 23-May-05 11:18 torchrec/metrics/metrics_config.py
+-rw-r--r--  2.0 unx     3643 b- defN 23-May-05 11:18 torchrec/metrics/metrics_namespace.py
+-rw-r--r--  2.0 unx     3904 b- defN 23-May-05 11:18 torchrec/metrics/model_utils.py
+-rw-r--r--  2.0 unx     4631 b- defN 23-May-05 11:18 torchrec/metrics/mse.py
+-rw-r--r--  2.0 unx     5605 b- defN 23-May-05 11:18 torchrec/metrics/multiclass_recall.py
+-rw-r--r--  2.0 unx     6811 b- defN 23-May-05 11:18 torchrec/metrics/ne.py
+-rw-r--r--  2.0 unx    30881 b- defN 23-May-05 11:18 torchrec/metrics/rec_metric.py
+-rw-r--r--  2.0 unx    10490 b- defN 23-May-05 11:18 torchrec/metrics/recall_session.py
+-rw-r--r--  2.0 unx     6057 b- defN 23-May-05 11:18 torchrec/metrics/throughput.py
+-rw-r--r--  2.0 unx    10622 b- defN 23-May-05 11:18 torchrec/metrics/tower_qps.py
+-rw-r--r--  2.0 unx     2867 b- defN 23-May-05 11:18 torchrec/metrics/weighted_avg.py
+-rw-r--r--  2.0 unx    16441 b- defN 23-May-05 11:18 torchrec/metrics/test_utils/__init__.py
+-rw-r--r--  2.0 unx      913 b- defN 23-May-05 11:18 torchrec/models/__init__.py
+-rw-r--r--  2.0 unx    11410 b- defN 23-May-05 11:18 torchrec/models/deepfm.py
+-rw-r--r--  2.0 unx    30000 b- defN 23-May-05 11:18 torchrec/models/dlrm.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-05 11:18 torchrec/models/experimental/__init__.py
+-rw-r--r--  2.0 unx     9823 b- defN 23-May-05 11:18 torchrec/models/experimental/test_transformerdlrm.py
+-rw-r--r--  2.0 unx     7434 b- defN 23-May-05 11:18 torchrec/models/experimental/transformerdlrm.py
+-rw-r--r--  2.0 unx     1179 b- defN 23-May-05 11:18 torchrec/modules/__init__.py
+-rw-r--r--  2.0 unx     1456 b- defN 23-May-05 11:18 torchrec/modules/activation.py
+-rw-r--r--  2.0 unx    15163 b- defN 23-May-05 11:18 torchrec/modules/crossnet.py
+-rw-r--r--  2.0 unx     8415 b- defN 23-May-05 11:18 torchrec/modules/deepfm.py
+-rw-r--r--  2.0 unx     5131 b- defN 23-May-05 11:18 torchrec/modules/embedding_configs.py
+-rw-r--r--  2.0 unx    12822 b- defN 23-May-05 11:18 torchrec/modules/embedding_modules.py
+-rw-r--r--  2.0 unx     4858 b- defN 23-May-05 11:18 torchrec/modules/embedding_tower.py
+-rw-r--r--  2.0 unx    12360 b- defN 23-May-05 11:18 torchrec/modules/feature_processor.py
+-rw-r--r--  2.0 unx    31184 b- defN 23-May-05 11:18 torchrec/modules/fused_embedding_modules.py
+-rw-r--r--  2.0 unx    10696 b- defN 23-May-05 11:18 torchrec/modules/lazy_extension.py
+-rw-r--r--  2.0 unx     6309 b- defN 23-May-05 11:18 torchrec/modules/mlp.py
+-rw-r--r--  2.0 unx     4022 b- defN 23-May-05 11:18 torchrec/modules/utils.py
+-rw-r--r--  2.0 unx     1639 b- defN 23-May-05 11:18 torchrec/optim/__init__.py
+-rw-r--r--  2.0 unx     2012 b- defN 23-May-05 11:18 torchrec/optim/apply_optimizer_in_backward.py
+-rw-r--r--  2.0 unx     1569 b- defN 23-May-05 11:18 torchrec/optim/clipping.py
+-rw-r--r--  2.0 unx     1353 b- defN 23-May-05 11:18 torchrec/optim/fused.py
+-rw-r--r--  2.0 unx    16069 b- defN 23-May-05 11:18 torchrec/optim/keyed.py
+-rw-r--r--  2.0 unx     4420 b- defN 23-May-05 11:18 torchrec/optim/optimizers.py
+-rw-r--r--  2.0 unx     7405 b- defN 23-May-05 11:18 torchrec/optim/rowwise_adagrad.py
+-rw-r--r--  2.0 unx     4865 b- defN 23-May-05 11:18 torchrec/optim/warmup.py
+-rw-r--r--  2.0 unx      560 b- defN 23-May-05 11:18 torchrec/optim/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1140 b- defN 23-May-05 11:18 torchrec/quant/__init__.py
+-rw-r--r--  2.0 unx    22824 b- defN 23-May-05 11:18 torchrec/quant/embedding_modules.py
+-rw-r--r--  2.0 unx     3691 b- defN 23-May-05 11:18 torchrec/quant/utils.py
+-rw-r--r--  2.0 unx     1163 b- defN 23-May-05 11:18 torchrec/sparse/__init__.py
+-rw-r--r--  2.0 unx    52763 b- defN 23-May-05 11:18 torchrec/sparse/jagged_tensor.py
+-rw-r--r--  2.0 unx     1430 b- defN 23-May-05 11:18 torchrec/sparse/test_utils/__init__.py
+-rw-r--r--  2.0 unx     5661 b- defN 23-May-05 11:18 torchrec/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1530 b- defN 23-May-05 11:22 torchrec_nightly-2023.5.5.dist-info/LICENSE
+-rw-r--r--  2.0 unx     5011 b- defN 23-May-05 11:22 torchrec_nightly-2023.5.5.dist-info/METADATA
+-rw-r--r--  2.0 unx       93 b- defN 23-May-05 11:22 torchrec_nightly-2023.5.5.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 23-May-05 11:22 torchrec_nightly-2023.5.5.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    12974 b- defN 23-May-05 11:22 torchrec_nightly-2023.5.5.dist-info/RECORD
+139 files, 1379518 bytes uncompressed, 318449 bytes compressed:  76.9%
```

## zipnote {}

```diff
@@ -396,23 +396,23 @@
 
 Filename: torchrec/sparse/test_utils/__init__.py
 Comment: 
 
 Filename: torchrec/test_utils/__init__.py
 Comment: 
 
-Filename: torchrec_nightly-2023.5.3.dist-info/LICENSE
+Filename: torchrec_nightly-2023.5.5.dist-info/LICENSE
 Comment: 
 
-Filename: torchrec_nightly-2023.5.3.dist-info/METADATA
+Filename: torchrec_nightly-2023.5.5.dist-info/METADATA
 Comment: 
 
-Filename: torchrec_nightly-2023.5.3.dist-info/WHEEL
+Filename: torchrec_nightly-2023.5.5.dist-info/WHEEL
 Comment: 
 
-Filename: torchrec_nightly-2023.5.3.dist-info/top_level.txt
+Filename: torchrec_nightly-2023.5.5.dist-info/top_level.txt
 Comment: 
 
-Filename: torchrec_nightly-2023.5.3.dist-info/RECORD
+Filename: torchrec_nightly-2023.5.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## torchrec/distributed/batched_embedding_kernel.py

```diff
@@ -5,26 +5,15 @@
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 import abc
 import copy
 import itertools
 from dataclasses import dataclass
-from typing import (
-    Any,
-    cast,
-    Dict,
-    Generic,
-    Iterator,
-    List,
-    Optional,
-    Tuple,
-    TypeVar,
-    Union,
-)
+from typing import Any, cast, Dict, Iterator, List, Optional, Tuple, Union
 
 import torch
 import torch.distributed as dist
 from fbgemm_gpu.split_table_batched_embeddings_ops import (
     ComputeDevice,
     DenseTableBatchedEmbeddingBagsCodegen,
     EmbeddingLocation,
@@ -432,18 +421,15 @@
             end_offset=offset + table_count * rows * dim,
             num_embeddings=-1,
             embedding_dim=dim,
         )
         yield (table_name, weight)
 
 
-SplitWeightType = TypeVar("SplitWeightType")
-
-
-class BaseBatchedEmbedding(BaseEmbedding, Generic[SplitWeightType]):
+class BaseBatchedEmbedding(BaseEmbedding):
     def __init__(
         self,
         config: GroupedEmbeddingConfig,
         pg: Optional[dist.ProcessGroup] = None,
         device: Optional[torch.device] = None,
     ) -> None:
         super().__init__()
@@ -499,22 +485,21 @@
         destination: Optional[Dict[str, Any]] = None,
         prefix: str = "",
         keep_vars: bool = False,
     ) -> Dict[str, Any]:
         self.flush()
         return get_state_dict(
             self._config.embedding_tables,
-            # pyre-ignore
             self.split_embedding_weights(),
             self._pg,
             destination,
             prefix,
         )
 
-    def split_embedding_weights(self) -> List[SplitWeightType]:
+    def split_embedding_weights(self) -> List[torch.Tensor]:
         return self.emb_module.split_embedding_weights()
 
     @property
     @abc.abstractmethod
     def emb_module(
         self,
     ) -> Union[
@@ -552,15 +537,15 @@
         For a single table with multiple shards (i.e CW) these are combined into one table/weight.
         Used in composability.
         """
         for name, param in self._param_per_table.items():
             yield name, param
 
 
-class BatchedFusedEmbedding(BaseBatchedEmbedding[torch.Tensor], FusedOptimizerModule):
+class BatchedFusedEmbedding(BaseBatchedEmbedding, FusedOptimizerModule):
     def __init__(
         self,
         config: GroupedEmbeddingConfig,
         pg: Optional[dist.ProcessGroup] = None,
         device: Optional[torch.device] = None,
     ) -> None:
         super().__init__(config, pg, device)
@@ -643,15 +628,15 @@
             param._in_backward_optimizers = [EmptyFusedOptimizer()]
             yield name, param
 
     def flush(self) -> None:
         self._emb_module.flush()
 
 
-class BatchedDenseEmbedding(BaseBatchedEmbedding[torch.Tensor]):
+class BatchedDenseEmbedding(BaseBatchedEmbedding):
     def __init__(
         self,
         config: GroupedEmbeddingConfig,
         pg: Optional[dist.ProcessGroup] = None,
         device: Optional[torch.device] = None,
     ) -> None:
         super().__init__(config, pg, device)
@@ -693,15 +678,15 @@
             [config.name for config in self._config.embedding_tables]
         )
         yield append_prefix(prefix, f"{combined_key}.weight"), cast(
             nn.Parameter, self._emb_module.weights
         )
 
 
-class BaseBatchedEmbeddingBag(BaseEmbedding, Generic[SplitWeightType]):
+class BaseBatchedEmbeddingBag(BaseEmbedding):
     def __init__(
         self,
         config: GroupedEmbeddingConfig,
         pg: Optional[dist.ProcessGroup] = None,
         device: Optional[torch.device] = None,
     ) -> None:
         super().__init__()
@@ -765,22 +750,21 @@
         destination: Optional[Dict[str, Any]] = None,
         prefix: str = "",
         keep_vars: bool = False,
     ) -> Dict[str, Any]:
         self.flush()
         return get_state_dict(
             self._config.embedding_tables,
-            # pyre-ignore
             self.split_embedding_weights(),
             self._pg,
             destination,
             prefix,
         )
 
-    def split_embedding_weights(self) -> List[SplitWeightType]:
+    def split_embedding_weights(self) -> List[torch.Tensor]:
         return self.emb_module.split_embedding_weights()
 
     @property
     @abc.abstractmethod
     def emb_module(
         self,
     ) -> Union[
@@ -818,17 +802,15 @@
         For a single table with multiple shards (i.e CW) these are combined into one table/weight.
         Used in composability.
         """
         for name, param in self._param_per_table.items():
             yield name, param
 
 
-class BatchedFusedEmbeddingBag(
-    BaseBatchedEmbeddingBag[torch.Tensor], FusedOptimizerModule
-):
+class BatchedFusedEmbeddingBag(BaseBatchedEmbeddingBag, FusedOptimizerModule):
     def __init__(
         self,
         config: GroupedEmbeddingConfig,
         pg: Optional[dist.ProcessGroup] = None,
         device: Optional[torch.device] = None,
     ) -> None:
         super().__init__(config, pg, device)
@@ -914,15 +896,15 @@
             param._in_backward_optimizers = [EmptyFusedOptimizer()]
             yield name, param
 
     def flush(self) -> None:
         self._emb_module.flush()
 
 
-class BatchedDenseEmbeddingBag(BaseBatchedEmbeddingBag[torch.Tensor]):
+class BatchedDenseEmbeddingBag(BaseBatchedEmbeddingBag):
     def __init__(
         self,
         config: GroupedEmbeddingConfig,
         pg: Optional[dist.ProcessGroup] = None,
         device: Optional[torch.device] = None,
     ) -> None:
         super().__init__(config, pg, device)
```

## torchrec/distributed/embedding_kernel.py

```diff
@@ -4,15 +4,15 @@
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 import abc
 import logging
 from collections import defaultdict, OrderedDict
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import Any, Dict, List, Optional, Union
 
 import torch
 import torch.distributed as dist
 from torch import nn
 from torchrec.distributed.embedding_types import (
     EmbeddingComputeKernel,
     GroupedEmbeddingConfig,
@@ -50,15 +50,14 @@
 
 def get_state_dict(
     embedding_tables: List[ShardedEmbeddingTable],
     params: Union[
         nn.ModuleList,
         List[Union[nn.Module, torch.Tensor]],
         List[torch.Tensor],
-        List[Tuple[torch.Tensor, Optional[torch.Tensor]]],
     ],
     pg: Optional[dist.ProcessGroup] = None,
     destination: Optional[Dict[str, Any]] = None,
     prefix: str = "",
 ) -> Dict[str, Any]:
     if destination is None:
         destination = OrderedDict()
@@ -73,44 +72,35 @@
     key_to_global_metadata: Dict[str, ShardedTensorMetadata] = {}
 
     def get_key_from_embedding_table(embedding_table: ShardedEmbeddingTable) -> str:
         return prefix + f"{embedding_table.name}.weight"
 
     for embedding_table, param in zip(embedding_tables, params):
         key = get_key_from_embedding_table(embedding_table)
-        is_quant = embedding_table.compute_kernel in [
+        assert embedding_table.local_rows == param.size(0)
+        if embedding_table.compute_kernel not in [
             EmbeddingComputeKernel.QUANT,
             EmbeddingComputeKernel.QUANT_UVM,
             EmbeddingComputeKernel.QUANT_UVM_CACHING,
-        ]
-        qscaleshift_split = None
-        if is_quant:
-            # For QUANT* param is Tuple[torch.Tensor, Optional[torch.Tensor]] where first argument is the weight table, the second is optional quantization extra information, depending on quantization type. e.g. for fbgemm rowwise quantization this is scale and shift for each row.
-            assert isinstance(param, tuple)
-            qscaleshift_split = param[1]
-            param = param[0]
-
-        assert embedding_table.local_rows == param.size(0)
-        assert embedding_table.local_cols == param.size(1)
-
+        ]:
+            assert embedding_table.local_cols == param.size(1)
+        # for inference there is no pg, all tensors are local
         if embedding_table.global_metadata is not None and pg is not None:
             # set additional field of sharded tensor based on local tensor properties
             embedding_table.global_metadata.tensor_properties.dtype = param.dtype
             embedding_table.global_metadata.tensor_properties.requires_grad = (
                 param.requires_grad
             )
             key_to_global_metadata[key] = embedding_table.global_metadata
 
             key_to_local_shards[key].append(
                 Shard(param, embedding_table.local_metadata)
             )
         else:
             destination[key] = param
-            if qscaleshift_split is not None:
-                destination[f"{key}_qscaleshift"] = qscaleshift_split
 
     if pg is not None:
         # Populate the remaining destinations that have a global metadata
         for key in key_to_local_shards:
             global_metadata = key_to_global_metadata[key]
             destination[
                 key
```

## torchrec/distributed/embedding_lookup.py

```diff
@@ -403,15 +403,15 @@
         fused_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         # TODO rename to _create_embedding_kernel
         def _create_lookup(
             config: GroupedEmbeddingConfig,
             device: Optional[torch.device] = None,
             fused_params: Optional[Dict[str, Any]] = None,
-        ) -> BaseBatchedEmbedding[Tuple[torch.Tensor, Optional[torch.Tensor]]]:
+        ) -> BaseBatchedEmbedding:
             return QuantBatchedEmbedding(
                 config=config,
                 device=device,
                 fused_params=fused_params,
             )
 
         super().__init__()
@@ -520,15 +520,15 @@
         fused_params: Optional[Dict[str, Any]] = None,
     ) -> None:
         # TODO rename to _create_embedding_kernel
         def _create_lookup(
             config: GroupedEmbeddingConfig,
             device: Optional[torch.device] = None,
             fused_params: Optional[Dict[str, Any]] = None,
-        ) -> BaseBatchedEmbeddingBag[Tuple[torch.Tensor, Optional[torch.Tensor]]]:
+        ) -> BaseBatchedEmbeddingBag:
             return QuantBatchedEmbeddingBag(
                 config=config,
                 device=device,
                 fused_params=fused_params,
             )
 
         super().__init__()
```

## torchrec/distributed/quant_embedding.py

```diff
@@ -202,27 +202,21 @@
             self.embeddings[table.name] = torch.nn.Module()
 
         for _sharding_type, lookup in zip(
             self._sharding_type_to_sharding.keys(), self._lookups
         ):
             lookup_state_dict = lookup.state_dict()
             for key in lookup_state_dict:
-                if key.endswith(".weight"):
-                    table_name = key[: -len(".weight")]
-                    # Register as buffer because this is an inference model, and can potentially use uint8 types.
-                    self.embeddings[table_name].register_buffer(
-                        "weight", lookup_state_dict[key]
-                    )
-                elif key.endswith(".weight_qscaleshift"):
-                    table_name = key[: -len(".weight_qscaleshift")]
-                    self.embeddings[table_name].register_buffer(
-                        "weight_qscaleshift", lookup_state_dict[key]
-                    )
-                else:
+                if not key.endswith(".weight"):
                     continue
+                table_name = key[: -len(".weight")]
+                # Register as buffer because this is an inference model, and can potentially use uint8 types.
+                self.embeddings[table_name].register_buffer(
+                    "weight", lookup_state_dict[key]
+                )
 
         # Optional registration of TBEs for model post processing utilities
         if is_fused_param_register_tbe(fused_params):
             tbes: Dict[
                 IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig
             ] = get_tbes_to_register_from_iterable(self._lookups)
```

## torchrec/distributed/quant_embedding_kernel.py

```diff
@@ -109,18 +109,15 @@
         quant_weight = quantized_weights[:, :-4]
         scale_shift = quantized_weights[:, -4:]
 
         quant_weight_list.append((quant_weight, scale_shift))
     return quant_weight_list
 
 
-class QuantBatchedEmbeddingBag(
-    BaseBatchedEmbeddingBag[Tuple[torch.Tensor, Optional[torch.Tensor]]],
-    TBEToRegisterMixIn,
-):
+class QuantBatchedEmbeddingBag(BaseBatchedEmbeddingBag, TBEToRegisterMixIn):
     def __init__(
         self,
         config: GroupedEmbeddingConfig,
         pg: Optional[dist.ProcessGroup] = None,
         device: Optional[torch.device] = None,
         fused_params: Optional[Dict[str, Any]] = None,
     ) -> None:
@@ -197,30 +194,25 @@
 
     def named_buffers(
         self, prefix: str = "", recurse: bool = True, remove_duplicate: bool = True
     ) -> Iterator[Tuple[str, torch.Tensor]]:
         assert (
             remove_duplicate
         ), "remove_duplicate=False not supported in QuantBatchedEmbeddingBag.named_split_embedding_weights"
-        for config, (weight, weight_qscaleshift) in zip(
+        for config, weight in zip(
             self._config.embedding_tables,
-            self.emb_module.split_embedding_weights(split_scale_shifts=True),
+            self.emb_module.split_embedding_weights(),
         ):
             yield append_prefix(prefix, f"{config.name}.weight"), weight[0]
-            yield append_prefix(
-                prefix, f"{config.name}.weight_qscaleshift"
-            ), weight_qscaleshift
 
-    def split_embedding_weights(
-        self,
-    ) -> List[Tuple[torch.Tensor, Optional[torch.Tensor]]]:
+    def split_embedding_weights(self) -> List[torch.Tensor]:
         return [
-            (weight, weight_qscaleshift)
-            for weight, weight_qscaleshift in self.emb_module.split_embedding_weights(
-                split_scale_shifts=True
+            weight
+            for weight, _ in self.emb_module.split_embedding_weights(
+                split_scale_shifts=False
             )
         ]
 
     @classmethod
     def from_float(cls, module: BaseEmbedding) -> "QuantBatchedEmbeddingBag":
         assert hasattr(
             module, "qconfig"
@@ -244,18 +236,15 @@
         # pyre-ignore
         quant_weight_list = _quantize_weight(state_dict, data_type)
         ret.emb_module.assign_embedding_weights(quant_weight_list)
 
         return ret
 
 
-class QuantBatchedEmbedding(
-    BaseBatchedEmbedding[Tuple[torch.Tensor, Optional[torch.Tensor]]],
-    TBEToRegisterMixIn,
-):
+class QuantBatchedEmbedding(BaseBatchedEmbedding, TBEToRegisterMixIn):
     def __init__(
         self,
         config: GroupedEmbeddingConfig,
         pg: Optional[dist.ProcessGroup] = None,
         device: Optional[torch.device] = None,
         fused_params: Optional[Dict[str, Any]] = None,
     ) -> None:
@@ -301,21 +290,19 @@
         return self._emb_module
 
     def get_tbes_to_register(
         self,
     ) -> Dict[IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig]:
         return {self._emb_module: self._config}
 
-    def split_embedding_weights(
-        self,
-    ) -> List[Tuple[torch.Tensor, Optional[torch.Tensor]]]:
+    def split_embedding_weights(self) -> List[torch.Tensor]:
         return [
-            (weight, weight_qscaleshift)
-            for weight, weight_qscaleshift in self.emb_module.split_embedding_weights(
-                split_scale_shifts=True
+            weight
+            for weight, _ in self.emb_module.split_embedding_weights(
+                split_scale_shifts=False
             )
         ]
 
     def forward(self, features: KeyedJaggedTensor) -> torch.Tensor:
         if self._emb_module_registered:
             return self.emb_module(
                 indices=features.values().int(),
@@ -326,22 +313,19 @@
                 indices=features.values().int(),
                 offsets=features.offsets().int(),
             )
 
     def named_buffers(
         self, prefix: str = "", recurse: bool = True, remove_duplicate: bool = True
     ) -> Iterator[Tuple[str, torch.Tensor]]:
-        for config, (weight, weight_qscaleshift) in zip(
+        for config, weight in zip(
             self._config.embedding_tables,
-            self.emb_module.split_embedding_weights(split_scale_shifts=True),
+            self.emb_module.split_embedding_weights(),
         ):
-            yield append_prefix(prefix, f"{config.name}.weight"), weight
-            yield append_prefix(
-                prefix, f"{config.name}.weight_qscaleshift"
-            ), weight_qscaleshift
+            yield append_prefix(prefix, f"{config.name}.weight"), weight[0]
 
     @classmethod
     def from_float(cls, module: BaseEmbedding) -> "QuantBatchedEmbedding":
         assert hasattr(
             module, "qconfig"
         ), "BaseEmbedding input float module must have qconfig defined"
```

## torchrec/distributed/quant_embeddingbag.py

```diff
@@ -131,27 +131,21 @@
             self.embedding_bags[table.name] = torch.nn.Module()
 
         for _sharding_type, lookup in zip(
             self._sharding_type_to_sharding.keys(), self._lookups
         ):
             lookup_state_dict = lookup.state_dict()
             for key in lookup_state_dict:
-                if key.endswith(".weight"):
-                    table_name = key[: -len(".weight")]
-                    # Register as buffer because this is an inference model, and can potentially use uint8 types.
-                    self.embedding_bags[table_name].register_buffer(
-                        "weight", lookup_state_dict[key]
-                    )
-                elif key.endswith("weight_qscaleshift"):
-                    table_name = key[: -len(".weight_qscaleshift")]
-                    self.embedding_bags[table_name].register_buffer(
-                        "weight_qscaleshift", lookup_state_dict[key]
-                    )
-                else:
+                if not key.endswith(".weight"):
                     continue
+                table_name = key[: -len(".weight")]
+                # Register as buffer because this is an inference model, and can potentially use uint8 types.
+                self.embedding_bags[table_name].register_buffer(
+                    "weight", lookup_state_dict[key]
+                )
 
         # Optional registration of TBEs for model post processing utilities
         if is_fused_param_register_tbe(fused_params):
             tbes: Dict[
                 IntNBitTableBatchedEmbeddingBagsCodegen, GroupedEmbeddingConfig
             ] = get_tbes_to_register_from_iterable(self._lookups)
```

## torchrec/metrics/metric_module.py

```diff
@@ -11,14 +11,15 @@
 import logging
 import time
 from typing import Any, Dict, List, Optional, Type, Union
 
 import torch
 import torch.distributed as dist
 import torch.nn as nn
+from torch.profiler import record_function
 from torchrec.metrics.auc import AUCMetric
 from torchrec.metrics.calibration import CalibrationMetric
 from torchrec.metrics.ctr import CTRMetric
 from torchrec.metrics.mae import MAEMetric
 from torchrec.metrics.metrics_config import (
     MetricsConfig,
     RecMetricEnum,
@@ -238,18 +239,19 @@
     def update(self, model_out: Dict[str, torch.Tensor], **kwargs: Any) -> None:
         r"""update() is called per batch, usually right after forward() to
         update the local states of metrics based on the model_output.
 
         Throughput.update() is also called due to the implementation sliding window
         throughput.
         """
-        self._update_rec_metrics(model_out, **kwargs)
-        if self.throughput_metric:
-            self.throughput_metric.update()
-        self.trained_batches += 1
+        with record_function("## RecMetricModule:update ##"):
+            self._update_rec_metrics(model_out, **kwargs)
+            if self.throughput_metric:
+                self.throughput_metric.update()
+            self.trained_batches += 1
 
     def _adjust_compute_interval(self) -> None:
         """
         Adjust the compute interval (in batches) based on the first two time
         elapsed between the first two compute().
         """
         if self.last_compute_time > 0 and self.min_compute_interval >= 0:
@@ -302,29 +304,29 @@
 
     def compute(self) -> Dict[str, MetricValue]:
         r"""compute() is called when the global metrics are required, usually
         right before logging the metrics results to the data sink.
         """
         self.compute_count += 1
         self.check_memory_usage(self.compute_count)
-
-        ret: Dict[str, MetricValue] = {}
-        if self.rec_metrics:
-            self._adjust_compute_interval()
-            ret.update(self.rec_metrics.compute())
-        if self.throughput_metric:
-            ret.update(self.throughput_metric.compute())
-        if self.state_metrics:
-            for namespace, component in self.state_metrics.items():
-                ret.update(
-                    {
-                        f"{compose_customized_metric_key(namespace, metric_name)}": metric_value
-                        for metric_name, metric_value in component.get_metrics().items()
-                    }
-                )
+        with record_function("## RecMetricModule:compute ##"):
+            ret: Dict[str, MetricValue] = {}
+            if self.rec_metrics:
+                self._adjust_compute_interval()
+                ret.update(self.rec_metrics.compute())
+            if self.throughput_metric:
+                ret.update(self.throughput_metric.compute())
+            if self.state_metrics:
+                for namespace, component in self.state_metrics.items():
+                    ret.update(
+                        {
+                            f"{compose_customized_metric_key(namespace, metric_name)}": metric_value
+                            for metric_name, metric_value in component.get_metrics().items()
+                        }
+                    )
         return ret
 
     def local_compute(self) -> Dict[str, MetricValue]:
         r"""local_compute() is called when per-trainer metrics are required. It's
         can be used for debugging. Currently only rec_metrics is supported.
         """
         ret: Dict[str, MetricValue] = {}
```

## torchrec/metrics/rec_metric.py

```diff
@@ -30,14 +30,15 @@
     TypeVar,
     Union,
 )
 
 import torch
 import torch.distributed as dist
 import torch.nn as nn
+from torch.profiler import record_function
 from torchmetrics import Metric
 from torchrec.metrics.metrics_config import RecComputeMode, RecTaskInfo
 from torchrec.metrics.metrics_namespace import (
     compose_metric_key,
     MetricNameBase,
     MetricNamespaceBase,
     MetricPrefix,
@@ -235,18 +236,19 @@
         some pre-processing of the local state before `compute()` as TorchMetric
         wraps `RecMetricComputation.compute()` and will do the global aggregation
         before `RecMetricComputation.compute()` is called.
         """
         return
 
     def compute(self) -> List[MetricComputationReport]:
-        if self._my_rank == 0 or self._compute_on_all_ranks:
-            return self._compute()
-        else:
-            return []
+        with record_function(f"## {self.__class__.__name__}:compute ##"):
+            if self._my_rank == 0 or self._compute_on_all_ranks:
+                return self._compute()
+            else:
+                return []
 
     def local_compute(self) -> List[MetricComputationReport]:
         return self._compute()
 
 
 class RecMetric(nn.Module, abc.ABC):
     r"""The main class template to implement a recommendation metric.
@@ -584,24 +586,25 @@
         self,
         *,
         predictions: RecModelOutput,
         labels: RecModelOutput,
         weights: Optional[RecModelOutput],
         **kwargs: Dict[str, Any],
     ) -> None:
-        if self._fused_update_limit > 0:
-            self._update_buffers[self.PREDICTIONS].append(predictions)
-            self._update_buffers[self.LABELS].append(labels)
-            if weights is not None:
-                self._update_buffers[self.WEIGHTS].append(weights)
-            self._check_fused_update(force=False)
-        else:
-            self._update(
-                predictions=predictions, labels=labels, weights=weights, **kwargs
-            )
+        with record_function(f"## {self.__class__.__name__}:update ##"):
+            if self._fused_update_limit > 0:
+                self._update_buffers[self.PREDICTIONS].append(predictions)
+                self._update_buffers[self.LABELS].append(labels)
+                if weights is not None:
+                    self._update_buffers[self.WEIGHTS].append(weights)
+                self._check_fused_update(force=False)
+            else:
+                self._update(
+                    predictions=predictions, labels=labels, weights=weights, **kwargs
+                )
 
     # The implementation of compute is very similar to local_compute, but compute overwrites
     # the abstract method compute in torchmetrics.Metric, which is wrapped by _wrap_compute
     def compute(self) -> Dict[str, torch.Tensor]:
         self._check_fused_update(force=True)
         ret = {}
         for task, metric_name, metric_value, prefix, description in self._tasks_iter(
```

## torchrec/quant/embedding_modules.py

```diff
@@ -262,28 +262,25 @@
         # We map over the parameters from FBGEMM backed kernels to the canonical nn.EmbeddingBag
         # representation. This provides consistency between this class and the EmbeddingBagCollection
         # nn.Module API calls (state_dict, named_modules, etc)
         self.embedding_bags: nn.ModuleDict = nn.ModuleDict()
         for (_key, tables), emb_module in zip(
             self._key_to_tables.items(), self._emb_modules
         ):
-            for embedding_config, (weight, qscaleshift) in zip(
-                tables, emb_module.split_embedding_weights(split_scale_shifts=True)
+            for embedding_config, (weight, _) in zip(
+                tables, emb_module.split_embedding_weights(split_scale_shifts=False)
             ):
                 self.embedding_bags[embedding_config.name] = torch.nn.Module()
                 # register as a buffer so it's exposed in state_dict.
                 # TODO: register as param instead of buffer
                 # however, since this is only needed for inference, we do not need to expose it as part of parameters.
                 # Additionally, we cannot expose uint8 weights as parameters due to autograd restrictions.
                 self.embedding_bags[embedding_config.name].register_buffer(
                     "weight", weight
                 )
-                self.embedding_bags[embedding_config.name].register_buffer(
-                    "weight_qscaleshift", qscaleshift
-                )
         self.register_tbes = register_tbes
         if register_tbes:
             self.tbes: torch.nn.ModuleList = torch.nn.ModuleList(self._emb_modules)
 
     def forward(
         self,
         features: KeyedJaggedTensor,
@@ -510,19 +507,16 @@
 
             self._emb_modules.append(emb_module)
             self.embeddings[config.name] = torch.nn.Module()
             # register as a buffer so it's exposed in state_dict.
             # TODO: register as param instead of buffer
             # however, since this is only needed for inference, we do not need to expose it as part of parameters.
             # Additionally, we cannot expose uint8 weights as parameters due to autograd restrictions.
-            weights_list = emb_module.split_embedding_weights(split_scale_shifts=True)
+            weights_list = emb_module.split_embedding_weights(split_scale_shifts=False)
             self.embeddings[config.name].register_buffer("weight", weights_list[0][0])
-            self.embeddings[config.name].register_buffer(
-                "weight_qscaleshift", weights_list[0][1]
-            )
 
             if not config.feature_names:
                 config.feature_names = [config.name]
 
         self._embedding_names_by_table: List[List[str]] = get_embedding_names_by_table(
             tables
         )
```

## Comparing `torchrec_nightly-2023.5.3.dist-info/LICENSE` & `torchrec_nightly-2023.5.5.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `torchrec_nightly-2023.5.3.dist-info/METADATA` & `torchrec_nightly-2023.5.5.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: torchrec-nightly
-Version: 2023.5.3
+Version: 2023.5.5
 Summary: Pytorch domain library for recommendation systems
 Home-page: https://github.com/pytorch/torchrec
 Author: TorchRec Team
 Author-email: packages@pytorch.org
 License: BSD-3
 Keywords: pytorch,recommendation systems,sharding
 Classifier: Development Status :: 4 - Beta
```

## Comparing `torchrec_nightly-2023.5.3.dist-info/RECORD` & `torchrec_nightly-2023.5.5.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -9,35 +9,35 @@
 torchrec/datasets/scripts/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/datasets/scripts/contiguous_preproc_criteo.py,sha256=8jjtDiQScJH6R4FfKLgJ3fm-FzfnJxyfbV52zj3SM_8,2448
 torchrec/datasets/scripts/npy_preproc_criteo.py,sha256=QOyHZpPGL6HtPrVijk7qTALvnDqgecWPEZxKB_eVA94,2847
 torchrec/datasets/scripts/shuffle_preproc_criteo.py,sha256=PC1t5EkJkG6qu3ioewAVZM-Bnzo01HKMUH92IprFth0,3077
 torchrec/datasets/test_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/datasets/test_utils/criteo_test_utils.py,sha256=Ob2fJniGOsfbWNF_Gy2RJhrGAVnLAFPSlUTJOp2kay4,5308
 torchrec/distributed/__init__.py,sha256=VCy8GKOM-1dejxUWNSA3gozG3HQ4x5-Y9c9-WFbAMGg,1912
-torchrec/distributed/batched_embedding_kernel.py,sha256=av-G7HZpWNhzaDaFSc1SzlU6n_-qC5EW41zqrBpkXfY,37254
+torchrec/distributed/batched_embedding_kernel.py,sha256=nRkTFv6KK7aHD3Lwwmy7J-Q2pmIeyIr-orfMBtVsztk,36972
 torchrec/distributed/collective_utils.py,sha256=r7Aawq-KSVC-HjjEd6U8k0vNnRMx_-8_sAhYdElGaJw,2069
 torchrec/distributed/comm.py,sha256=ag0NuBUMGpTY8DLKfd7IVs6yRgVdsRwQFKXyDiqjoVg,4925
 torchrec/distributed/comm_ops.py,sha256=vrTHX7h81UxYoS5X3ufY4lE6WC2Ufo4mMJpGQC4dyV0,55820
 torchrec/distributed/dist_data.py,sha256=TwNB0e24k_tmn0XARaXKjUCf-92bgFDzBncj8BcXobY,35443
 torchrec/distributed/embedding.py,sha256=Ew0hvvY4XxwxYmvtCXa4Y6D_yhNkYdcHx0BOA_2nKbw,29817
-torchrec/distributed/embedding_kernel.py,sha256=X24OliYNs5D2taRYncTQGN5MdUKWYZvv4FpXzgSho0c,4443
-torchrec/distributed/embedding_lookup.py,sha256=u_mAudcQEdgMdyahhwIB1hfa9CF5D7whaVFDQ-EBzpI,27279
+torchrec/distributed/embedding_kernel.py,sha256=SNpvTbW1V4HtjSmAzPPa5imrUzSOe7zuQDkK3GFudHc,3872
+torchrec/distributed/embedding_lookup.py,sha256=JrLfam6U0B7X-QdC6nLeqxItyiV-tXoOxhRUqkPDhE0,27189
 torchrec/distributed/embedding_sharding.py,sha256=_4xDvyTm9Ut5XwnQGiANmvko3AzkgaB50d9Ooxr55xI,14990
 torchrec/distributed/embedding_tower_sharding.py,sha256=ypr4JbTZUh_35dYUoKWoOSJNEVG3c6gV9g5gt-fs6lQ,37089
 torchrec/distributed/embedding_types.py,sha256=RAyug556PJTk5QlutuiXbo3ip4r9YXAmJupTNLhqKuk,15021
 torchrec/distributed/embeddingbag.py,sha256=6H9HFiHGgX9LXt67GKfFazrp5iWrbD30HleJZP8btas,34625
 torchrec/distributed/fbgemm_qcomm_codec.py,sha256=StYltKC6Eq6SE_YiX6GsVW3ZF0VyqTcGHXuCYmPAFlU,7373
 torchrec/distributed/fused_embedding.py,sha256=uIgeaoEPujTgkcq8S2OPRYBe03J4TUF04uOOy_iRsMk,5273
 torchrec/distributed/fused_embeddingbag.py,sha256=7DIMc5sHdsDAgqCnNomcPo6V4aIH1wlkzyshHeJB3pc,5110
 torchrec/distributed/fused_params.py,sha256=WnPW8Crs9SSDFF33f4RZx_ok51I8-ql0yeA8N93aMtw,1699
 torchrec/distributed/grouped_position_weighted.py,sha256=q-QE0U306BiPkXIAlJGIQ80EUDZj-FXTbWwjz3EyvLI,3807
 torchrec/distributed/model_parallel.py,sha256=yfk_pYRT7ZrBh6UrukOgYbzHpg0yd6pFSU5NfA4SXxo,19528
-torchrec/distributed/quant_embedding.py,sha256=tXcvkorboPGLH8-KhDgt1i0IRF1LAQYhCAa4FOJXHNQ,13636
-torchrec/distributed/quant_embedding_kernel.py,sha256=GoZ7VWhmgFqie9GB96DCOGMhuywvgyk-TtyktRZ2OtE,14006
-torchrec/distributed/quant_embeddingbag.py,sha256=IdPIlh1LjUEloqKU73pMsoyRy2lsi_6NhR6ayz7Gkhc,10833
+torchrec/distributed/quant_embedding.py,sha256=yFLsrmulaC0jopBSfjb6X-DaJ-08_sf-7lWqG6NlNdQ,13316
+torchrec/distributed/quant_embedding_kernel.py,sha256=wdCN5ZMGg3T-0sErGqLMJtDDVzVT44zX5rfQ-rWWUMM,13385
+torchrec/distributed/quant_embeddingbag.py,sha256=asSGkfXeG0hCB4pJlVhw_Wx97AjYwotIZd0h_e0ZbVg,10510
 torchrec/distributed/shard.py,sha256=4Dr5ixWCoMEFEuL5WN4fL2gIdl9wmSUjZWsiF-kdCdQ,9261
 torchrec/distributed/sharding_plan.py,sha256=1CZyCmPp_lJPeitOgufRNfe77aXp9WtxsKDqVOm8XnU,19218
 torchrec/distributed/train_pipeline.py,sha256=RotmGdM_UduxSbrf3l8JKrowIeMhrB2DaNJHFwXPjIY,22330
 torchrec/distributed/types.py,sha256=tNd_B5PXjaOqPmUXxSZLXB3a71sjU8LBtSY9kuGxe7I,24927
 torchrec/distributed/utils.py,sha256=isPXbyPhHlbjiA0280ZY0V_wPUBuvDvvH4w6ziP5XA8,11373
 torchrec/distributed/composable/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/distributed/composable/table_batched_embedding_slice.py,sha256=x439M8TTXQtzoihan1OKKbmGYkqJlAxxTHCDz5295RY,3207
@@ -81,22 +81,22 @@
 torchrec/inference/modules.py,sha256=4Qfea1MUXjX-qknC7VO8BkodJU8-BA-kMQUGjdq2j64,7834
 torchrec/inference/state_dict_transform.py,sha256=eFejfFRzU3pL7CmiJtkjq2mpz6Ue_NOUuO2g8-LnLig,3797
 torchrec/metrics/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/metrics/auc.py,sha256=-aRUv-fVdnTAYAhoV6fp5zvcoadl54BAOzzBFLIPF2I,12728
 torchrec/metrics/calibration.py,sha256=hngIvPjcAHusdz1KBrPvwMZX6OkyS0nv18EAmMcyIl0,3703
 torchrec/metrics/ctr.py,sha256=2HY53CX9If6wYcOErvYCT73NH8UdY8oI0UIitoREsMs,3465
 torchrec/metrics/mae.py,sha256=QDPT7l4LCBtTgoxEeEm9vFzCEDtEIlfzs58PtH1oS_U,3836
-torchrec/metrics/metric_module.py,sha256=ZiwNrhrVS79jMXdMAtdcWuDZtaUIzumSDILFWAKDeaI,17573
+torchrec/metrics/metric_module.py,sha256=Zb871gbVS2ymaoslwJIaFbps0tEjDiOiCStoppMj_bA,17812
 torchrec/metrics/metrics_config.py,sha256=eZW2aD6fLK9eqhFU6G62Ph5R0KwkDUjErj7Irbg0k68,6615
 torchrec/metrics/metrics_namespace.py,sha256=nHQCcGJujWcHR8KWDRgGY2yLpKvqtNTAL42o953nPck,3643
 torchrec/metrics/model_utils.py,sha256=WNMsAKK9b5OVPWeuRrNDZ2HjwKuAw3gjNG3OdE99EXw,3904
 torchrec/metrics/mse.py,sha256=tkK3yBV8Uu9Z8Mw_3c1k4kYAhp1jUQkFEJxc0l2LtWU,4631
 torchrec/metrics/multiclass_recall.py,sha256=7NqjkA4IUy4Db90du3hVt8cJxoWS18PBaOourHETkEs,5605
 torchrec/metrics/ne.py,sha256=wXEb9Eh5Mn3D_uuAIaUb3CvJ1eDs06ahVxaWOjR4CtU,6811
-torchrec/metrics/rec_metric.py,sha256=0WOAHMBhvPJ2zCrXR2aNKFiYxJ9S1uUGAuzuIxOx4F8,30635
+torchrec/metrics/rec_metric.py,sha256=OLiACQ0nL9LCRtJt23_pax02VVdKV9Hq6O-3af3Ml1E,30881
 torchrec/metrics/recall_session.py,sha256=24aDBSi8V5XXSnwpqCR5bWcm2CWslBBZ9_a0LQ6VGeg,10490
 torchrec/metrics/throughput.py,sha256=A_WqccFNgu4XpBZzZxnGxAT84Xp2ld43qQM6R1zaqik,6057
 torchrec/metrics/tower_qps.py,sha256=YC-EzaU2hbAQ3VZ1GJDHjWNKLD8fEvyyM5A5WerjwZU,10622
 torchrec/metrics/weighted_avg.py,sha256=dX-qwha2__F-p-R5u27152OwlGRS6Xfnbt1DEIBw8NQ,2867
 torchrec/metrics/test_utils/__init__.py,sha256=p_uep7Hg7_aY3rOg8CxtS-REopQ4ywQq075KFoso1Lw,16441
 torchrec/models/__init__.py,sha256=iYpG-yLxFaa3ZdN-UU9PWNIdgm9EXoEj_QzJ2OLKGJQ,913
 torchrec/models/deepfm.py,sha256=-SxFKF-kHKFEzC0XIsLigQh673-B_Lu6xFlZPkR5t2g,11410
@@ -122,18 +122,18 @@
 torchrec/optim/fused.py,sha256=J4u2PWaHD65PIEdg0wTWoK_riZCoSKAVKYPGuZtfKLY,1353
 torchrec/optim/keyed.py,sha256=5QASHSkw2oYwu3OVHU5JJ2mm5tQaSCOQ3z_vBVRLPtk,16069
 torchrec/optim/optimizers.py,sha256=tmf-ww_47F_tu04ufffVCczM2AOMR5yTNh_8S-gJl-8,4420
 torchrec/optim/rowwise_adagrad.py,sha256=jHpCZDFWRrXbwlKiwROg2dl0qK1lG2uXMJkTtbZ_uSg,7405
 torchrec/optim/warmup.py,sha256=QaMcWU-jMZbDOfRHEH_x9r7EsjtI_LRgOGAmEgzJWSQ,4865
 torchrec/optim/test_utils/__init__.py,sha256=mkNZr5iNV3kseuamPBnnAeknjbv4ELPFMGyJ1lWiJGU,560
 torchrec/quant/__init__.py,sha256=A6NIA6ztq6iP1JTLRLNzlgnCcd-LaN8efnxGub3Ii4A,1140
-torchrec/quant/embedding_modules.py,sha256=DUTwXLCIofNUwdZW7gFp8s3YH4Yy9pd6BB73PdSrcGw,23109
+torchrec/quant/embedding_modules.py,sha256=V4lPuXukSXBXz92AqQ1PBpQcQXh03TiK-GNtlJNiqug,22824
 torchrec/quant/utils.py,sha256=2oUJIsrzE7ijvPs5DYUa06wOfmRvU1KdU1aQI7DUccs,3691
 torchrec/sparse/__init__.py,sha256=dLqSye4Jo6obnNNTUKdPDxPQb9sL2U4weemSn-DjpYk,1163
 torchrec/sparse/jagged_tensor.py,sha256=MoyqQJC8ezXOPLLEGK1o4zzI_SQ18tUUQ3zzOlW-_b4,52763
 torchrec/sparse/test_utils/__init__.py,sha256=BLxfGKJvwjjCiQM64O5wGAA_Cea0sG-buw9lTDWuqug,1430
 torchrec/test_utils/__init__.py,sha256=JncJcXS4N3gI7-fsizQ2-qiWM6MhIrpvskF_9gDf0Go,5661
-torchrec_nightly-2023.5.3.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
-torchrec_nightly-2023.5.3.dist-info/METADATA,sha256=URYGxobSLkQJffwDR9UT-lG-Rq9vOeJbsuNMOfM74Qs,5011
-torchrec_nightly-2023.5.3.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
-torchrec_nightly-2023.5.3.dist-info/top_level.txt,sha256=LoLcTAPLj_7x62AuyYmhEVBcx2WJ1Z1Nrknv0Jnk_gQ,9
-torchrec_nightly-2023.5.3.dist-info/RECORD,,
+torchrec_nightly-2023.5.5.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
+torchrec_nightly-2023.5.5.dist-info/METADATA,sha256=WywCY3ZJn2ajLjyuMhOuvN7iAxizMS2pRZjtJE1JSao,5011
+torchrec_nightly-2023.5.5.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
+torchrec_nightly-2023.5.5.dist-info/top_level.txt,sha256=LoLcTAPLj_7x62AuyYmhEVBcx2WJ1Z1Nrknv0Jnk_gQ,9
+torchrec_nightly-2023.5.5.dist-info/RECORD,,
```

